<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Quiz: Check Your Understanding
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Temporal-Difference Methods
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introduction.html">
       01. Introduction
      </a>
     </li>
     <li class="">
      <a href="02. Review MC Control Methods.html">
       02. Review: MC Control Methods
      </a>
     </li>
     <li class="">
      <a href="03. Quiz MC Control Methods.html">
       03. Quiz: MC Control Methods
      </a>
     </li>
     <li class="">
      <a href="04. TD Control Sarsa.html">
       04. TD Control: Sarsa
      </a>
     </li>
     <li class="">
      <a href="05. Quiz Sarsa.html">
       05. Quiz: Sarsa
      </a>
     </li>
     <li class="">
      <a href="06. TD Control Q-Learning.html">
       06. TD Control: Q-Learning
      </a>
     </li>
     <li class="">
      <a href="07. Quiz Q-Learning.html">
       07. Quiz: Q-Learning
      </a>
     </li>
     <li class="">
      <a href="08. TD Control Expected Sarsa.html">
       08. TD Control: Expected Sarsa
      </a>
     </li>
     <li class="">
      <a href="09. Quiz Expected Sarsa.html">
       09. Quiz: Expected Sarsa
      </a>
     </li>
     <li class="">
      <a href="10. TD Control Theory and Practice.html">
       10. TD Control: Theory and Practice
      </a>
     </li>
     <li class="">
      <a href="11. OpenAI Gym CliffWalkingEnv.html">
       11. OpenAI Gym: CliffWalkingEnv
      </a>
     </li>
     <li class="">
      <a href="12. Workspace - Introduction.html">
       12. Workspace - Introduction
      </a>
     </li>
     <li class="">
      <a href="13. Coding Exercise.html">
       13. Coding Exercise
      </a>
     </li>
     <li class="">
      <a href="14. Workspace.html">
       14. Workspace
      </a>
     </li>
     <li class="">
      <a href="15. Analyzing Performance.html">
       15. Analyzing Performance
      </a>
     </li>
     <li class="">
      <a href="16. Quiz Check Your Understanding.html">
       16. Quiz: Check Your Understanding
      </a>
     </li>
     <li class="">
      <a href="17. Summary.html">
       17. Summary
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          16. Quiz: Check Your Understanding
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="quiz-check-your-understanding">
          Quiz: Check Your Understanding
         </h1>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          In this lesson, you learned about many different algorithms for Temporal-Difference (TD) control.  Later in this nanodegree, you'll learn more about how to adapt the Q-Learning algorithm to produce the Deep Q-Learning algorithm that demonstrated
          <a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk" rel="noopener noreferrer" target="_blank">
           superhuman performance
          </a>
          at Atari games.
         </p>
         <p>
          Before moving on, you're encouraged to check your understanding by completing this brief quiz on
          <strong>
           Q-Learning
          </strong>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="The Agent and Environment" class="img img-fluid" src="img/screen-shot-2018-03-07-at-3.53.08-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            The Agent and Environment
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-the-agent-and-environment">
          ## The Agent and Environment
         </h2>
         <p>
          Imagine an agent that moves along a line with only five discrete positions (0, 1, 2, 3, or 4). The agent can move left, right or stay put. (
          <em>
           If the agent chooses to move left when at position 0 or right at position 4, the agent just remains in place.
          </em>
          )
         </p>
         <p>
          The Q-table has:
         </p>
         <ul>
          <li>
           five rows, corresponding to the five possible states that may be observed, and
          </li>
          <li>
           three columns, corresponding to three possible actions that the agent can take in response.
          </li>
         </ul>
         <p>
          The goal state is position 3, but the agent doesn't know that and is going to learn the best policy for getting to the goal via the Q-Learning algorithm (with learning rate
          <span class="mathquill ud-math">
           \alpha=0.2
          </span>
          ). The environment will provide a reward of -1 for all locations except the goal state. The episode ends when the goal is reached.
         </p>
         <h2 id="-episode-0-time-0">
          ## Episode 0, Time 0
         </h2>
         <p>
          The Q-table is initialized.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Episode 0, Time 0" class="img img-fluid" src="img/screen-shot-2018-03-07-at-2.33.19-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            Episode 0, Time 0
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Say the agent observes the initial
          <strong>
           state
          </strong>
          (position 1) and selects
          <strong>
           action
          </strong>
          stay.
         </p>
         <p>
          As a result, it receives the
          <strong>
           next state
          </strong>
          (position 1) and a
          <strong>
           reward
          </strong>
          (-1.0) from the environment.
         </p>
         <p>
          Let:
         </p>
         <ul>
          <li>
           <span class="mathquill ud-math">
            s_t
           </span>
           denote the state at time step
           <span class="mathquill ud-math">
            t
           </span>
           ,
          </li>
          <li>
           <span class="mathquill ud-math">
            a_t
           </span>
           denote the action at time step
           <span class="mathquill ud-math">
            t
           </span>
           , and
          </li>
          <li>
           <span class="mathquill ud-math">
            r_t
           </span>
           denote the reward at time step
           <span class="mathquill ud-math">
            t
           </span>
           .
          </li>
         </ul>
         <p>
          Then, the agent now knows
          <span class="mathquill ud-math">
           s_0, a_0,r_1
          </span>
          and
          <span class="mathquill ud-math">
           s_1
          </span>
          .  Namely,
          <span class="mathquill ud-math">
           s_0 = 1, a_0=\text{stay},r_1=-1.0,
          </span>
          and
          <span class="mathquill ud-math">
           s_1=1
          </span>
          .
         </p>
         <p>
          Using this information, it can update the Q-table value for
          <span class="mathquill ud-math">
           Q(s_0, a_0)
          </span>
          .
          <br/>
          Recall the equation for updating the Q-table:
         </p>
         <div class="mathquill">
          {\displaystyle Q(s_{t},a_{t})\leftarrow (1-\alpha )\cdot \underbrace {Q(s_{t},a_{t})} _{\rm {old~value}}+\underbrace {\alpha } _{\rm {learning~rate}}\cdot \overbrace {{\bigg (}\underbrace {r_{t+1}} _{\rm {reward}}+\underbrace {\gamma } _{\rm {discount~factor}}\cdot \underbrace {\max _{a}Q(s_{t+1},a)} _{\rm {estimate~of~optimal~future~value}}{\bigg )}} ^{\rm {learned~value}}}
         </div>
         <p>
          Note that this is equivalent to the equation below (from the video on
          <strong>
           Q-Learning
          </strong>
          ):
         </p>
         <div class="mathquill">
          Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha(r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t))
         </div>
         <p>
          So the equation for updating
          <span class="mathquill ud-math">
           Q(s_0, a_0)
          </span>
          is:
         </p>
         <div class="mathquill">
          Q(s_{0},a_{0})\leftarrow (1-\alpha )\cdot Q(s_{0},a_{0}) + \alpha \cdot (r_1 + \gamma \max_aQ(s_1,a))
         </div>
         <p>
          Substituting our known values:
         </p>
         <div class="mathquill">
          {\displaystyle Q(s_{0},a_{0})\leftarrow (1-0.2 )\cdot  {Q(s_{0},a_{0})} +0.2\cdot  {{\bigg (} {r_{1}} +{\max _{a}Q(s_{1},a)} {\bigg )}} }
         </div>
         <p>
          We can find the
          <em>
           old value
          </em>
          for
          <span class="mathquill ud-math">
           Q(s_{0},a_{0})
          </span>
          by looking it up in the table for state
          <span class="mathquill ud-math">
           s_{0}=1
          </span>
          and action
          <span class="mathquill ud-math">
           a_{0}=stay
          </span>
          which is a value of 0.  To find the
          <em>
           estimate of the optimal future value
          </em>
          <span class="mathquill ud-math">
           \max
           <em>
            {a}Q(s
           </em>
           {1},a)
          </span>
          , we need to look at the entire row of actions for the
          <em>
           next
          </em>
          state,
          <span class="mathquill ud-math">
           s_{1}=1
          </span>
          and choose the maximum value across all actions.  They are all 0 right now, so the maximum is 0.  Reducing the equation, we can now update
          <span class="mathquill ud-math">
           Q(s_{0},a_{0})
          </span>
          .
         </p>
         <div class="mathquill">
          {\displaystyle Q(s_{0},a_{0})\leftarrow -0.2 }
         </div>
         <h2 id="-episode-0-time-1">
          ## Episode 0, Time 1
         </h2>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="" class="img img-fluid" src="img/screen-shot-2018-03-07-at-2.43.07-pm.png"/>
          <figcaption class="figure-caption">
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          At this step, an action must be chosen.  The best action for position 1 could be either "left" or "right", since their values in the Q-table are equal.
         </p>
         <p>
          Remember that in Q-Learning, the agent uses the epsilon-greedy policy to select an action.  Say that in this case, the agent selects
          <strong>
           action
          </strong>
          right at random.
         </p>
         <p>
          Then, the agent receives a
          <strong>
           new state
          </strong>
          (position 2) and
          <strong>
           reward
          </strong>
          (-1.0) from the environment.
         </p>
         <p>
          The agent now knows
          <span class="mathquill ud-math">
           s_1, a_1,r_2,
          </span>
          and
          <span class="mathquill ud-math">
           s_2
          </span>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <div>
          <p>
           <strong>
            QUESTION:
           </strong>
           <p>
            What is the updated value for
            <span class="mathquill ud-math">
             Q(s_1, a_1)
            </span>
            ? (round to the nearest 10th)
           </p>
          </p>
          <div class="" form-group"="">
           <label for="answer">
            <strong>
             ANSWER:
            </strong>
           </label>
           <textarea class="form-control" id="answer"></textarea>
          </div>
         </div>
         <details>
          <summary>
           <strong>
            SOLUTION:
           </strong>
          </summary>
          <p>
           <i>
            NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer
           </i>
          </p>
         </details>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-episode-n">
          ## Episode n
         </h2>
         <p>
          Now assume that a number of episodes have been run, and the Q-table includes the values shown below.
         </p>
         <p>
          A new episode begins, as before.  The environment gives an initial
          <strong>
           state
          </strong>
          (position 1), and the agent selects
          <strong>
           action
          </strong>
          stay.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Episode n, Time 0" class="img img-fluid" src="img/screen-shot-2018-03-07-at-3.16.47-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            Episode n, Time 0
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <div>
          <p>
           <strong>
            QUESTION:
           </strong>
           <p>
            What is the new value for Q(1,stay)? (round your answer to the nearest 10th)
           </p>
          </p>
          <div class="" form-group"="">
           <label for="answer">
            <strong>
             ANSWER:
            </strong>
           </label>
           <textarea class="form-control" id="answer"></textarea>
          </div>
         </div>
         <details>
          <summary>
           <strong>
            SOLUTION:
           </strong>
          </summary>
          <p>
           <i>
            NOTE: The solutions are expressed in RegEx pattern. Udacity uses these patterns to check the given answer
           </i>
          </p>
         </details>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="17. Summary.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('16. Quiz: Check Your Understanding')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
