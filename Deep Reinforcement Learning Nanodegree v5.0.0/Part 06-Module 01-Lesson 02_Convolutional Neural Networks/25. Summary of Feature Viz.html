<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Summary of Feature Viz
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Convolutional Neural Networks
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introducing Cezanne.html">
       01. Introducing Cezanne
      </a>
     </li>
     <li class="">
      <a href="02. Lesson Outline and Data.html">
       02. Lesson Outline and Data
      </a>
     </li>
     <li class="">
      <a href="03. CNN Architecture, VGG-16.html">
       03. CNN Architecture, VGG-16
      </a>
     </li>
     <li class="">
      <a href="04. Convolutional Layers.html">
       04. Convolutional Layers
      </a>
     </li>
     <li class="">
      <a href="05. Defining Layers in PyTorch.html">
       05. Defining Layers in PyTorch
      </a>
     </li>
     <li class="">
      <a href="06. Notebook Visualizing a Convolutional Layer.html">
       06. Notebook: Visualizing a Convolutional Layer
      </a>
     </li>
     <li class="">
      <a href="07. Pooling, VGG-16 Architecture.html">
       07. Pooling, VGG-16 Architecture
      </a>
     </li>
     <li class="">
      <a href="08. Pooling Layers.html">
       08. Pooling Layers
      </a>
     </li>
     <li class="">
      <a href="09. Notebook Visualizing a Pooling Layer.html">
       09. Notebook: Visualizing a Pooling Layer
      </a>
     </li>
     <li class="">
      <a href="10. Fully-Connected Layers, VGG-16.html">
       10. Fully-Connected Layers, VGG-16
      </a>
     </li>
     <li class="">
      <a href="11. Notebook Visualizing FashionMNIST.html">
       11. Notebook: Visualizing FashionMNIST
      </a>
     </li>
     <li class="">
      <a href="12. Training in PyTorch.html">
       12. Training in PyTorch
      </a>
     </li>
     <li class="">
      <a href="13. Notebook Fashion MNIST Training Exercise.html">
       13. Notebook: Fashion MNIST Training Exercise
      </a>
     </li>
     <li class="">
      <a href="14. Notebook FashionMNIST, Solution 1.html">
       14. Notebook: FashionMNIST, Solution 1
      </a>
     </li>
     <li class="">
      <a href="15. Review Dropout.html">
       15. Review: Dropout
      </a>
     </li>
     <li class="">
      <a href="16. Notebook FashionMNIST, Solution 2.html">
       16. Notebook: FashionMNIST, Solution 2
      </a>
     </li>
     <li class="">
      <a href="17. Feature Visualization.html">
       17. Feature Visualization
      </a>
     </li>
     <li class="">
      <a href="18. Feature Maps.html">
       18. Feature Maps
      </a>
     </li>
     <li class="">
      <a href="19. First Convolutional Layer.html">
       19. First Convolutional Layer
      </a>
     </li>
     <li class="">
      <a href="20. Visualizing CNNs (Part 2).html">
       20. Visualizing CNNs (Part 2)
      </a>
     </li>
     <li class="">
      <a href="21. Visualizing Activations.html">
       21. Visualizing Activations
      </a>
     </li>
     <li class="">
      <a href="22. Notebook Feature Viz for FashionMNIST.html">
       22. Notebook: Feature Viz for FashionMNIST
      </a>
     </li>
     <li class="">
      <a href="23. Last Feature Vector and t-SNE.html">
       23. Last Feature Vector and t-SNE
      </a>
     </li>
     <li class="">
      <a href="24. Occlusion, Saliency, and Guided Backpropagation.html">
       24. Occlusion, Saliency, and Guided Backpropagation
      </a>
     </li>
     <li class="">
      <a href="25. Summary of Feature Viz.html">
       25. Summary of Feature Viz
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          25. Summary of Feature Viz
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
         <p>
          20 Summary Of Feature Viz V2 RENDER V2
         </p>
        </h3>
        <video controls="">
         <source src="25. 20 Summary Of Feature Viz V2 RENDER V2-r2LBoEkXskU.mp4" type="video/mp4"/>
         <track default="false" kind="subtitles" label="zh-CN" src="25. 20 Summary Of Feature Viz V2 RENDER V2-r2LBoEkXskU.zh-CN.vtt" srclang="zh-CN"/>
         <track default="true" kind="subtitles" label="en" src="25. 20 Summary Of Feature Viz V2 RENDER V2-r2LBoEkXskU.en.vtt" srclang="en"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="deep-dream">
          Deep Dream
         </h3>
         <p>
          DeepDream takes in an input image and uses the features in a trained CNN to amplifying the existing, detected features in the input image! The process is as follows:
         </p>
         <ol>
          <li>
           Choose an input image, and choose a convolutional layer in the network whose features you want to amplify (the first layer will amplify simple edges and later layers will amplify more complex features).
          </li>
          <li>
           Compute the activation maps for the input image at your chosen layer.
          </li>
          <li>
           Set the gradient of the chosen layer equal to the activations and and use this to compute the gradient image.
          </li>
          <li>
           Update the input image and repeat!
          </li>
         </ol>
         <p>
          In step 3, by setting the gradient in the layer equal to the activation, weâ€™re telling that layer to give more weight to the features in the activation map. So, if a layer detects corners, then the corners in an input image will be amplified, and you can see such corners in the upper-right sky of the mountain image, below. For any layer, changing the gradient to be equal to the activations in that layer will amplify the features in the given image that the layer is responding to the most.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="DeepDream on an image of a mountain." class="img img-fluid" src="img/screen-shot-2018-04-23-at-8.35.17-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            DeepDream on an image of a mountain.
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="style-transfer">
          Style Transfer
         </h3>
         <p>
          Style transfer aims to separate the content of an image from its style. So, how does it do this?
         </p>
         <p>
          <strong>
           Isolating content
          </strong>
         </p>
         <p>
          When Convolutional Neural Networks are trained to recognize objects, further layers in the network extract features that distill information about the content of an image and discard any extraneous information. That is, as we go deeper into a CNN, the input image is transformed into feature maps that increasingly care about the content of the image rather than any detail about the texture and color of pixels (which is something close to style).
         </p>
         <p>
          You may hear features, in later layers of a network, referred to as a "content representation" of an image.
         </p>
         <p>
          <strong>
           Isolating style
          </strong>
         </p>
         <p>
          To isolate the style of an input image, a feature space designed to capture texture information is used. This space essentially looks at the correlations between feature maps in each layer of a network; the correlations give us an idea of texture and color information but leave out information about the arrangement of different objects in an image.
         </p>
         <p>
          <strong>
           Combining style and content to create a new image
          </strong>
         </p>
         <p>
          Style transfer takes in two images, and separates the content and style of each of those images. Then, to transfer the style of one image to another, it takes the content of the new image and applies the style of an another image (often a famous artwork).
         </p>
         <p>
          The objects and shape arrangement of the new image is preserved, and the colors and textures (style) that make up the image are taken from another image. Below you can see an example of an image of a cat [content] being combined with the a Hokusai image of waves [style]. Effectively, style transfer renders the cat image in the style of the wave artwork.
         </p>
         <p>
          If you'd like to try out Style Transfer on your own images, check out the
          <strong>
           Elective Section: "Applications of Computer Vision and Deep Learning."
          </strong>
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Style transfer on an image of a cat and waves." class="img img-fluid" src="img/screen-shot-2018-04-23-at-8.35.25-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            Style transfer on an image of a cat and waves.
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('25. Summary of Feature Viz')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
