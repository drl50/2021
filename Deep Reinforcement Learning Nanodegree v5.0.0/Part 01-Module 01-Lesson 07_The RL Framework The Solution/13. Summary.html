<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Summary
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      The RL Framework: The Solution
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introduction.html">
       01. Introduction
      </a>
     </li>
     <li class="">
      <a href="02. Policies.html">
       02. Policies
      </a>
     </li>
     <li class="">
      <a href="03. Quiz Interpret the Policy.html">
       03. Quiz: Interpret the Policy
      </a>
     </li>
     <li class="">
      <a href="04. Gridworld Example.html">
       04. Gridworld Example
      </a>
     </li>
     <li class="">
      <a href="05. State-Value Functions.html">
       05. State-Value Functions
      </a>
     </li>
     <li class="">
      <a href="06. Bellman Equations.html">
       06. Bellman Equations
      </a>
     </li>
     <li class="">
      <a href="07. Quiz State-Value Functions.html">
       07. Quiz: State-Value Functions
      </a>
     </li>
     <li class="">
      <a href="08. Optimality.html">
       08. Optimality
      </a>
     </li>
     <li class="">
      <a href="09. Action-Value Functions.html">
       09. Action-Value Functions
      </a>
     </li>
     <li class="">
      <a href="10. Quiz Action-Value Functions.html">
       10. Quiz: Action-Value Functions
      </a>
     </li>
     <li class="">
      <a href="11. Optimal Policies.html">
       11. Optimal Policies
      </a>
     </li>
     <li class="">
      <a href="12. Quiz Optimal Policies.html">
       12. Quiz: Optimal Policies
      </a>
     </li>
     <li class="">
      <a href="13. Summary.html">
       13. Summary
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          13. Summary
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="summary">
          Summary
         </h1>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="State-value function for golf-playing agent (Sutton and Barto, 2017)" class="img img-fluid" src="img/screen-shot-2017-09-25-at-11.35.38-am.png"/>
          <figcaption class="figure-caption">
           <p>
            State-value function for golf-playing agent (Sutton and Barto, 2017)
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-policies">
          ### Policies
         </h2>
         <ul>
          <li>
           A
           <strong>
            deterministic policy
           </strong>
           is a mapping
           <span class="mathquill ud-math">
            \pi: \mathcal{S}\to\mathcal{A}
           </span>
           .  For each state
           <span class="mathquill ud-math">
            s\in\mathcal{S}
           </span>
           , it yields the action
           <span class="mathquill ud-math">
            a\in\mathcal{A}
           </span>
           that the agent will choose while in state
           <span class="mathquill ud-math">
            s
           </span>
           .
          </li>
          <li>
           A
           <strong>
            stochastic policy
           </strong>
           is a mapping
           <span class="mathquill ud-math">
            \pi: \mathcal{S}\times\mathcal{A}\to [0,1]
           </span>
           .  For each state
           <span class="mathquill ud-math">
            s\in\mathcal{S}
           </span>
           and action
           <span class="mathquill ud-math">
            a\in\mathcal{A}
           </span>
           , it yields the probability
           <span class="mathquill ud-math">
            \pi(a|s)
           </span>
           that the agent chooses action
           <span class="mathquill ud-math">
            a
           </span>
           while in state
           <span class="mathquill ud-math">
            s
           </span>
           .
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-state-value-functions">
          ### State-Value Functions
         </h2>
         <ul>
          <li>
           The
           <strong>
            state-value function
           </strong>
           for a policy
           <span class="mathquill ud-math">
            \pi
           </span>
           is denoted
           <span class="mathquill ud-math">
            v_\pi
           </span>
           .  For each state
           <span class="mathquill ud-math">
            s \in\mathcal{S}
           </span>
           , it yields the expected return if the agent starts in state
           <span class="mathquill ud-math">
            s
           </span>
           and then uses the policy to choose its actions for all time steps.  That is,
           <span class="mathquill ud-math">
            v_\pi(s) \doteq \text{} \mathbb{E}
            <em>
             \pi[G_t|S_t=s]
            </em>
           </span>
           .  We refer to
           <span class="mathquill ud-math">
            v
            \pi(s)
           </span>
           as the
           <strong>
            value of state
            <span class="mathquill ud-math">
             s
            </span>
            under policy
            <span class="mathquill ud-math">
             \pi
            </span>
           </strong>
           .
          </li>
          <li>
           The notation
           <span class="mathquill ud-math">
            \mathbb{E}
            <em>
             \pi[\cdot]
            </em>
           </span>
           is borrowed from the suggested textbook, where
           <span class="mathquill ud-math">
            \mathbb{E}
            \pi[\cdot]
           </span>
           is defined as the expected value of a random variable, given that the agent follows policy
           <span class="mathquill ud-math">
            \pi
           </span>
           .
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-bellman-equations">
          ### Bellman Equations
         </h2>
         <ul>
          <li>
           The
           <strong>
            Bellman expectation equation for
            <span class="mathquill ud-math">
             v_\pi
            </span>
           </strong>
           is:
           <span class="mathquill ud-math">
            v_\pi(s) = \text{} \mathbb{E}
            <em>
             \pi[R
            </em>
            {t+1} + \gamma v_\pi(S_{t+1})|S_t =s].
           </span>
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-optimality">
          ### Optimality
         </h2>
         <ul>
          <li>
           A policy
           <span class="mathquill ud-math">
            \pi'
           </span>
           is defined to be better than or equal to a policy
           <span class="mathquill ud-math">
            \pi
           </span>
           if and only if
           <span class="mathquill ud-math">
            v_{\pi'}(s) \geq v_\pi(s)
           </span>
           for all
           <span class="mathquill ud-math">
            s\in\mathcal{S}
           </span>
           .
          </li>
          <li>
           An
           <strong>
            optimal policy
            <span class="mathquill ud-math">
             \pi_
             <em>
             </em>
            </span>
           </strong>
           satisfies
           <span class="mathquill ud-math">
            \pi_
            \geq \pi
           </span>
           for all policies
           <span class="mathquill ud-math">
            \pi
           </span>
           .  An optimal policy is guaranteed to exist but may not be unique.
          </li>
          <li>
           All optimal policies have the same state-value function
           <span class="mathquill ud-math">
            v_*
           </span>
           , called the
           <strong>
            optimal state-value function
           </strong>
           .
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-action-value-functions">
          ### Action-Value Functions
         </h2>
         <ul>
          <li>
           The
           <strong>
            action-value function
           </strong>
           for a policy
           <span class="mathquill ud-math">
            \pi
           </span>
           is denoted
           <span class="mathquill ud-math">
            q_\pi
           </span>
           .  For each state
           <span class="mathquill ud-math">
            s \in\mathcal{S}
           </span>
           and action
           <span class="mathquill ud-math">
            a \in\mathcal{A}
           </span>
           , it yields the expected return if the agent starts in state
           <span class="mathquill ud-math">
            s
           </span>
           , takes action
           <span class="mathquill ud-math">
            a
           </span>
           , and then follows the policy for all future time steps.  That is,
           <span class="mathquill ud-math">
            q_\pi(s,a) \doteq \mathbb{E}
            <em>
             \pi[G_t|S_t=s, A_t=a]
            </em>
           </span>
           .  We refer to
           <span class="mathquill ud-math">
            q
            \pi(s,a)
           </span>
           as the
           <strong>
            value of taking action
            <span class="mathquill ud-math">
             a
            </span>
            in state
            <span class="mathquill ud-math">
             s
            </span>
            under a policy
            <span class="mathquill ud-math">
             \pi
            </span>
           </strong>
           (or alternatively as the
           <strong>
            value of the state-action pair
            <span class="mathquill ud-math">
             s, a
            </span>
           </strong>
           ).
          </li>
          <li>
           All optimal policies have the same action-value function
           <span class="mathquill ud-math">
            q_*
           </span>
           , called the
           <strong>
            optimal action-value function
           </strong>
           .
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-optimal-policies">
          ### Optimal Policies
         </h2>
         <ul>
          <li>
           Once the agent determines the optimal action-value function
           <span class="mathquill ud-math">
            q_
            <em>
            </em>
           </span>
           , it can quickly obtain an optimal policy
           <span class="mathquill ud-math">
            \pi_
           </span>
           by setting
           <span class="mathquill ud-math">
            \pi_
            <em>
             (s) = \arg\max_{a\in\mathcal{A}(s)} q_
            </em>
            (s,a)
           </span>
           .
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('13. Summary')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
