WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:04.605
大家好 欢迎学习这节关于用 PyTorch 进行深度学习的课程

00:00:04.605 --> 00:00:08.910
我构建了大量 Jupyter Notebook

00:00:08.910 --> 00:00:14.160
指导大家编写实现深度学习网络的 Python 代码

00:00:14.160 --> 00:00:17.399
我们将使用 PyTorch 框架

00:00:17.399 --> 00:00:22.034
它比 TensorFlow 和 Keras 更新

00:00:22.035 --> 00:00:27.435
是由 Facebook AI 研究团队开发的 完全开源

00:00:27.434 --> 00:00:33.030
我们选择 PyTorch 而不是 TensorFlow 和 Keras 的原因是

00:00:33.030 --> 00:00:38.734
它与 Python 本身更相关 就像 Python 编程一样

00:00:38.734 --> 00:00:42.454
而且涉及了深度学习概念

00:00:42.454 --> 00:00:45.509
当你完成这些 notebook 时会发现

00:00:45.509 --> 00:00:48.945
你学习的反向传播等深度学习概念

00:00:48.945 --> 00:00:53.113
在 PyTorch 中显得非常自然

00:00:53.113 --> 00:00:55.859
而在 TensorFlow 和 Keras 中

00:00:55.859 --> 00:01:00.539
编写代码的方式很奇怪

00:01:00.539 --> 00:01:05.459
与深度学习的概念结构不太相符

00:01:05.459 --> 00:01:08.250
你可以在课堂上找到这些 notebook

00:01:08.250 --> 00:01:09.989
位于视频下方的 workspace 中

00:01:09.989 --> 00:01:12.059
还可以从 GitHub 下载这些 notebook

00:01:12.060 --> 00:01:14.924
并在你自己的计算机上完成这些 notebook

00:01:14.924 --> 00:01:17.774
在查看实际代码之前

00:01:17.775 --> 00:01:20.310
你需要理解一个概念 张量

00:01:20.310 --> 00:01:25.710
张量是向量和矩阵的泛化

00:01:25.709 --> 00:01:29.834
例如 向量是一维张量

00:01:29.834 --> 00:01:33.284
有一行值

00:01:33.284 --> 00:01:36.179
矩阵是二维张量

00:01:36.180 --> 00:01:38.400
是一个长方形

00:01:38.400 --> 00:01:40.350
有行和列

00:01:40.349 --> 00:01:44.189
这些数字排列在二维空间中

00:01:44.189 --> 00:01:48.495
我们有 x 坐标和 y 坐标

00:01:48.495 --> 00:01:52.170
彩色图像是三维张量

00:01:52.170 --> 00:01:54.420
对于彩色图像

00:01:54.420 --> 00:01:55.480
每个像素

00:01:55.480 --> 00:01:58.075
都可以用 x 和 y 表示

00:01:58.075 --> 00:01:59.740
但是它还有

00:01:59.739 --> 00:02:01.309
红绿蓝分量

00:02:01.310 --> 00:02:06.329
张量是你将在 PyTorch 中使用的主要数据结构

00:02:06.329 --> 00:02:09.555
当你使用张量时 如果能够在脑中想象出它们的结构

00:02:09.555 --> 00:02:12.879
则对理解很有帮助 你将对它们执行很多操作

00:02:12.879 --> 00:02:16.525
例如查看形状 进行线性代数运算

00:02:16.525 --> 00:02:19.560
因此 有必要了解张量是什么

00:02:19.560 --> 00:02:23.039
它们是如何穿过网络的

00:02:23.039 --> 00:02:27.944
我们可以开始学习如何使用 PyTorch 构建神经网络了

00:02:27.944 --> 00:02:32.375
但是首先要讲解下如何在 PyTorch 中处理张量

00:02:32.375 --> 00:02:37.185
我提到张量是 PyTorch 中的基本数据结构

00:02:37.185 --> 00:02:39.629
要构建神经网络

00:02:39.629 --> 00:02:42.134
很多时候都需要处理张量

00:02:42.134 --> 00:02:45.859
因此有必要理解张量的工作原理

00:02:45.860 --> 00:02:48.660
从概念上理解它们

00:02:48.659 --> 00:02:49.995
首先

00:02:49.995 --> 00:02:52.379
导入 NumPy 和 PyTorch

00:02:52.379 --> 00:02:57.044
导入 torch 以使用 PyTorch 模块

00:02:57.044 --> 00:03:01.625
首先我将创建一个随机张量

00:03:01.625 --> 00:03:04.694
可以看到这些随机数字 大小是 3x2

00:03:04.694 --> 00:03:11.055
然后输出另一个随机张量 大小一样

00:03:11.055 --> 00:03:14.305
要获得张量的大小 使用 x.size

00:03:14.305 --> 00:03:20.055
这行代码会输出大小为 3 x 2 的张量

00:03:20.055 --> 00:03:23.805
创建一个二维张量数组

00:03:23.805 --> 00:03:28.620
值全为 1 大小和 x 相同

00:03:28.620 --> 00:03:30.930
然后我们可以将二者相加

00:03:30.930 --> 00:03:35.925
基本上和 NumPy 的一样

00:03:35.925 --> 00:03:39.465
在很多方面 PyTorch 使用起来和 NumPy 都非常相似

00:03:39.465 --> 00:03:42.000
使学习过程变得轻松简单

00:03:42.000 --> 00:03:43.800
因为如果你有 NumPy 经验

00:03:43.800 --> 00:03:47.505
那么 PyTorch 用起来将很简单

00:03:47.504 --> 00:03:53.174
例如 你可以对张量使用索引

00:03:53.175 --> 00:03:55.320
这样会获得第一行

00:03:55.319 --> 00:03:58.275
还可以使用切片

00:03:58.275 --> 00:04:02.325
如果你想要第二列的所有行 可以这么操作

00:04:02.324 --> 00:04:07.034
PyTorch 中的张量有两种形式的方法

00:04:07.034 --> 00:04:13.155
一种形式是创建新的张量

00:04:13.155 --> 00:04:18.105
z.add 使张量 z 加 1

00:04:18.105 --> 00:04:21.290
得出这个结果 它会创建一个新的张量

00:04:21.290 --> 00:04:25.875
创建一个 z 的副本并使其加 1

00:04:25.875 --> 00:04:28.800
检查下 没错 这是之前的张量

00:04:28.800 --> 00:04:30.905
这是新建的张量

00:04:30.904 --> 00:04:37.514
但是 关于张量的几乎所有方法都存在原地版本

00:04:37.514 --> 00:04:42.044
意思是 如果我们再加 1

00:04:42.045 --> 00:04:44.020
似乎创建了新的张量

00:04:44.019 --> 00:04:46.870
但实际上更改了张量

00:04:46.870 --> 00:04:49.759
原地操作是指

00:04:49.759 --> 00:04:54.199
更改了内存中的值 更改了此张量的内存

00:04:54.199 --> 00:04:58.139
没有下划线的话 创建一个新的张量

00:04:58.139 --> 00:04:59.824
有下划线的话

00:04:59.824 --> 00:05:01.694
执行原地操作

00:05:01.694 --> 00:05:03.435
保持相同的张量

00:05:03.435 --> 00:05:06.949
但是更改了该张量在内存中指向的值

00:05:06.949 --> 00:05:10.550
你在 PyTorch 中经常会执行的操作是

00:05:10.550 --> 00:05:14.500
查看张量的大小和形状 然后调整张量的形状

00:05:14.500 --> 00:05:19.160
要检查张量的大小或形状

00:05:19.160 --> 00:05:23.200
使用 do.size 结果显示是 3 x 2 张量

00:05:23.199 --> 00:05:28.490
假设我们想将形状调整为 2 x 3

00:05:28.490 --> 00:05:32.730
输入 z.resize_(2,3)

00:05:32.730 --> 00:05:34.550
你会注意到 这里有个下划线

00:05:34.550 --> 00:05:35.870
表明这个方法

00:05:35.870 --> 00:05:38.000
是原地调整形状

00:05:38.000 --> 00:05:39.355
再次查看 z 会发现

00:05:39.355 --> 00:05:41.480
现在是一个 2 x 3 矩阵

00:05:41.480 --> 00:05:44.064
原来是 3 x 2 矩阵

00:05:44.064 --> 00:05:48.110
最后 我最喜欢的 PyTorch 功能之一是

00:05:48.110 --> 00:05:51.770
可以轻松地从 NumPy 数组转换为 Torch 张量

00:05:51.769 --> 00:05:53.069
这样的话

00:05:53.069 --> 00:05:54.599
处理数据将非常非常轻松

00:05:54.600 --> 00:05:55.620
因为大部分时间

00:05:55.620 --> 00:05:59.209
你都是预处理数据 这些操作可以在 NumPy 中完成

00:05:59.209 --> 00:06:02.769
然后在 PyTorch 中构建网络

00:06:02.769 --> 00:06:07.154
然后再回到 NumPy 中完成剩余分析步骤

00:06:07.154 --> 00:06:08.669
或创建图形

00:06:08.670 --> 00:06:12.495
或使用 NumPy 完成程序的剩余步骤

00:06:12.495 --> 00:06:19.064
首先 我将使用 NumPy 创建一个随机数组

00:06:19.064 --> 00:06:21.959
随机 4 x 3 数组 看起来这样

00:06:21.959 --> 00:06:26.399
要将此数组转换为 Torch 张量

00:06:26.399 --> 00:06:30.029
输入 from_numpy 然后传入该数组

00:06:30.029 --> 00:06:33.554
这样就会获得具有相同值的 PyTorch 张量

00:06:33.555 --> 00:06:37.560
如果想要将 PyTorch 张量

00:06:37.560 --> 00:06:40.240
变成 NumPy 数组

00:06:40.240 --> 00:06:44.860
输入 b 它是一个张量 然后输入 .numpy

00:06:44.860 --> 00:06:50.465
numpy 方法将返回一个 NumPy 数组

00:06:50.464 --> 00:06:56.199
注意 当你将 NumPy 数组变成张量时

00:06:56.199 --> 00:06:59.495
NumPy 数组和张量共享内存

00:06:59.495 --> 00:07:03.550
这意味着 如果原地更改张量的值

00:07:03.550 --> 00:07:06.370
也会更改 NumPy 数组的值

00:07:06.370 --> 00:07:12.259
例如 我们原地乘以 2 得出这个结果

00:07:12.259 --> 00:07:15.579
再回来看看 NumPy 数组 它也变了

00:07:15.579 --> 00:07:19.389
在使用该方法时 需要注意的是

00:07:19.389 --> 00:07:23.519
如果你要将 PyTorch 张量与 NumPy 数组相关联

00:07:23.519 --> 00:07:25.284
它们将共享相同的内存

00:07:25.285 --> 00:07:27.945
这一点要注意

00:07:27.944 --> 00:07:30.209
防止造成 bug

00:07:30.209 --> 00:07:32.399
下个视频见 加油

