WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:02.700
So, in this video,

00:00:02.700 --> 00:00:06.330
I'll be showing you how to use transfer learning to train a network

00:00:06.330 --> 00:00:10.785
that can properly classify those images of cats and dogs.

00:00:10.785 --> 00:00:13.679
What we'll do here is use a pre-trained network to

00:00:13.679 --> 00:00:16.989
actually detect and extract features from the images.

00:00:16.989 --> 00:00:18.869
This is really good for solving

00:00:18.870 --> 00:00:22.714
many challenging problems in computer vision and other fields.

00:00:22.714 --> 00:00:26.990
So, specifically, these networks have been trained on the ImageNet dataset,

00:00:26.989 --> 00:00:28.424
this is a giant dataset with

00:00:28.425 --> 00:00:32.009
over a million labeled images and a thousand different categories.

00:00:32.009 --> 00:00:36.914
These networks use an architecture called convolutional layers that exploit

00:00:36.914 --> 00:00:42.960
patterns and structure that we see in images and learn from that.

00:00:42.960 --> 00:00:48.649
These networks have dozens or even hundreds of different convolutional layers.

00:00:48.649 --> 00:00:52.589
They are extremely deep compared to what you've seen so far.

00:00:52.590 --> 00:00:55.760
This combination of a massive dataset with

00:00:55.759 --> 00:01:00.359
a massive neural network is come to be known as deep learning.

00:01:00.359 --> 00:01:04.689
What's really cool is that once these networks are trained on ImageNet,

00:01:04.689 --> 00:01:09.424
the models work amazingly well as feature detectors on images that aren't in ImageNet.

00:01:09.424 --> 00:01:11.674
So, you can, for instance,

00:01:11.674 --> 00:01:17.359
apply these networks to images of these cats and dogs and actually extract the features.

00:01:17.359 --> 00:01:21.349
Then you use those features as inputs to a new classifier which you can

00:01:21.349 --> 00:01:25.884
train to accurately classify your cat and dog photos.

00:01:25.885 --> 00:01:30.350
You can download these pre-trained networks using torchvision.models.

00:01:30.349 --> 00:01:33.979
So, we're going to include that in our imports.

00:01:33.980 --> 00:01:36.015
Otherwise is pretty much what you've seen before.

00:01:36.015 --> 00:01:38.224
Now we have our models.

00:01:38.224 --> 00:01:44.359
Most of these models from towards vision and also other deep learning frameworks

00:01:44.359 --> 00:01:49.799
like Cara's require these images to be 224 by 224.

00:01:49.799 --> 00:01:54.864
So, these are the size of the images that the networks should train on originally.

00:01:54.864 --> 00:02:00.144
You'll also need to measure the normalization that was used when the models were trained.

00:02:00.144 --> 00:02:03.319
These colored channels were normalized separately,

00:02:03.319 --> 00:02:06.544
so the red green and blue and the means for each of these channels

00:02:06.545 --> 00:02:10.784
are listed here as their standard deviations are listed here.

00:02:10.784 --> 00:02:16.030
So, I'll let you actually define the trained transforms.

00:02:16.030 --> 00:02:17.360
So, the transforms for

00:02:17.360 --> 00:02:22.315
the training data and the transforms for the test and validation data.

00:02:22.314 --> 00:02:26.800
So, you build your transforms like I showed you before and you should note

00:02:26.800 --> 00:02:31.830
that you need to make the images 224 by 224,

00:02:31.830 --> 00:02:34.785
you need to normalize them with these means and

00:02:34.784 --> 00:02:38.349
these standard deviations and you'd probably

00:02:38.349 --> 00:02:42.544
want to use data augmentation on the training images.

00:02:42.544 --> 00:02:44.584
As an example of transfer learning,

00:02:44.585 --> 00:02:47.564
I'm going show you how to do this with a model called DenseNet.

00:02:47.564 --> 00:02:50.324
So, we can load the model like so.

00:02:50.324 --> 00:02:54.149
So, model equals models.densenet.

00:02:54.879 --> 00:02:58.389
So, there's a few and typically this number

00:02:58.389 --> 00:03:01.539
afterwards tells you how many layers are in the network.

00:03:01.539 --> 00:03:04.655
So, in general, more layers means it's going to have better accuracy,

00:03:04.655 --> 00:03:06.430
it will just get this 121,

00:03:06.430 --> 00:03:09.375
we don't need crazy accuracy right now.

00:03:09.375 --> 00:03:12.319
So, you want say pre-trained equals true and so this will

00:03:12.319 --> 00:03:16.144
download the model to your computer if you don't already have it.

00:03:16.145 --> 00:03:19.025
So, we can look at the architecture.

00:03:19.025 --> 00:03:23.060
So, here we see this DenseNet architecture has a part

00:03:23.060 --> 00:03:27.034
called features that is built up of all these convolutional layers.

00:03:27.034 --> 00:03:32.715
As you scroll down very far because this is a very deep network, keep going, keep going,

00:03:32.715 --> 00:03:36.349
at the end we see apart called classifier

00:03:36.349 --> 00:03:39.500
which is just one linear transformation at the end as taking

00:03:39.500 --> 00:03:47.099
the features from the feature part and passing them to 1,000 output units.

00:03:47.099 --> 00:03:51.799
So, transfer learning the features part like all these layers up

00:03:51.800 --> 00:03:57.090
here above here work really well with other datasets.

00:03:57.090 --> 00:04:00.700
However, this classifier has a thousand outputs.

00:04:00.699 --> 00:04:06.060
So, this has been trained particularly like specifically for the ImageNet dataset.

00:04:06.060 --> 00:04:10.145
So, this part we're going to want to replace it

00:04:10.145 --> 00:04:14.844
with a new classifier that has built for our dataset.

00:04:14.844 --> 00:04:19.189
Then we will train this classifier on our dataset and

00:04:19.189 --> 00:04:24.029
just use the features that come into here as the input for this classifier.

00:04:24.029 --> 00:04:27.709
So, the first thing we want to do is actually freeze

00:04:27.709 --> 00:04:33.444
the parameters in the features section of DenseNet.

00:04:33.444 --> 00:04:38.139
So, we don't want to train or otherwise

00:04:38.139 --> 00:04:40.659
calculate the gradients for that part because we just

00:04:40.660 --> 00:04:43.450
want to keep it static and just use it as a feature detector.

00:04:43.449 --> 00:04:47.339
So, you can basically do for param and model.parameters.

00:04:47.339 --> 00:04:51.459
So, this is going to go through all the parameters in a model and then we can

00:04:51.459 --> 00:04:57.060
say requires grad equals false.

00:04:57.060 --> 00:05:00.790
So, this will shut off gradients like calculating gradients for other parameters in

00:05:00.790 --> 00:05:05.814
the model and we won't train them when we do the training steps.

00:05:05.814 --> 00:05:09.855
Now, we're just going to define our classifier.

00:05:09.855 --> 00:05:12.770
So, this is similarly to what we built before where

00:05:12.769 --> 00:05:15.740
you can just build a classifier as a sequential model.

00:05:15.740 --> 00:05:18.949
So, I'm just using an order dict here to get

00:05:18.949 --> 00:05:21.050
our first linear transformation into

00:05:21.050 --> 00:05:23.840
a revenue into another linear transformation for our output.

00:05:23.839 --> 00:05:27.149
So, we have two outputs here and one for cat and one for dog.

00:05:27.149 --> 00:05:32.199
The input features here for this classifier are coming from here.

00:05:32.199 --> 00:05:33.899
So, we can see that this classifier,

00:05:33.899 --> 00:05:36.810
the input features are 1,024.

00:05:36.810 --> 00:05:40.084
So, that means that the feature section of this network,

00:05:40.084 --> 00:05:44.724
the output of that is 1,024 values.

00:05:44.725 --> 00:05:47.230
So, we want to use that as the input for

00:05:47.230 --> 00:05:50.980
a classifier and so we have 1,024 values here as well.

00:05:50.980 --> 00:05:54.610
So, in general, you're going to need to match up the size of

00:05:54.610 --> 00:05:59.560
the output from the features part with the input of your classifier.

00:05:59.560 --> 00:06:02.300
Now that we have our classifier defined,

00:06:02.300 --> 00:06:09.920
we just need to replace the DenseNet classifier with our classifier.

00:06:10.170 --> 00:06:14.480
Now, this network is ready to be trained.

00:06:14.480 --> 00:06:18.730
The problem though is that now we're using a really deep neural network.

00:06:18.730 --> 00:06:23.855
So, like I said before this DenseNet has 121 different layers.

00:06:23.855 --> 00:06:27.900
So, if you're going try to train this on a CPU like we had before,

00:06:27.899 --> 00:06:29.779
it's going to take a very long time.

00:06:29.779 --> 00:06:33.879
So, instead, we're going to use the GPU to do the calculations.

00:06:33.879 --> 00:06:37.259
Using the GPU you can get speedups of like 100 times,

00:06:37.259 --> 00:06:40.319
500 times, it's pretty crazy.

00:06:40.319 --> 00:06:45.199
PyTorch like pretty much all the other frameworks uses library called

00:06:45.199 --> 00:06:50.800
CUDA to actually run our networks on the GPU.

00:06:50.800 --> 00:06:53.655
So, if you're wondering more about CUDA it's all here.

00:06:53.654 --> 00:06:59.719
CUDA is a software library that runs on NVIDIA GPUs and it uses

00:06:59.720 --> 00:07:05.860
these GPUs to efficiently compute linear algebra computations.

00:07:05.860 --> 00:07:09.545
So, remember that a lot of the stuff and deep learning networks is

00:07:09.545 --> 00:07:14.020
basically just linear algebra matrix multiplications and that sort of thing.

00:07:14.019 --> 00:07:18.604
These computations can be done really fast on a GPU using CUDA.

00:07:18.605 --> 00:07:21.725
So, the idea with PyTorch is that we move

00:07:21.725 --> 00:07:25.810
our model parameters and other tensors to the GPU using something

00:07:25.810 --> 00:07:33.899
like model.to(cuda) and we can move them back from the GPU to the CPU with model.to(CPU).

00:07:34.490 --> 00:07:38.915
So, I'm going to demonstrate how fast this actually goes by

00:07:38.915 --> 00:07:44.500
comparing how long it takes to do a forward and backward pass with and without a GPU.

00:07:44.500 --> 00:07:47.959
So, here is fairly normal training pass that you've seen

00:07:47.959 --> 00:07:51.423
before just to find my criterion to find my optimizer.

00:07:51.423 --> 00:07:53.779
But now, remember that we don't want to

00:07:53.779 --> 00:07:57.024
train the features part but we just want to train the classifier part.

00:07:57.024 --> 00:08:01.484
So, in optimizer I'm passing in model.classifier.parameters.

00:08:01.485 --> 00:08:05.389
Here I'm looping through using a device CPU and

00:08:05.389 --> 00:08:10.399
CUDA and to get our model to whatever device we're using you

00:08:10.399 --> 00:08:14.734
say model.to(device) and as usual we're getting

00:08:14.735 --> 00:08:19.800
our images and our labels from the training loader and to use CUDA,

00:08:19.800 --> 00:08:26.750
to use GPU needs to move them to CUDA or wherever.

00:08:26.750 --> 00:08:33.750
So, inputs.to(device), labels.to(device).

00:08:34.019 --> 00:08:36.559
So, when a device is CPU,

00:08:36.559 --> 00:08:37.879
it'll move them to the CPU.

00:08:37.879 --> 00:08:39.299
When a device is CUDA,

00:08:39.299 --> 00:08:41.699
it'll move them to our GPU.

00:08:41.700 --> 00:08:45.395
Otherwise, it's basically all you need to do to change

00:08:45.394 --> 00:08:49.370
your code for your networks to run on a GPU.

00:08:49.370 --> 00:08:55.565
So, we can see comparing writing this code on a CPU verse GPU,

00:08:55.565 --> 00:08:57.480
that we get a huge time-saver.

00:08:57.480 --> 00:09:04.115
So, for one batch it takes over five seconds on the CPU but nine milliseconds.

00:09:04.115 --> 00:09:07.134
So, this is just a massive improvement.

00:09:07.134 --> 00:09:09.950
We can write device agnostic code which will

00:09:09.950 --> 00:09:13.509
automatically use CUDA if it's available on your machine.

00:09:13.509 --> 00:09:18.500
It's basically you just do torch.device is CUDA if CUDA is just

00:09:18.500 --> 00:09:24.000
available and then otherwise use the CPU and this becomes this device variable.

00:09:24.000 --> 00:09:27.789
So, then anytime you have a tensor or a model,

00:09:27.789 --> 00:09:32.304
you just say tensor.to(device) or model.to(device).

00:09:32.304 --> 00:09:34.379
If it's already on the CPU,

00:09:34.379 --> 00:09:35.519
then it'll just stay on the CPU.

00:09:35.519 --> 00:09:38.674
If it's ready on the GPU it'll stay there but if it's not,

00:09:38.674 --> 00:09:40.620
it'll move to the appropriate one.

00:09:40.620 --> 00:09:44.585
From here I'm going to let you finish training the model and pretty much

00:09:44.585 --> 00:09:48.350
everything is exactly the same except that you're only training,

00:09:48.350 --> 00:09:51.950
the classifier which you've defined like

00:09:51.950 --> 00:09:57.330
this and you're using the GPU now. All right. Good luck.

