WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.100
大家好 在这个 notebook 中

00:00:02.100 --> 00:00:05.015
我将演示如何使用 PyTorch 训练神经网络

00:00:05.014 --> 00:00:09.375
在之前的视频中 我们构建了神经网络

00:00:09.375 --> 00:00:13.929
但是它无法判断这些图像中的数字是多少

00:00:13.929 --> 00:00:16.620
我们希望能够向某个函数中

00:00:16.620 --> 00:00:19.740
传入一个图像

00:00:19.739 --> 00:00:21.614
此函数将能够返回一个概率分布

00:00:21.614 --> 00:00:26.159
告诉我们此图像是 4

00:00:26.160 --> 00:00:28.199
我们要做的是训练神经网络

00:00:28.199 --> 00:00:32.219
逼近此函数

00:00:32.219 --> 00:00:34.600
方法是传入一个图像

00:00:34.600 --> 00:00:36.270
告诉它正确答案

00:00:36.270 --> 00:00:40.705
然后神经网络会学习如何逼近该函数

00:00:40.704 --> 00:00:45.224
这就是我们将在此 notebook 中实现的目标

00:00:45.225 --> 00:00:50.020
快速解释下训练的工作原理

00:00:50.020 --> 00:00:51.410
正如我刚才提到的

00:00:51.409 --> 00:00:57.049
我们传入一个图像并传入正确的类别

00:00:57.049 --> 00:00:58.320
即正确的标签

00:00:58.320 --> 00:01:03.799
然后我们衡量网络预测结果（即这部分）

00:01:03.799 --> 00:01:08.015
与图像真实标签之间的差别

00:01:08.015 --> 00:01:13.040
预测结果和真实标签之间的差别称为误差

00:01:13.040 --> 00:01:14.820
误差有不同的衡量方式

00:01:14.819 --> 00:01:17.259
例如 均方损失或交叉熵

00:01:17.260 --> 00:01:22.295
我们之前见过 并且你使用此损失更新了权重

00:01:22.295 --> 00:01:26.180
我们通过这个叫做梯度下降的迭代流程更新权重

00:01:26.180 --> 00:01:30.710
原理是计算损失函数的梯度

00:01:30.709 --> 00:01:33.140
即损失函数的斜率

00:01:33.140 --> 00:01:35.900
它始终指向变化最快的方向

00:01:35.900 --> 00:01:38.570
我们希望最小化损失

00:01:38.569 --> 00:01:42.269
这样就意味着预测结果与真实标签越来越接近

00:01:42.269 --> 00:01:46.640
因此使用梯度下降 因为它始终指向变化最快的方向

00:01:46.640 --> 00:01:51.769
使我们能够尽快抵达损失函数的底部

00:01:51.769 --> 00:01:54.839
对于多层级神经网络

00:01:54.840 --> 00:01:57.575
我们通过反向传播更新权重

00:01:57.575 --> 00:02:01.770
反向传播实际上是链式法则的应用

00:02:01.769 --> 00:02:05.569
也就是说 对于 W1 中的更改

00:02:05.569 --> 00:02:08.930
此更改将一直在网络中反向传播 从左到右

00:02:08.930 --> 00:02:13.120
一直到损失 即 L

00:02:13.120 --> 00:02:19.724
同样 相同的更改在网络中反向传播到权重 2W1

00:02:19.724 --> 00:02:22.909
要更新权重

00:02:22.909 --> 00:02:26.569
我们需要知道损失相对于权重的梯度

00:02:26.569 --> 00:02:33.319
为此 我们可以将这一系列运算中的每一步的梯度相乘

00:02:33.319 --> 00:02:38.254
在 PyTorch 中 首先我们需要定义损失函数

00:02:38.254 --> 00:02:42.509
通常损失会分配给叫做 criterion 的变量

00:02:42.509 --> 00:02:44.810
如果要使用 softmax 输出

00:02:44.810 --> 00:02:46.405
我们需要将损失（即 criterion）设为交叉熵

00:02:46.405 --> 00:02:51.909
可以通过 nn.CrossEntropyLoss 获得该损失

00:02:51.909 --> 00:02:55.979
稍后 输入网络的输出和真实标签（即目标）

00:02:55.979 --> 00:02:58.569
获得真实损失

00:02:58.569 --> 00:03:00.169
还需要优化器

00:03:00.169 --> 00:03:03.324
优化器会接受损失

00:03:03.324 --> 00:03:05.334
我们还需要优化器

00:03:05.335 --> 00:03:06.740
如果有损失

00:03:06.740 --> 00:03:09.735
我们可以计算梯度 获得这些梯度后

00:03:09.735 --> 00:03:14.870
优化器会使用这些梯度更新网络中的所有权重和参数

00:03:14.870 --> 00:03:19.355
对于优化器 我们将使用随机梯度下降 简称 SGD

00:03:19.354 --> 00:03:21.394
还有更高级的优化器 例如 Adam

00:03:21.395 --> 00:03:24.055
暂时不需要了解 Adam

00:03:24.055 --> 00:03:29.064
为了计算梯度 PyTorch 会使用一个叫做 autograd 的模块

00:03:29.064 --> 00:03:32.569
autograd 的作用是

00:03:32.569 --> 00:03:34.789
如果你有一个张量 告诉该张量它需要梯度

00:03:34.789 --> 00:03:38.439
autograd 会跟踪发生在该张量上的每个运算

00:03:38.439 --> 00:03:40.834
在运算结束时

00:03:40.834 --> 00:03:42.814
例如 z.backward

00:03:42.814 --> 00:03:47.210
然后 autograd 会反向传播所有运算

00:03:47.210 --> 00:03:48.520
它会跟踪所有这些运算

00:03:48.520 --> 00:03:50.350
知道所有这些运算的梯度函数

00:03:50.349 --> 00:03:52.534
我们可以反向传播

00:03:52.534 --> 00:03:55.604
最终计算所需的所有梯度

00:03:55.604 --> 00:03:57.424
我将演示下工作原理

00:03:57.425 --> 00:04:00.310
首先 正常地导入组件

00:04:00.310 --> 00:04:05.569
在这里创建一个随机张量

00:04:05.569 --> 00:04:09.025
大小是 2x2

00:04:09.025 --> 00:04:11.680
从正态分布里随机抽样

00:04:11.680 --> 00:04:16.280
输入 requires_grad=True

00:04:16.279 --> 00:04:19.809
这行代码告诉 PyTorch

00:04:19.810 --> 00:04:23.785
使用 autograd 跟踪发生在此张量上的所有运算

00:04:23.785 --> 00:04:27.010
最终 我们将能够计算它的梯度

00:04:27.009 --> 00:04:31.740
这就是该张量的样貌

00:04:31.740 --> 00:04:38.405
输入 y = x**2 print(y)

00:04:38.404 --> 00:04:41.189
对值求平方

00:04:41.189 --> 00:04:45.860
如果输入 print(y.grad_fn)

00:04:45.860 --> 00:04:51.475
它会显示为了获得 y 所执行的运算

00:04:51.475 --> 00:04:54.770
我们在这里对 x 求平方 创建了 y

00:04:54.769 --> 00:04:58.479
查看 y 的梯度函数 这里是求幂

00:04:58.480 --> 00:05:01.165
计算 x 的二次幂

00:05:01.165 --> 00:05:04.400
最终 当我们反向通过这些运算时

00:05:04.399 --> 00:05:08.524
将使用这个梯度函数计算 y 的梯度

00:05:08.524 --> 00:05:12.234
现在 如果计算 y 的均值

00:05:12.235 --> 00:05:16.985
输出结果 是一个数字

00:05:16.985 --> 00:05:22.000
我们可以检查 x 和 y 的梯度 但是什么也没有

00:05:22.000 --> 00:05:24.740
什么也没有的原因是

00:05:24.740 --> 00:05:27.750
我们尚未反向传播这些运算

00:05:27.750 --> 00:05:32.779
我们尚未要求 PyTorch 或 Autograd 计算梯度

00:05:32.779 --> 00:05:35.229
要计算梯度

00:05:35.230 --> 00:05:38.455
我们需要反向通过这些运算

00:05:38.454 --> 00:05:43.889
例如 如果要计算 z 相对于 x 的梯度

00:05:43.889 --> 00:05:53.509
输入 z.backward() 它会计算 z 相对于 x 的梯度

00:05:53.509 --> 00:05:57.810
输入 print 输出这个

00:05:57.810 --> 00:06:01.129
根据我们执行的运算

00:06:01.129 --> 00:06:06.584
z 相对于 x 的梯度应该是 x/2

00:06:06.584 --> 00:06:08.739
再次输出结果

00:06:08.740 --> 00:06:13.340
获得了梯度 和 x/2 一样

00:06:13.339 --> 00:06:16.810
计算梯度后的结果和预期一样

00:06:16.810 --> 00:06:19.509
现在我们将获取数据并构建网络

00:06:19.509 --> 00:06:22.894
和在上一部分执行的操作一样

00:06:22.894 --> 00:06:27.324
我们已经有了模型 现在将训练该模型

00:06:27.324 --> 00:06:29.064
和之前提到的一样

00:06:29.064 --> 00:06:32.915
我们需要定义损失函数 criterion

00:06:32.915 --> 00:06:43.035
输入 criterion = nn.CrossEntropyLoss 然后定义优化器

00:06:43.035 --> 00:06:47.655
输入 optimizer = optim.SGD

00:06:47.654 --> 00:06:49.179
对于优化器来说

00:06:49.180 --> 00:06:52.180
我们需要传入想要优化的参数

00:06:52.180 --> 00:06:55.894
在此示例中 我们想要优化模型的所有参数

00:06:55.894 --> 00:06:59.649
因此输入 model.parameters() 然后设置学习速率

00:06:59.649 --> 00:07:02.419
在此示例中 将学习速率设为 0.01

00:07:02.420 --> 00:07:06.400
对于 PyTorch 来说 一般步骤是

00:07:06.399 --> 00:07:09.929
对网络进行前向传播以获得输出

00:07:09.930 --> 00:07:11.449
例如 logit 或 softmax

00:07:11.449 --> 00:07:13.149
或其他网络输出

00:07:13.149 --> 00:07:16.399
我们将使用该输出计算损失

00:07:16.399 --> 00:07:17.964
获得损失后

00:07:17.964 --> 00:07:21.619
可以使用 loss.backward 反向通过网络

00:07:21.620 --> 00:07:25.740
这样会计算网络中所有参数的梯度

00:07:25.740 --> 00:07:30.420
获得梯度后 可以使用优化器更新权重

00:07:30.420 --> 00:07:35.615
我将输出训练网络之前的权重

00:07:35.615 --> 00:07:41.090
输入 model.fc1.weight

00:07:41.089 --> 00:07:45.379
然后正常地获取图像和标签

00:07:45.379 --> 00:07:52.379
但是首先需要调用 optimizer.zero_grad()

00:07:53.410 --> 00:08:00.220
这行代码的作用是使张量上的所有梯度

00:08:00.220 --> 00:08:02.665
被训练的权重和偏差归零

00:08:02.665 --> 00:08:05.720
需要这么做的原因是

00:08:05.720 --> 00:08:08.900
每次反向传播时 例如 loss.backward

00:08:08.899 --> 00:08:11.324
会积累梯度

00:08:11.324 --> 00:08:13.594
意味着如果反向传播两次

00:08:13.595 --> 00:08:16.010
会再次加上这两个梯度运算

00:08:16.009 --> 00:08:17.920
如果反向传播第三次 则加上这三个梯度运算

00:08:17.920 --> 00:08:21.970
如果反向传播第四次 则加上这四个梯度运算

00:08:21.970 --> 00:08:23.100
总之 你不希望出现这种现象

00:08:23.100 --> 00:08:26.095
只希望传播一次 获得梯度

00:08:26.095 --> 00:08:28.525
使用这些梯度训练网络 然后下次传播时

00:08:28.524 --> 00:08:31.089
计算新的梯度并使用这些梯度训练网络

00:08:31.089 --> 00:08:35.929
如果不归零的话 则会累积起来

00:08:35.929 --> 00:08:40.384
使多次训练传播和多个批次的梯度累积起来

00:08:40.384 --> 00:08:43.189
在训练传播中的第一件事

00:08:43.190 --> 00:08:47.440
通常是确保归零梯度

00:08:47.779 --> 00:08:52.034
现在可以前向传播了

00:08:52.034 --> 00:08:56.904
输入 output = model.forward

00:08:56.904 --> 00:08:59.074
传入图像

00:08:59.075 --> 00:09:01.785
使用我们的 criterion 计算损失

00:09:01.784 --> 00:09:05.769
在这里传入输出和真实标签

00:09:05.769 --> 00:09:08.990
然后反向传播

00:09:08.990 --> 00:09:13.044
它将计算所有参数的梯度

00:09:13.044 --> 00:09:16.735
在这里输出梯度

00:09:16.735 --> 00:09:19.095
对于梯度

00:09:19.095 --> 00:09:23.000
我们可以执行优化步骤 更新权重

00:09:23.000 --> 00:09:25.109
这是训练之前的样子

00:09:25.109 --> 00:09:26.259
这是梯度

00:09:26.259 --> 00:09:29.840
这是更新后的权重

00:09:29.840 --> 00:09:32.555
也就是获取梯度

00:09:32.554 --> 00:09:35.529
使用某个学习速率加到现有参数上

00:09:35.529 --> 00:09:37.414
即权重上

00:09:37.414 --> 00:09:39.724
获得更新后的权重

00:09:39.725 --> 00:09:43.129
这部分代码

00:09:43.129 --> 00:09:46.450
这整个单元格就是一个基本训练传播

00:09:46.450 --> 00:09:50.170
我们从 trainloader 获得图像和标签

00:09:50.169 --> 00:09:51.714
trainloader 会加载数据

00:09:51.715 --> 00:09:57.075
然后使数据传入模型中 计算损失

00:09:57.075 --> 00:10:01.370
对网络反向传播

00:10:01.370 --> 00:10:05.185
执行优化步骤

00:10:05.184 --> 00:10:07.649
可以将这整个步骤封装到 for 循环里

00:10:07.649 --> 00:10:10.799
遍历所有的图像和标签

00:10:10.799 --> 00:10:12.979
即数据集中的所有数据

00:10:12.980 --> 00:10:16.904
更新网络 这就是网络的训练方式

00:10:16.904 --> 00:10:20.679
现在 我们可以将这个训练方法放入循环中

00:10:20.679 --> 00:10:26.429
采用的方式是定义要训练的周期数量

00:10:26.429 --> 00:10:29.449
一个周期是

00:10:29.450 --> 00:10:31.379
经过整个数据集一次

00:10:31.379 --> 00:10:34.389
三个周期是经过整个数据集三次

00:10:34.389 --> 00:10:37.669
定义一些要使用的参数

00:10:37.669 --> 00:10:41.183
输入  

00:10:41.183 --> 00:10:45.039
for e in range(epochs)

00:10:45.039 --> 00:10:47.179
定义 running_loss

00:10:47.179 --> 00:10:51.889
训练过程中 我们将输出损失

00:10:51.889 --> 00:10:56.610
以便检查训练损失（网络损失）是否真的下降了

00:10:56.610 --> 00:11:00.389
当网络不断学习时 预测会越来越准确

00:11:00.389 --> 00:11:02.299
损失将下降

00:11:02.299 --> 00:11:05.234
我们希望看到训练过程中出现这一效果

00:11:05.235 --> 00:11:10.685
从 trainloader 获取图像和标签

00:11:10.684 --> 00:11:14.789
跟踪时间步

00:11:14.789 --> 00:11:17.615
即训练了多少次

00:11:17.615 --> 00:11:21.585
再次输入 images.resize

00:11:21.585 --> 00:11:24.040
现在是训练传播步骤

00:11:24.039 --> 00:11:26.649
输入 optimizer.zero_grad()

00:11:26.649 --> 00:11:28.379
前向传播

00:11:28.379 --> 00:11:34.080
输入 output = model.forward loss = criterion

00:11:34.080 --> 00:11:39.025
反向传播 然后是更新权重步骤

00:11:39.024 --> 00:11:46.429
这里 损失只是一个数字 也就是它是一个标量张量

00:11:46.429 --> 00:11:52.629
要从标量张量中获取这个数字 与这个 0 相加

00:11:52.629 --> 00:11:55.620
我们需要执行 loss.item

00:11:55.700 --> 00:12:05.320
然后多次输出训练损失

00:12:05.320 --> 00:12:09.260
好了 往下滚动

00:12:09.259 --> 00:12:13.980
可以看出当网络不断训练时 训练损失一直在下降 这是个好现象

00:12:13.980 --> 00:12:16.370
训练好网络后

00:12:16.370 --> 00:12:18.965
可以看看它的性能如何

00:12:18.965 --> 00:12:22.820
我们向网络中传入数字 6

00:12:22.820 --> 00:12:26.750
网络知道这是数字 6

00:12:26.750 --> 00:12:30.879
传入不同的数字 它知道 2 是 2

00:12:30.879 --> 00:12:34.924
知道 3 是 3  很酷

00:12:34.924 --> 00:12:36.924
借助反向传播

00:12:36.924 --> 00:12:43.194
我们能够训练神经网络识别这些手写数字图像

00:12:43.195 --> 00:12:46.930
这对扫描文档来说很有用 对吧

00:12:46.929 --> 00:12:50.329
可以想象 对文档拍摄大量照片

00:12:50.330 --> 00:12:51.625
扫描文档

00:12:51.625 --> 00:12:55.399
然后将图像传入网络中

00:12:55.399 --> 00:12:57.709
网络能够根据文档图像

00:12:57.710 --> 00:13:02.500
告诉计算机文档中的内容是什么

00:13:02.500 --> 00:13:05.720
在下个部分 你将能够再次构建你自己的网络

00:13:05.720 --> 00:13:09.399
并且这次能够训练网络 加油

