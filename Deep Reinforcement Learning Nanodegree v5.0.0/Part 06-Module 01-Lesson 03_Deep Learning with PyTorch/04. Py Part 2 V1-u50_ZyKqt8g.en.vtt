WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.879
Hello everyone and welcome back.So,in this video,

00:00:02.879 --> 00:00:07.580
I'm going to be showing you how you actually build neural networks with PyTorch.

00:00:07.580 --> 00:00:09.599
At the end of this notebook which I'll provide for

00:00:09.599 --> 00:00:11.949
you you'll build your own neural network.

00:00:11.949 --> 00:00:13.259
So, let's get started.

00:00:13.259 --> 00:00:16.214
So, the first step we import things like Normal,

00:00:16.214 --> 00:00:19.245
to import PyTorch we do import torch.

00:00:19.245 --> 00:00:21.179
We're also going to import this package called

00:00:21.179 --> 00:00:23.439
torchvision which is associated with PyTorch

00:00:23.440 --> 00:00:28.530
and it allows us to download and use some existing datasets.

00:00:28.530 --> 00:00:31.425
The dataset that we're going to use for this is called MNIST.

00:00:31.425 --> 00:00:35.600
The MNIST dataset is just a bunch of images of hand drawn digits.

00:00:35.600 --> 00:00:37.795
So, 0 through 9,

00:00:37.795 --> 00:00:42.234
this dataset is used to train a network or other machine learning models

00:00:42.234 --> 00:00:47.240
such they can classify the images into those details, right?

00:00:47.240 --> 00:00:53.015
So then an image that shows at eight will be classified as the digit 8.

00:00:53.015 --> 00:00:56.439
That's what we'll be using for this example.

00:00:56.439 --> 00:01:01.724
So, here I'm using torchvision to download and load the training data.

00:01:01.725 --> 00:01:07.344
So, this is the MNIST dataset that we can grab and put it into the trainset.

00:01:07.344 --> 00:01:08.939
So here I'm saying download equals true,

00:01:08.939 --> 00:01:11.694
so if it doesn't exist on disk already,

00:01:11.694 --> 00:01:13.649
then it will download it for us.

00:01:13.650 --> 00:01:16.264
I'm also providing a transform.

00:01:16.263 --> 00:01:19.879
So, what this does is it basically reads in these images and then applies

00:01:19.879 --> 00:01:24.864
these transforms to give us our dataset that we feed into our network.

00:01:24.864 --> 00:01:29.449
So, as it transform it looks like this so basically the first thing we do is

00:01:29.450 --> 00:01:35.900
transform these images into a PyTorch tensor and then we normalize them.

00:01:35.900 --> 00:01:41.180
So, what normalizing means in this case is that we want

00:01:41.180 --> 00:01:47.110
to take our images which are grayscale images and each value,

00:01:47.109 --> 00:01:50.599
each pixel is a float from zero to one but we actually

00:01:50.599 --> 00:01:55.179
want our pixels to be from negative one to one.

00:01:55.180 --> 00:01:58.750
So, this is what this does is it subtracts 0.5,

00:01:58.750 --> 00:02:00.105
so this is the mean,

00:02:00.105 --> 00:02:02.359
so it will subtract from each color chamber,

00:02:02.359 --> 00:02:05.935
from each pixel, and then it's going to divide by 0.5.

00:02:05.935 --> 00:02:10.015
So, what this does is it moves this range of zero to one to

00:02:10.014 --> 00:02:14.474
negative half to one half and then you divide by 0.5 or you multiply it by two,

00:02:14.474 --> 00:02:19.324
so that stretches out the variance to minus one to one.

00:02:19.324 --> 00:02:22.774
So, this just makes it easier for our neural networks to learn and you'll

00:02:22.775 --> 00:02:26.840
pretty much always want to normalize your data going into a neural network.

00:02:26.840 --> 00:02:28.849
The other thing to note is that,

00:02:28.849 --> 00:02:31.314
here I've set a batch size of 64.

00:02:31.314 --> 00:02:34.789
So, that basically means when we get our data out of trainloader here,

00:02:34.789 --> 00:02:37.759
is going to give us 64 images at a time.

00:02:37.759 --> 00:02:39.794
Okay. So let me go and load this,

00:02:39.794 --> 00:02:45.694
then load our data and now we can see what one of these images looks like.

00:02:45.694 --> 00:02:48.799
So, here is one example and this is number seven.

00:02:48.800 --> 00:02:51.340
So this is just a hand-drawn digit that we

00:02:51.340 --> 00:02:54.245
are going to be classifying with our neural network.

00:02:54.245 --> 00:02:59.634
So, here is the network that we'll be building as example with MNIST.

00:02:59.634 --> 00:03:03.125
So,MNIST, the images are 28 by 28,

00:03:03.125 --> 00:03:07.240
and so what we actually want to do is transform them

00:03:07.240 --> 00:03:11.590
into a vector that's 28 times 28 units long.

00:03:11.590 --> 00:03:14.860
So 784, which is 28 times 28.

00:03:14.860 --> 00:03:16.380
So this is the size of our input layer.

00:03:16.379 --> 00:03:18.384
So we're going to take our images, we convert them into

00:03:18.384 --> 00:03:21.509
a vector and then pass them in here as our inputs.

00:03:21.509 --> 00:03:24.444
We're going to use two hidden layers,

00:03:24.444 --> 00:03:27.824
the first with 128 units and the second was 64 units,

00:03:27.824 --> 00:03:30.639
this is going to go to an output layer with 10 units.

00:03:30.639 --> 00:03:33.379
This output layer has 10 units because we want

00:03:33.379 --> 00:03:36.009
to classify these digits and there's 10 of them, right?

00:03:36.009 --> 00:03:40.649
So this one correspond to the digit 0,1,2 and so on.

00:03:40.650 --> 00:03:44.689
Here the number of units in each of these hidden layers and

00:03:44.689 --> 00:03:49.120
the number of hidden layers you actually use is somewhat arbitrary.

00:03:49.120 --> 00:03:53.620
In general, the more units you have on a layer and the more layers you have,

00:03:53.620 --> 00:03:56.719
the better your network will be able to fit the data.

00:03:56.719 --> 00:04:00.500
The large part of training neural networks is actually finding

00:04:00.500 --> 00:04:05.294
the best number of units and the best number of layers to using your network.

00:04:05.294 --> 00:04:08.149
For activation functions, for the hidden layers

00:04:08.150 --> 00:04:10.740
are going to be using relus and for the output,

00:04:10.740 --> 00:04:12.064
we're going to be using softmax.

00:04:12.064 --> 00:04:15.449
So, remember that softmax actually takes

00:04:15.449 --> 00:04:23.560
the values of these output units and transforms them into a probability distribution.

00:04:23.560 --> 00:04:27.079
So, what that means is that it actually squishes

00:04:27.079 --> 00:04:31.350
all these values between zero and one but then divides by the total things,

00:04:31.350 --> 00:04:34.620
so, if you sum up the values for each of these units it equals one.

00:04:34.620 --> 00:04:39.170
So, this is a discrete probability distribution that gives us

00:04:39.170 --> 00:04:44.210
the probability that whatever our input is it belongs to this class.

00:04:44.209 --> 00:04:45.734
And to train this network,

00:04:45.735 --> 00:04:49.595
we're going to need a loss and so we're using a cross entropy loss.

00:04:49.595 --> 00:04:53.600
What this does is compare the probability,

00:04:53.600 --> 00:04:56.300
like the prediction from the softmax layer with

00:04:56.300 --> 00:05:00.180
the true category and then you can use the difference,

00:05:00.180 --> 00:05:03.550
that loss there to update the weights in your network.

00:05:03.550 --> 00:05:06.189
Okay, so it's time to build this network.

00:05:06.189 --> 00:05:13.170
So, firstly, we need to import a few modules from PyTorch.

00:05:13.170 --> 00:05:17.060
So, from torch import nn,

00:05:17.060 --> 00:05:20.139
this is like neural network functions.

00:05:20.139 --> 00:05:26.495
We can also import functional as F. So,

00:05:26.495 --> 00:05:29.230
this is more functions that are

00:05:29.230 --> 00:05:33.390
specifically around neural networks and this is just like a functional form of them.

00:05:33.389 --> 00:05:36.294
With those imported, we can start building our network.

00:05:36.295 --> 00:05:38.585
So, the kind of general,

00:05:38.584 --> 00:05:43.094
most basic way you do this in PyTorch is by creating a class.

00:05:43.095 --> 00:05:49.295
It doesn't really matter because those class network that is subclass from nn.Module.

00:05:49.295 --> 00:05:52.020
Then in the init function,

00:05:52.019 --> 00:05:54.329
you first need to call super.

00:05:54.329 --> 00:05:56.449
So, basically what super does is

00:05:56.449 --> 00:06:03.594
a calls functions or attributes of whatever class that this is subclassing from.

00:06:03.595 --> 00:06:06.745
So, what this line is going to do is it is actually going to call

00:06:06.745 --> 00:06:10.569
the init method of an nn.Module.

00:06:10.569 --> 00:06:13.985
So that done, we're going to define the layers we are using,

00:06:13.985 --> 00:06:19.045
the operations that will define the architecture of our neural network.

00:06:19.045 --> 00:06:21.300
To do this, we'll call it self.fc,

00:06:21.300 --> 00:06:22.889
so a fully connected layer.

00:06:22.889 --> 00:06:26.389
So fc is linear.

00:06:26.389 --> 00:06:28.680
So this is, if you look at this,

00:06:28.680 --> 00:06:32.855
this is applies that linear transformation to an incoming data.

00:06:32.855 --> 00:06:36.379
So, this we need to put in our input,

00:06:36.379 --> 00:06:39.415
size, and then the output size.

00:06:39.415 --> 00:06:42.340
So, we know that we have 784 units in

00:06:42.339 --> 00:06:46.264
our input layer and we want our first hidden layer to be 128 units.

00:06:46.264 --> 00:06:51.245
And for the second layer we can do something similar, 128 to 64.

00:06:51.245 --> 00:06:55.084
Then the output layer call fc3.

00:06:55.084 --> 00:07:00.489
So, this is going from 64 to 10.

00:07:00.490 --> 00:07:04.590
That's pretty much all the operations that we need to define in this part.

00:07:04.589 --> 00:07:08.049
So, to get this to work as a neural network,

00:07:08.050 --> 00:07:11.915
we need to define a forward function.

00:07:11.915 --> 00:07:15.310
So, all PyTorch networks that you

00:07:15.310 --> 00:07:20.180
build from nn.Module need to have this forward function.

00:07:20.180 --> 00:07:26.030
So, what this is expecting is that x is a PyTorch tensor and

00:07:26.029 --> 00:07:28.549
then what you're going to do is basically just pass this tensor

00:07:28.550 --> 00:07:31.305
through each of your layers in each of your operations.

00:07:31.305 --> 00:07:37.250
So for example, to do the first layer, so self.fc1 x.

00:07:37.250 --> 00:07:42.350
So, this is passing the tensor x through our first layer,

00:07:42.350 --> 00:07:46.470
through this first linear operation and that gives us another tensor.

00:07:46.470 --> 00:07:50.390
Then we can take that tensor and put

00:07:50.389 --> 00:07:55.209
the relu activation function on it and then go to the next one.

00:07:55.209 --> 00:07:57.734
So this kind of keep going like this.

00:07:57.735 --> 00:08:02.670
Finally we do our softmax function.

00:08:02.670 --> 00:08:06.280
So,here we actually need to define the dimension that

00:08:06.279 --> 00:08:09.224
we want to calculate the softmax across.

00:08:09.225 --> 00:08:13.540
So, if you remember earlier I said that our images are actually going

00:08:13.540 --> 00:08:18.500
to be batches and there's going to be 64 different images per batch,

00:08:18.500 --> 00:08:23.420
and the way that looks with the tensor size is at the batch size is first.

00:08:23.420 --> 00:08:25.395
So, that would be dimension zero.

00:08:25.394 --> 00:08:31.299
Then our actual vector that we're passing through the network is the second dimension.

00:08:31.300 --> 00:08:38.085
So, this tensor here is going to end up being 64 which is the batch size by 10.

00:08:38.085 --> 00:08:43.740
So, we actually want to take our softmax across the dimension that is has 10 values,

00:08:43.740 --> 00:08:45.375
and so that's dimension one here.

00:08:45.375 --> 00:08:47.730
And finally, you just return this.

00:08:47.730 --> 00:08:49.909
So, we can create our model,

00:08:49.909 --> 00:08:53.894
our network like this and then see what it looks like.

00:08:53.894 --> 00:08:57.159
So, this basically will just tell us like all these modules,

00:08:57.159 --> 00:09:01.904
these operations that we registered in the init function.

00:09:01.904 --> 00:09:06.954
So, we create these linear operations, these linear transformations.

00:09:06.955 --> 00:09:10.285
The weights and biases are automatically created for us.

00:09:10.284 --> 00:09:16.794
So, we can see that with model.fc1.weight, and I'll print this out.

00:09:16.794 --> 00:09:22.474
Then also print fc1.bias.

00:09:22.475 --> 00:09:25.250
So, we can see we have these parameters

00:09:25.250 --> 00:09:29.815
here that have automatically been created and initialized for us.

00:09:29.815 --> 00:09:35.665
So, if we want to go in and reinitialize these parameters in bias in weights,

00:09:35.664 --> 00:09:39.855
then we can get to our bias like this bias.

00:09:39.855 --> 00:09:41.889
Then to get the underlying data,

00:09:41.889 --> 00:09:47.595
you say.data and now we can fill it in place with zeros.

00:09:47.595 --> 00:09:50.800
So, now our bias that's attached to this linear operation in

00:09:50.799 --> 00:09:54.620
the first fully connected layer is filled with zeros.

00:09:54.620 --> 00:09:59.080
Similarly, you can look at your weights.data,

00:09:59.090 --> 00:10:04.100
and suppose we want to do a normal distribution.

00:10:04.100 --> 00:10:07.555
So, we want to initialize these random weights

00:10:07.554 --> 00:10:14.309
with a normal distribution with a standard deviation of 0.01.

00:10:15.120 --> 00:10:19.164
There we go. So, it's really convenient because basically

00:10:19.164 --> 00:10:23.605
the bias and weight matrices are always available to you in your model.

00:10:23.605 --> 00:10:25.470
So, you just do model.whatever layer,

00:10:25.470 --> 00:10:29.490
whatever operation and then.weight and that you can get your weights out.

00:10:29.490 --> 00:10:31.480
Now that we have a network, I'm going to pass

00:10:31.480 --> 00:10:34.164
some data through it and see what it looks like.

00:10:34.164 --> 00:10:37.549
We get our data from the train loader and what it does is it

00:10:37.549 --> 00:10:40.519
returns images and then labels for each of those images.

00:10:40.519 --> 00:10:43.840
So, labels are the categories that the images belong to.

00:10:43.840 --> 00:10:46.610
So, if there's an image of a zero then the label is zero,

00:10:46.610 --> 00:10:49.610
if there's an image of a five the label is five and so on.

00:10:49.610 --> 00:10:55.134
So, we do is we call next iter trainloader.

00:10:55.134 --> 00:10:58.710
So, train loader returns a generator.

00:10:58.710 --> 00:11:02.065
To make it something that you can actually iterate over,

00:11:02.065 --> 00:11:06.455
you do iter and to get the first value of it, you do next.

00:11:06.455 --> 00:11:08.360
So, basically every time you call next on this,

00:11:08.360 --> 00:11:13.430
it's going to give you the next batch in your training data.

00:11:13.429 --> 00:11:17.779
With images, we actually need to resize them now because remember they

00:11:17.779 --> 00:11:22.089
come as 28 by 28 images and we need to make them a 784 long vector.

00:11:22.090 --> 00:11:28.019
So, say images.resize our batch size.

00:11:28.019 --> 00:11:33.824
This is like the number of color channels for instance and then 784.

00:11:33.825 --> 00:11:37.855
A somewhat better way to do this is if you actually pass in

00:11:37.855 --> 00:11:41.779
the batch size that you get from the images tensor itself.

00:11:41.779 --> 00:11:45.214
So, this is the first element of the shape.

00:11:45.215 --> 00:11:47.820
You won't do this because a lot of times you don't actually

00:11:47.820 --> 00:11:50.754
know what the batch size is going to be beforehand.

00:11:50.754 --> 00:11:53.929
So, in general, you don't want to hard code that

00:11:53.929 --> 00:11:57.500
into your network or your tensors or anywhere.

00:11:57.500 --> 00:11:59.664
So, you do something like that.

00:11:59.664 --> 00:12:01.959
With the images resized,

00:12:01.960 --> 00:12:03.750
we can now pass it through our network.

00:12:03.750 --> 00:12:06.639
So to do this, you say model.forward.

00:12:06.639 --> 00:12:07.970
I'm going to put this into ps,

00:12:07.970 --> 00:12:10.125
so this just stands for probabilities.

00:12:10.125 --> 00:12:12.475
So, forward pass our images,

00:12:12.475 --> 00:12:14.409
we're just going to take the first image.

00:12:14.409 --> 00:12:16.839
That'll calculate our probabilities.

00:12:16.840 --> 00:12:21.310
So now, I'm going to use dysfunction and the helper file

00:12:21.309 --> 00:12:25.699
so helpers at the file I wrote that sits next to this and makes things easier and nicer.

00:12:25.700 --> 00:12:31.210
So, I want to look at what our image is and then how our network classified it.

00:12:31.210 --> 00:12:33.884
So, what is its prediction for the class.

00:12:33.884 --> 00:12:38.220
So, here we're going to use image is zero.

00:12:38.220 --> 00:12:43.180
So this time, we actually need to convert it back to a 28 by 28 image.

00:12:43.179 --> 00:12:44.849
So, I'm going to use view here.

00:12:44.850 --> 00:12:51.925
So, view is basically like resizing except that it returns a new tensor.

00:12:51.924 --> 00:12:55.574
So, resize it has an underscore at the end which means it's done in place,

00:12:55.575 --> 00:12:57.600
but view is the same as resizing,

00:12:57.600 --> 00:13:00.045
reshaping except it returns a new tensor.

00:13:00.044 --> 00:13:02.659
So, we can pass in our image and when you pass

00:13:02.659 --> 00:13:05.839
in probability distribution. I typed that wrong.

00:13:05.840 --> 00:13:07.384
So, you can see here,

00:13:07.384 --> 00:13:10.960
the image that we passed through our network is a seven and it's trying to make

00:13:10.960 --> 00:13:14.530
some predictions but since the weights are random, we haven't trained it,

00:13:14.529 --> 00:13:17.949
the network has no idea what the digit actually is, and so,

00:13:17.950 --> 00:13:20.440
it's just giving us a random guesses where

00:13:20.440 --> 00:13:23.380
like all the probabilities are roughly the same.

00:13:23.379 --> 00:13:27.360
So, it's a uniform distribution right now because we haven't trained it yet.

00:13:27.360 --> 00:13:34.355
PyTorch provides a more convenient way for building a model called nn.Sequential.

00:13:34.355 --> 00:13:37.254
So, I'll show you how to do this.

00:13:37.254 --> 00:13:40.759
First is want to define our hyper-parameters.

00:13:40.759 --> 00:13:45.754
So, hyper parameters are parameters that define the architecture of your network,

00:13:45.754 --> 00:13:48.985
and the parameters are the weights and biases themselves.

00:13:48.985 --> 00:13:52.925
So, for instance, your input size is going to be 784.

00:13:52.924 --> 00:13:55.849
We can define the size of our hidden layers.

00:13:55.850 --> 00:14:03.264
So, this is would be 128 and 64 and then we can define the output size, is equal to 10.

00:14:03.264 --> 00:14:06.754
So with nn.Sequential we can say our model is this.

00:14:06.754 --> 00:14:10.000
Then basically, you just pass in the operations,

00:14:10.000 --> 00:14:12.120
the transformations, modules that you want to do.

00:14:12.120 --> 00:14:20.970
So, an nn.Linear input size and then hidden size zero. So that's the first module.

00:14:20.970 --> 00:14:24.335
Then we know we want this to go through yellow activation.

00:14:24.335 --> 00:14:33.730
So, nn.ReLU, and then the second Linear transformation and then another ReLU,

00:14:33.730 --> 00:14:40.394
and finally, our output and then the softmax.

00:14:40.394 --> 00:14:42.475
Then we print out the model,

00:14:42.475 --> 00:14:47.055
we see it's a sequential model and then we have this linear transformation,

00:14:47.054 --> 00:14:48.875
ReLU, linear transformation, ReLU,

00:14:48.875 --> 00:14:50.820
linear and then soft max.

00:14:50.820 --> 00:14:54.770
This is practically the same as the network that we built before,

00:14:54.769 --> 00:14:58.345
except that it's much simpler to do and fewer lines of code.

00:14:58.345 --> 00:15:02.825
Now we can do a forward pass through this and see that it's the same.

00:15:02.825 --> 00:15:04.265
So, we put another seven.

00:15:04.264 --> 00:15:06.009
Since our network hasn't trained yet,

00:15:06.009 --> 00:15:09.534
it doesn't know what the seven actually is.

00:15:09.534 --> 00:15:14.615
Another cool thing you can do sequential is you can actually pass in in order to dict,

00:15:14.615 --> 00:15:16.269
in order to dictionary,

00:15:16.269 --> 00:15:21.475
to name each of the layers in your sequential model.

00:15:21.475 --> 00:15:23.370
So, to get an order dict,

00:15:23.370 --> 00:15:27.794
do from collections "import OrderedDict.

00:15:27.794 --> 00:15:32.615
Then we can build our model sequential.

00:15:32.615 --> 00:15:41.685
Then OrderedDict takes in a list of tuples that builds the dictionary keys and values.

00:15:41.684 --> 00:15:44.234
So here, the keys are going to be the names of

00:15:44.235 --> 00:15:48.269
our layers and the values are going to be the operations themselves.

00:15:48.269 --> 00:15:52.289
So, we can name our first layer FC1 for

00:15:52.289 --> 00:15:57.414
instance and given our normal linear thing and felt the rest of the network.

00:15:57.414 --> 00:16:01.750
So, we call this one FC1 and then our first ReLU is relu1,

00:16:01.750 --> 00:16:04.568
FC2, relu2 output softmax.

00:16:04.568 --> 00:16:08.934
So, one thing to note and remember that in a dictionary the keys have to be unique.

00:16:08.934 --> 00:16:12.939
So, you have to name this one relu1 or

00:16:12.940 --> 00:16:18.380
whatever and this ReLU activation has to have a different name,

00:16:18.379 --> 00:16:20.625
and now we can look at our model.

00:16:20.625 --> 00:16:26.330
So, you can see now each of our layers has a name attached to it.

00:16:26.330 --> 00:16:33.165
So, FC2 is this linear operation and we can actually get out FC2 here,

00:16:33.164 --> 00:16:34.939
and so, there you go. They're named.

00:16:34.940 --> 00:16:37.840
You can access them just like attributes

00:16:37.840 --> 00:16:42.230
like we defined in the original network and go forward from there.

00:16:42.419 --> 00:16:47.209
So now, your turn to build a network.

00:16:47.210 --> 00:16:49.605
You can use any of these methods that I talked about here.

00:16:49.605 --> 00:16:54.105
So what I want you to do is build a network to classify the images with

00:16:54.105 --> 00:16:57.289
three hidden layers and the first one should have 400 units second

00:16:57.289 --> 00:17:00.610
washed up 200 units and last one should have 100 units,

00:17:00.610 --> 00:17:02.860
and make sure you use ReLU activation functions on each of

00:17:02.860 --> 00:17:05.765
them and softmax on the output layer.

00:17:05.765 --> 00:17:07.940
Here, you're not training the network yet,

00:17:07.940 --> 00:17:10.120
so it's not going to be able to make predictions.

00:17:10.119 --> 00:17:11.879
But in the next video notebook,

00:17:11.880 --> 00:17:13.700
you'll learn how to do that,

00:17:13.700 --> 00:17:15.730
and so, you'll be able to train your network.

00:17:15.730 --> 00:17:18.000
See you in the next video. Cheers.

