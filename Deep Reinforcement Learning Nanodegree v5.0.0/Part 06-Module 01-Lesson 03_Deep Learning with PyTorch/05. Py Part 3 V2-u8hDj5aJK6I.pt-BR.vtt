WEBVTT
Kind: captions
Language: pt-BR

00:00:00.363 --> 00:00:02.068
Olá. Neste notebook,

00:00:02.102 --> 00:00:05.201
vou mostrar como treinar
uma rede neural com o PyTorch.

00:00:05.235 --> 00:00:08.767
Na parte anterior,
construímos uma rede neural,

00:00:08.801 --> 00:00:11.805
mas ela não podia nos dizer

00:00:11.839 --> 00:00:13.914
qual era o dígito
nas imagens.

00:00:13.948 --> 00:00:18.145
Queremos poder passar
uma imagem

00:00:18.179 --> 00:00:21.517
para alguma função
e que essa função retorne

00:00:21.551 --> 00:00:23.497
uma distribuição
de probabilidade

00:00:23.531 --> 00:00:26.425
que nos diga
que esta é a imagem de um 4.

00:00:26.459 --> 00:00:28.132
O que vamos fazer

00:00:28.166 --> 00:00:32.260
é treinar uma rede neural
para aproximar esta função.

00:00:32.294 --> 00:00:34.627
Para isso,
você fornece uma imagem,

00:00:34.661 --> 00:00:36.347
diz a resposta certa,

00:00:36.381 --> 00:00:40.958
e a rede neural aprende
a aproximar esta função.

00:00:40.992 --> 00:00:44.667
Isso é o que vamos ver
neste notebook.

00:00:45.767 --> 00:00:49.316
Vamos lembrar rapidamente
como funciona o treinamento.

00:00:49.350 --> 00:00:54.094
Como eu disse,
passamos uma imagem,

00:00:54.128 --> 00:00:58.421
passamos a categoria,
o rótulo correto dela,

00:00:58.455 --> 00:01:03.852
e medimos a diferença
entre a predição das nossas redes,

00:01:03.886 --> 00:01:08.140
que é isto,
e o rótulo true da imagem.

00:01:08.174 --> 00:01:13.048
A diferença entre a predição
e o rótulo true chama-se "perda".

00:01:13.082 --> 00:01:15.468
Há outros jeitos de medir isso,
por exemplo,

00:01:15.502 --> 00:01:18.302
perda quadrática média
ou entropia cruzada, que vimos,

00:01:18.336 --> 00:01:22.029
e você usa essa perda
para atualizar os pesos.

00:01:22.564 --> 00:01:25.063
Fazemos a atualização
por um processo iterativo

00:01:25.097 --> 00:01:26.700
chamado "gradiente descendente".

00:01:26.734 --> 00:01:30.644
A ideia é que você tem o gradiente
da sua função de perda,

00:01:30.678 --> 00:01:33.186
que é basicamente o declive
da função de perda,

00:01:33.220 --> 00:01:36.085
e ele sempre aponta na direção
da mudança mais rápida.

00:01:36.119 --> 00:01:37.458
Queremos minimizar a perda

00:01:37.492 --> 00:01:39.357
porque isso significa
que as predições

00:01:39.391 --> 00:01:42.127
estão o mais perto possível
dos rótulos true.

00:01:42.569 --> 00:01:43.963
É o gradiente descendente,

00:01:43.997 --> 00:01:46.677
pois sempre aponta na direção
da mudança mais rápida.

00:01:46.711 --> 00:01:51.597
Isso irá nos levar à raiz da função
de perda o mais rápido possível.

00:01:52.110 --> 00:01:54.855
Com redes neurais
multicamadas,

00:01:54.889 --> 00:01:57.517
fazemos isso
por retropropagação.

00:01:57.819 --> 00:02:01.403
Retropropagação é só uma aplicação
da regra da cadeia.

00:02:02.103 --> 00:02:05.340
Isso significa
que uma mudança em w1

00:02:05.374 --> 00:02:08.799
é propagada por toda a rede,
da esquerda para a direita,

00:02:08.833 --> 00:02:11.893
por toda a perda,
que é script L.

00:02:11.927 --> 00:02:15.166
De forma semelhante,
as mesmas mudanças são propagadas

00:02:15.200 --> 00:02:19.722
regressivamente pela rede
para 2w1, nossos pesos.

00:02:19.989 --> 00:02:22.898
Para atualizar os pesos,
precisamos saber

00:02:22.932 --> 00:02:26.273
o gradiente da perda
em relação aos nossos pesos.

00:02:26.745 --> 00:02:30.470
Para isso, podemos multiplicar
os gradientes de cada passo

00:02:30.504 --> 00:02:33.338
desta série de operações.

00:02:33.717 --> 00:02:38.089
A primeira coisa a fazer no PyTorch
é definir a função de perda.

00:02:38.702 --> 00:02:42.533
Normalmente, a perda está atribuída
a uma variável chamada "critério".

00:02:42.885 --> 00:02:44.954
Se estivermos usando
o output softmax,

00:02:44.988 --> 00:02:48.207
queremos que a perda, o critério,
seja perda de entropia cruzada.

00:02:48.241 --> 00:02:51.897
Assim, podemos obtê-la
com nn.CrossEntropyLoss.

00:02:52.312 --> 00:02:54.746
Depois, colocamos
o output da nossa rede

00:02:54.780 --> 00:02:58.910
e nossos rótulos true, nossos alvos,
para obter a perda real.

00:02:58.944 --> 00:03:03.618
Também precisamos de um otimizador.
O otimizador pega uma perda.

00:03:03.652 --> 00:03:06.938
Também precisamos de um otimizador.
Como temos uma perda,

00:03:06.972 --> 00:03:09.716
podemos calcular os gradientes
e, quando os tivermos,

00:03:09.750 --> 00:03:12.555
o otimizador os usa
para atualizar

00:03:12.589 --> 00:03:15.124
todos os pesos e parâmetros
da nossa rede.

00:03:15.158 --> 00:03:19.456
Para isso, vou usar o gradiente
descendente estocástico, ou GDE.

00:03:19.490 --> 00:03:22.151
Também há otimizadores
mais avançados, como Adam,

00:03:22.185 --> 00:03:24.458
mas não vamos entrar
nisso agora.

00:03:24.492 --> 00:03:29.613
Para calcular gradientes, o PyTorch
usa um módulo chamado Autograd.

00:03:29.647 --> 00:03:32.552
Com o Autograd,
se você tiver um tensor

00:03:32.586 --> 00:03:34.963
e disser ao Autograd
que ele exige um gradiente,

00:03:34.997 --> 00:03:38.591
ele monitora todas as operações
que ocorrem no tensor.

00:03:38.625 --> 00:03:40.781
Então, no fim
de suas operações,

00:03:40.815 --> 00:03:44.362
você pode dizer,
por exemplo, z.backward,

00:03:44.396 --> 00:03:47.209
e ele faz um passe regressivo
por todas as operações.

00:03:47.243 --> 00:03:50.676
Ele manteve um registro de tudo
e sabe as funções de gradiente.

00:03:50.710 --> 00:03:52.142
Podemos regressar

00:03:52.176 --> 00:03:55.604
e depois calcular
todos os gradientes que você quiser.

00:03:55.949 --> 00:03:57.541
Vou mostrar como funciona.

00:03:57.575 --> 00:04:00.475
Primeiro, vou importar coisas,
como normalmente.

00:04:00.680 --> 00:04:05.476
Aqui, vou criar
um tensor aleatório.

00:04:05.510 --> 00:04:09.009
Será um tensor de 2 por 2,

00:04:09.043 --> 00:04:12.016
amostrado aleatoriamente
de uma distribuição normal.

00:04:12.050 --> 00:04:16.647
Vamos dizer
requires_grad=True.

00:04:16.681 --> 00:04:20.451
Isto dirá ao PyTorch para rastrear
todas as operações

00:04:20.485 --> 00:04:23.738
que ocorrerem neste tensor
usando o Autograd.

00:04:24.303 --> 00:04:29.035
Em algum momento, poderemos
calcular o gradiente dele.

00:04:29.759 --> 00:04:31.708
E assim é o nosso tensor.

00:04:32.170 --> 00:04:35.618
Podemos dizer
que y=x ao quadrado,

00:04:36.425 --> 00:04:40.810
e print(y), para elevar
os valores ao quadrado.

00:04:41.510 --> 00:04:45.273
Se fizermos print(y.grad_fn),

00:04:46.729 --> 00:04:51.618
isso mostra a operação
que foi feita para obter y.

00:04:51.652 --> 00:04:55.453
Elevamos x ao quadrado
aqui para criar y,

00:04:55.487 --> 00:04:58.639
e, se você observar, a função
de gradiente para y é power.

00:04:58.673 --> 00:05:01.201
Elevamos x à potência de 2.

00:05:01.595 --> 00:05:04.348
Quando fizermos a regressão
nessas operações,

00:05:04.382 --> 00:05:06.440
ele vai usar
essa função de gradiente

00:05:06.474 --> 00:05:08.515
para calcular
o gradiente de y.

00:05:08.908 --> 00:05:12.144
Se pegarmos a média de y

00:05:12.942 --> 00:05:16.929
e imprimirmos,
agora é só um número.

00:05:16.963 --> 00:05:22.187
Podemos verificar os gradientes
de x e y, mas não há nada lá.

00:05:22.221 --> 00:05:24.733
Não há nada lá
porque ainda não fizemos

00:05:24.767 --> 00:05:27.773
um passo atrás
para essas operações.

00:05:27.807 --> 00:05:32.522
Não pedimos ao PyTorch nem ao
Autograd para calcular o gradiente.

00:05:33.156 --> 00:05:35.170
Para calcular os gradientes,

00:05:35.204 --> 00:05:38.064
fazemos o passo atrás
para estas operações.

00:05:38.098 --> 00:05:44.060
Por exemplo, para fazer
o gradiente de z em relação a x,

00:05:44.094 --> 00:05:46.330
fazemos z.backward,

00:05:46.933 --> 00:05:52.839
e isso calcula os gradientes
de z em relação a x.

00:05:54.027 --> 00:05:57.954
E podemos imprimir isto.

00:05:57.988 --> 00:06:02.097
Com base nas operações que fizemos,
devemos esperar

00:06:02.131 --> 00:06:06.612
que o gradiente de z
em relação a x seja x/2.

00:06:06.646 --> 00:06:09.299
Vou imprimir isso também.

00:06:09.333 --> 00:06:13.422
Obtivemos o gradiente,
e é o mesmo que x/2.

00:06:13.456 --> 00:06:16.551
Calculamos o gradiente,
e era o que esperávamos.

00:06:17.118 --> 00:06:18.905
Agora, vou pegar nossos dados

00:06:18.939 --> 00:06:22.636
e construir a rede,
como fizemos na última parte.

00:06:23.009 --> 00:06:27.226
Temos nosso modelo,
e agora vou treiná-lo.

00:06:27.655 --> 00:06:29.384
Como eu disse antes,

00:06:29.418 --> 00:06:33.557
precisamos definir
a perda ou o critério. Então,

00:06:33.591 --> 00:06:39.530
criterion =
nn.CrossEntropyLoss.

00:06:40.209 --> 00:06:43.059
Então podemos
definir o otimizador,

00:06:43.275 --> 00:06:47.285
optimizer = optim.SGD.

00:06:47.905 --> 00:06:50.459
Para o otimizador,
precisamos passar os parâmetros

00:06:50.493 --> 00:06:52.397
que queremos otimizar.

00:06:52.431 --> 00:06:55.932
Nesse caso, queremos otimizar
todos os parâmetros do modelo.

00:06:55.966 --> 00:06:59.639
Dizemos model.parameters
e definimos a taxa de aprendizado.

00:06:59.673 --> 00:07:02.157
A taxa de aprendizado
será 0,01.

00:07:02.880 --> 00:07:05.921
O processo geral
com o PyTorch é, primeiro,

00:07:05.955 --> 00:07:08.641
fazer um passo adiante
pela rede

00:07:08.675 --> 00:07:11.343
para obter o output,
ou logits ou softmax,

00:07:11.377 --> 00:07:13.730
seja qual for o output
da sua rede.

00:07:13.764 --> 00:07:16.740
Depois, use o output
para calcular a perda.

00:07:16.774 --> 00:07:19.237
Quando tiver a perda,
faça um passo atrás

00:07:19.271 --> 00:07:21.188
pela rede com loss.backward,

00:07:21.222 --> 00:07:25.787
e isso irá calcular os gradientes
de todos os parâmetros da sua rede.

00:07:26.027 --> 00:07:29.129
Com os gradientes, você pode avançar
uma etapa com o otimizador

00:07:29.163 --> 00:07:30.372
para atualizar os pesos.

00:07:30.630 --> 00:07:34.149
Primeiro, vou imprimir

00:07:34.183 --> 00:07:37.500
os pesos antes
do passo de treinamento.

00:07:38.499 --> 00:07:41.799
model.fc1.weight.

00:07:42.303 --> 00:07:46.606
Vamos obter as imagens
e rótulos normalmente.

00:07:47.897 --> 00:07:50.125
Mas a primeira coisa
a fazer é chamar

00:07:50.159 --> 00:07:54.386
optimizer.zero_grad.

00:07:54.420 --> 00:07:58.730
Isso zera todos
os gradientes definidos

00:07:58.764 --> 00:08:02.733
em tensores, pesos e vieses
que estão sendo treinados.

00:08:02.767 --> 00:08:07.271
Precisamos fazer isso porque,
toda vez que fazemos regressão,

00:08:07.305 --> 00:08:11.273
como loss.backward,
isso acumula os gradientes.

00:08:11.307 --> 00:08:13.177
Ou seja,
se você fizer 2 vezes,

00:08:13.211 --> 00:08:16.093
ele vai adicionar os 2 cálculos
de gradiente de novo,

00:08:16.127 --> 00:08:18.249
se fizer 3 vezes,
vai adicionar todos os 3,

00:08:18.283 --> 00:08:20.390
se fizer 4 vezes,
vai adicionar todos os 4.

00:08:20.424 --> 00:08:23.125
Normalmente,
não queremos fazer isso.

00:08:23.159 --> 00:08:26.150
Queremos fazer um passo,
obter os gradientes,

00:08:26.184 --> 00:08:28.511
usá-los para treinar,
fazer o passo seguinte,

00:08:28.545 --> 00:08:31.433
calcular novos gradientes
e usá-los para treinar.

00:08:31.467 --> 00:08:34.983
Se você não zerar o gradiente,
você basicamente

00:08:35.017 --> 00:08:37.573
soma, acumula gradientes

00:08:37.607 --> 00:08:40.430
por vários passos
de treinamento e vários lotes.

00:08:40.696 --> 00:08:42.655
A primeira coisa
do passo de treinamento

00:08:42.689 --> 00:08:46.894
é zerar os gradientes.

00:08:48.681 --> 00:08:51.526
Agora, podemos fazer
o passo adiante.

00:08:52.679 --> 00:08:57.094
O output será model.forward,

00:08:57.128 --> 00:08:59.029
passamos as imagens,

00:08:59.518 --> 00:09:01.861
calculamos a perda
com nosso critério,

00:09:02.271 --> 00:09:05.584
passamos o output
e os rótulos true,

00:09:06.204 --> 00:09:08.991
e, depois,
fazemos o passo atrás.

00:09:09.025 --> 00:09:12.935
Isso vai calcular os gradientes
para todos os nossos parâmetros.

00:09:13.325 --> 00:09:15.734
Aqui, posso imprimir
os gradientes.

00:09:17.612 --> 00:09:19.131
Agora, com os gradientes,

00:09:19.165 --> 00:09:22.312
damos um passo otimizador
e atualizamos os pesos.

00:09:23.642 --> 00:09:26.240
Antes, ele era assim.
Aqui estão os gradientes.

00:09:26.274 --> 00:09:29.864
E, agora, podemos ver
os pesos atualizados.

00:09:29.898 --> 00:09:32.301
Basicamente, ele pegou
nossos gradientes,

00:09:33.208 --> 00:09:36.358
adicionou aos parâmetros
existentes, nossos pesos,

00:09:36.392 --> 00:09:39.617
com a taxa de aprendizado,
e os pesos foram atualizados.

00:09:40.092 --> 00:09:42.967
Esta parte aqui,

00:09:43.001 --> 00:09:46.560
esta célula inteira,
é só um passo de treinamento básico.

00:09:46.594 --> 00:09:50.093
Pegamos imagens e rótulos
do trainloader,

00:09:50.127 --> 00:09:51.890
que carrega nossos dados,

00:09:51.924 --> 00:09:57.099
passamos adiante através
do modelo, calculamos a perda,

00:09:57.133 --> 00:10:01.453
fazemos o passo atrás
pela rede

00:10:01.487 --> 00:10:04.230
e damos um passo otimizador.

00:10:04.981 --> 00:10:08.367
Você pode empacotar tudo isso
em um loop for

00:10:08.401 --> 00:10:10.836
que passe por todas
as suas imagens e rótulos,

00:10:10.870 --> 00:10:13.913
por tudo no seu conjunto de dados,
e atualizar sua rede.

00:10:13.947 --> 00:10:16.591
E é assim
que a sua rede aprende.

00:10:17.398 --> 00:10:20.796
Vamos pegar esse método
de treinamento e colocar em um loop.

00:10:20.830 --> 00:10:24.762
Normalmente, definimos
o número de epochs

00:10:24.796 --> 00:10:26.580
nos quais queremos treiná-la.

00:10:26.614 --> 00:10:30.201
1 epoch é 1 passo
por todo o conjunto de dados,

00:10:30.235 --> 00:10:34.059
então, 3 epochs serão 3 passos
por todo o conjunto de dados.

00:10:35.255 --> 00:10:38.261
Vou definir
algumas coisas que vou usar.

00:10:38.295 --> 00:10:44.018
for e in range(epochs).

00:10:46.013 --> 00:10:47.241
Vou definir a perda.

00:10:47.275 --> 00:10:51.388
Vamos imprimir nossa perda
durante o treinamento

00:10:51.422 --> 00:10:53.759
para vermos
que as perdas de treinamento,

00:10:53.793 --> 00:10:56.925
como a perda da nossa rede,
estão caindo.

00:10:56.959 --> 00:11:00.309
À medida que a rede aprende,
e suas predições ficam melhores,

00:11:00.343 --> 00:11:02.087
a perda diminui.

00:11:02.121 --> 00:11:04.696
Queremos ver isso acontecendo
durante o treinamento.

00:11:05.871 --> 00:11:10.313
Pegamos as imagens e rótulos
do trainloader.

00:11:11.157 --> 00:11:14.902
Vamos rastrear nossos passos,

00:11:14.936 --> 00:11:17.227
ou seja,
quantas vezes treinamos.

00:11:17.261 --> 00:11:20.880
Então, de novo,
images.resize.

00:11:22.408 --> 00:11:26.470
Agora, o passo de treinamento:
optimizer.zero_grad.

00:11:27.181 --> 00:11:31.155
Agora, o passo adiante:
output model.forward.

00:11:31.779 --> 00:11:33.902
loss = criterion.

00:11:34.533 --> 00:11:38.705
O passo atrás,
e a etapa de atualização do peso.

00:11:39.519 --> 00:11:43.230
Aqui, a perda é só um número,

00:11:43.264 --> 00:11:46.662
ou seja, é um tensor escalar.

00:11:46.696 --> 00:11:50.065
Para tirar esse número
do tensor escalar

00:11:50.099 --> 00:11:52.549
e somá-lo a um 0, aqui,

00:11:52.583 --> 00:11:55.152
temos que fazer loss.item.

00:11:56.006 --> 00:12:01.629
Aqui, a frequência
com que vamos imprimir

00:12:02.114 --> 00:12:04.547
a perda de treinamento.

00:12:06.002 --> 00:12:09.167
Certo. Ao rolar
a tela para baixo,

00:12:09.200 --> 00:12:12.412
vemos que a perda de treinamento
diminui ao longo do treinamento,

00:12:12.446 --> 00:12:13.871
o que é um bom sinal.

00:12:14.399 --> 00:12:18.722
Agora, com a rede treinada,
podemos ver seu desempenho.

00:12:19.251 --> 00:12:23.420
Aqui, passamos a imagem
do número 6 para a rede,

00:12:23.454 --> 00:12:26.514
e a rede aprendeu
que este é o número 6.

00:12:27.104 --> 00:12:28.920
Vamos ver outros números.

00:12:29.408 --> 00:12:31.467
Ele sabe que o 2 é 2

00:12:31.760 --> 00:12:34.918
e sabe que o 3 é 3.
Isso é muito legal.

00:12:34.952 --> 00:12:36.990
Usando a retropropagação,

00:12:37.024 --> 00:12:39.886
conseguimos treinar
nossa rede neural

00:12:39.920 --> 00:12:43.463
para identificar essas imagens
de dígitos escritos à mão.

00:12:43.497 --> 00:12:46.862
Isso é útil
para escanear documentos.

00:12:46.896 --> 00:12:51.691
Você pode pegar várias imagens
de um documento, escaneá-lo

00:12:51.725 --> 00:12:55.388
e passar as imagens
para uma rede,

00:12:55.422 --> 00:12:58.701
e a rede será capaz
de dizer ao computador

00:12:58.735 --> 00:13:02.247
o que há no documento,
só pela imagem do documento.

00:13:02.712 --> 00:13:05.168
Na próxima parte,
você vai poder,

00:13:05.202 --> 00:13:08.967
de novo, construir sua rede
e treiná-la dessa vez. Boa sorte.

