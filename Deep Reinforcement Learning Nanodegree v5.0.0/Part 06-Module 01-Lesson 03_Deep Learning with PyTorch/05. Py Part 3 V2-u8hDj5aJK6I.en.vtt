WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.100
Hello, in this notebook,

00:00:02.100 --> 00:00:05.015
I'll be showing you how to train a neural network with PyTorch.

00:00:05.014 --> 00:00:09.375
So, remember from the previous part we built a neural network but it wasn't

00:00:09.375 --> 00:00:13.929
able to actually tell us what the digit was in these images.

00:00:13.929 --> 00:00:16.620
So, what we want is we want to be able to

00:00:16.620 --> 00:00:19.740
pass in an image into some function and this function

00:00:19.739 --> 00:00:21.614
is going to return to us

00:00:21.614 --> 00:00:26.159
a probability distribution that tells us that this is an image of a four.

00:00:26.160 --> 00:00:28.199
So, what we're going to do here is

00:00:28.199 --> 00:00:32.219
actually train a neural network to approximate this function.

00:00:32.219 --> 00:00:34.600
The way you do that is you give it an image,

00:00:34.600 --> 00:00:36.270
you tell it the right answer,

00:00:36.270 --> 00:00:40.705
and then the neural network learns how to approximate this function.

00:00:40.704 --> 00:00:45.224
So, that is what we are going to be looking at in this notebook.

00:00:45.225 --> 00:00:50.020
Just as a quick reminder of how training actually works is,

00:00:50.020 --> 00:00:51.410
as I was saying,

00:00:51.409 --> 00:00:57.049
we pass in an image and then we also pass in the correct category,

00:00:57.049 --> 00:00:58.320
the correct label for it,

00:00:58.320 --> 00:01:03.799
and what we do is measure the difference between our networks prediction,

00:01:03.799 --> 00:01:08.015
what this thing is, and the true label of that image.

00:01:08.015 --> 00:01:13.040
The difference between the prediction and the true label is called the loss.

00:01:13.040 --> 00:01:14.820
There's different ways to measure this,

00:01:14.819 --> 00:01:17.259
for example, the mean squared loss or a cross entropy,

00:01:17.260 --> 00:01:22.295
which we've seen, and you use this loss to update the weights.

00:01:22.295 --> 00:01:26.180
We do the updating through this iterative process called gradient descent.

00:01:26.180 --> 00:01:30.710
So, then the idea is that you have this gradient of your loss function,

00:01:30.709 --> 00:01:33.140
which is basically the slope of the loss function,

00:01:33.140 --> 00:01:35.900
and it always points in the direction of fastest change.

00:01:35.900 --> 00:01:38.570
We want to minimize our loss because that means that

00:01:38.569 --> 00:01:42.269
our predictions are as close as possible to the true labels.

00:01:42.269 --> 00:01:46.640
So, it's great descent since it's always pointing in the direction of fastest change,

00:01:46.640 --> 00:01:51.769
this will get us to the bottom of our loss function the fastest way possible.

00:01:51.769 --> 00:01:54.839
With multilayer neural networks,

00:01:54.840 --> 00:01:57.575
we do this through back propagation.

00:01:57.575 --> 00:02:01.770
Back propagation is really just an application of the chain rule.

00:02:01.769 --> 00:02:05.569
So, what this means is that for a change in W1 that

00:02:05.569 --> 00:02:08.930
change propagates all the way through this network from left to right,

00:02:08.930 --> 00:02:13.120
all the way to the loss which is script L. Similarly,

00:02:13.120 --> 00:02:19.724
the same changes they propagate backwards through the network to the 2W1, our weights.

00:02:19.724 --> 00:02:22.909
So, to update our weights we actually need to know

00:02:22.909 --> 00:02:26.569
the gradient of the loss with respect to our weights.

00:02:26.569 --> 00:02:33.319
To do that, we can just multiply the gradients of each step in this series of operations.

00:02:33.319 --> 00:02:38.254
The first thing we need to do in PyTorch is define our loss function.

00:02:38.254 --> 00:02:42.509
You'll usually see the loss assigned to a variable called criterion.

00:02:42.509 --> 00:02:44.810
So, if we're using the softmax output,

00:02:44.810 --> 00:02:46.405
we want our loss, our criterion,

00:02:46.405 --> 00:02:51.909
to be cross-entropy loss so we can get that within nn.CrossEntropyLoss.

00:02:51.909 --> 00:02:55.979
Then later, we put in the output from our network and our true labels,

00:02:55.979 --> 00:02:58.569
our targets to get the actual loss.

00:02:58.569 --> 00:03:00.169
We also need an optimizer.

00:03:00.169 --> 00:03:03.324
So the optimizer takes a loss.

00:03:03.324 --> 00:03:05.334
We'll also need an optimizer.

00:03:05.335 --> 00:03:06.740
Since we have a loss,

00:03:06.740 --> 00:03:09.735
we can calculate our gradients and then once we have those gradients,

00:03:09.735 --> 00:03:14.870
the optimizer uses them to actually update all the weights in parameters in our network.

00:03:14.870 --> 00:03:19.355
So, for this, I'm going to be using stochastic gradient descent or SGD.

00:03:19.354 --> 00:03:21.394
There's also more advanced optimizers like

00:03:21.395 --> 00:03:24.055
Adam but we won't be worrying about that right now.

00:03:24.055 --> 00:03:29.064
To calculate gradients, PyTorch uses a module called autograd.

00:03:29.064 --> 00:03:32.569
What autograd does is that if you have a tensor,

00:03:32.569 --> 00:03:34.789
and you tell it that it requires a gradient,

00:03:34.789 --> 00:03:38.439
it tracks every single operation that happens on that tensor.

00:03:38.439 --> 00:03:40.834
So then, at the end of your operations,

00:03:40.834 --> 00:03:42.814
you can say for example,

00:03:42.814 --> 00:03:47.210
z.backward and then it does a backward pass through all the operations,

00:03:47.210 --> 00:03:48.520
and it kept track of all those things,

00:03:48.520 --> 00:03:50.350
and it knows the gradient functions for all those things.

00:03:50.349 --> 00:03:52.534
And so, we can go backwards and then

00:03:52.534 --> 00:03:55.604
eventually calculate all the gradients that you want.

00:03:55.604 --> 00:03:57.424
So, I'll show you how this works.

00:03:57.425 --> 00:04:00.310
So first, I'm just going to import stuff like normal.

00:04:00.310 --> 00:04:05.569
Here, I'm just going to create a random tensor.

00:04:05.569 --> 00:04:09.025
So this just going to be a two by two tensor,

00:04:09.025 --> 00:04:11.680
just randomly sampled from a normal distribution.

00:04:11.680 --> 00:04:16.280
We're going to say requires grad equals true.

00:04:16.279 --> 00:04:19.809
So, this will tell PyTorch to track

00:04:19.810 --> 00:04:23.785
all the operations that happen on this tensor using auto grad.

00:04:23.785 --> 00:04:27.010
Eventually, we'll be able to calculate

00:04:27.009 --> 00:04:31.740
the gradient for it and that's what our tensor looks like.

00:04:31.740 --> 00:04:38.405
So, we can say y equals x squared and print y,

00:04:38.404 --> 00:04:41.189
so it says squared r values.

00:04:41.189 --> 00:04:45.860
If we do print.y.grad fun,

00:04:45.860 --> 00:04:51.475
then this actually shows the operation that was done to get y.

00:04:51.475 --> 00:04:54.770
So, we squared x here to

00:04:54.769 --> 00:04:58.479
create y and then if you look at the gradient function for y is its power.

00:04:58.480 --> 00:05:01.165
So, we took x to the power of two.

00:05:01.165 --> 00:05:04.400
So, eventually when we're going backwards to these operations,

00:05:04.399 --> 00:05:08.524
it's going to use this gradient function to calculate the gradient for y.

00:05:08.524 --> 00:05:12.234
So, now if we take the mean of y,

00:05:12.235 --> 00:05:16.985
print that, it's now just a single number.

00:05:16.985 --> 00:05:22.000
We can check the gradients of x and y but there's nothing there.

00:05:22.000 --> 00:05:24.740
So, there's nothing there because we haven't actually done

00:05:24.740 --> 00:05:27.750
a backward pass through these operations yet.

00:05:27.750 --> 00:05:32.779
So, we haven't actually asked PyTorch or Autograd to calculate the gradient yet.

00:05:32.779 --> 00:05:35.229
So, now to actually calculate our gradients,

00:05:35.230 --> 00:05:38.455
we need to go backward through these operations.

00:05:38.454 --> 00:05:43.889
For instance, if we want to do the gradient of z with respect to x,

00:05:43.889 --> 00:05:53.509
then we say z.backward and this calculates the gradients of z with respect to x,

00:05:53.509 --> 00:05:57.810
then we can print this out and also print out.

00:05:57.810 --> 00:06:01.129
So, based on the operations that we did,

00:06:01.129 --> 00:06:06.584
we should expect that the gradient of z with respect to x should be x over two.

00:06:06.584 --> 00:06:08.739
So, I'll just go and print that out too.

00:06:08.740 --> 00:06:13.340
So, we get our gradient and it is the same as x over two.

00:06:13.339 --> 00:06:16.810
So, calculate gradient and it is what we expect.

00:06:16.810 --> 00:06:19.509
So, now I'm just going to grab our data and

00:06:19.509 --> 00:06:22.894
build the network like we did in the last part.

00:06:22.894 --> 00:06:27.324
Now we have our model and now I'm going to actually train it.

00:06:27.324 --> 00:06:29.064
So like I said before,

00:06:29.064 --> 00:06:32.915
we need to define our loss or our criterion.

00:06:32.915 --> 00:06:43.035
So, criterion is equal to nn.CrossEntropyLoss and then we can define our optimizer.

00:06:43.035 --> 00:06:47.655
Optimizer optim.SGD.

00:06:47.654 --> 00:06:49.179
Then for the optimizer,

00:06:49.180 --> 00:06:52.180
we need to pass in the parameters that we actually want to optimize.

00:06:52.180 --> 00:06:55.894
So, in this case, you want to optimize all the parameters of our model.

00:06:55.894 --> 00:06:59.649
So, we say model.parameters and then we need to set the learning rate,

00:06:59.649 --> 00:07:02.419
so learning rate here is going to be 0.01.

00:07:02.420 --> 00:07:06.400
Then, the general process with PyTorch is first we're going to

00:07:06.399 --> 00:07:09.929
make a forward pass through the network to get our output,

00:07:09.930 --> 00:07:11.449
or logits, or a softmax,

00:07:11.449 --> 00:07:13.149
whatever the output of your network is.

00:07:13.149 --> 00:07:16.399
Then we're going to use that output to calculate the loss.

00:07:16.399 --> 00:07:17.964
Then once you have the loss,

00:07:17.964 --> 00:07:21.619
you can do a backward pass through the network with loss.backward and this will

00:07:21.620 --> 00:07:25.740
calculate the gradients for all the parameters in your network.

00:07:25.740 --> 00:07:30.420
Then with the gradients you can take a step with the optimizer to update the weights.

00:07:30.420 --> 00:07:35.615
So, first I'm going to do is actually print out the weights before

00:07:35.615 --> 00:07:41.090
our little training pass, model.fc1.weight.

00:07:41.089 --> 00:07:45.379
Then we get our images and labels like

00:07:45.379 --> 00:07:52.379
normal but the first thing you actually have to do is call optimizer.zero_grad.

00:07:53.410 --> 00:08:00.220
So, what this does is it zeros out all the gradients set are on tensors,

00:08:00.220 --> 00:08:02.665
weights, and biases that are being trained.

00:08:02.665 --> 00:08:05.720
So, the reason you need to do this is because every time

00:08:05.720 --> 00:08:08.900
you do backwards like if you do loss.backward,

00:08:08.899 --> 00:08:11.324
it actually accumulates the gradients.

00:08:11.324 --> 00:08:13.594
So, that means that if you go through twice then it's going to

00:08:13.595 --> 00:08:16.010
add the two gradient calculations again,

00:08:16.009 --> 00:08:17.920
and then if you do it a third time, it's going to add all three of those,

00:08:17.920 --> 00:08:21.970
if you do it a fourth time it's going to add all four of those and so in general,

00:08:21.970 --> 00:08:23.100
you don't want to do that.

00:08:23.100 --> 00:08:26.095
You just want it to do one pass, get your gradients,

00:08:26.095 --> 00:08:28.525
use those gradients to train, and then the next pass,

00:08:28.524 --> 00:08:31.089
calculate new gradients and use those gradients to train.

00:08:31.089 --> 00:08:35.929
So, if you don't zero grad and basically you are summing up,

00:08:35.929 --> 00:08:40.384
you're accumulating gradients over multiple training passes and multiple batches.

00:08:40.384 --> 00:08:43.189
The first thing in your training pass is

00:08:43.190 --> 00:08:47.440
typically you want to make sure that you zero out the gradients.

00:08:47.779 --> 00:08:52.034
So, now we can do our forward pass.

00:08:52.034 --> 00:08:56.904
In the output, is going to be model.forward,

00:08:56.904 --> 00:08:59.074
pass in our images,

00:08:59.075 --> 00:09:01.785
calculate the loss with our criterion,

00:09:01.784 --> 00:09:05.769
where we pass in our output and the true labels,

00:09:05.769 --> 00:09:08.990
and then go into do our backward pass.

00:09:08.990 --> 00:09:13.044
So, this is going to calculate the gradients for all of our parameters.

00:09:13.044 --> 00:09:16.735
Then here I can actually print out the gradients.

00:09:16.735 --> 00:09:19.095
Now, with the gradients,

00:09:19.095 --> 00:09:23.000
we're going to take an optimizer step and update our weights.

00:09:23.000 --> 00:09:25.109
That's what it looks like before,

00:09:25.109 --> 00:09:26.259
here are our gradients,

00:09:26.259 --> 00:09:29.840
and now we can look at our updated weights.

00:09:29.840 --> 00:09:32.555
So, basically, it took our gradients,

00:09:32.554 --> 00:09:35.529
added it to our existing parameters,

00:09:35.529 --> 00:09:37.414
our weights with some learning rate,

00:09:37.414 --> 00:09:39.724
and we have updated weights.

00:09:39.725 --> 00:09:43.129
So, this little bit right here is,

00:09:43.129 --> 00:09:46.450
this whole cell, is really just a basic training pass.

00:09:46.450 --> 00:09:50.170
We get some images and labels from our train loader,

00:09:50.169 --> 00:09:51.714
which loads our data,

00:09:51.715 --> 00:09:57.075
and then we pass it forward through our model, calculate the loss,

00:09:57.075 --> 00:10:01.370
do backwards pass through our network,

00:10:01.370 --> 00:10:05.185
and then we take an optimizer step.

00:10:05.184 --> 00:10:07.649
You can wrap this whole thing in

00:10:07.649 --> 00:10:10.799
a for-loop that goes through all of your images and labels,

00:10:10.799 --> 00:10:12.979
everything in your dataset and updates

00:10:12.980 --> 00:10:16.904
your network and that's how your network basically learns.

00:10:16.904 --> 00:10:20.679
So, now we can take this training method and put it in a loop.

00:10:20.679 --> 00:10:26.429
So, what you typically do is define the number of epochs you want to train it on.

00:10:26.429 --> 00:10:29.449
So, basically, one epoch is one pass through

00:10:29.450 --> 00:10:31.379
the entire dataset and so three epochs

00:10:31.379 --> 00:10:34.389
is going to be three passes through the entire dataset.

00:10:34.389 --> 00:10:37.669
Just defining some things which I'll use.

00:10:37.669 --> 00:10:41.183
So for E in epochs,

00:10:41.183 --> 00:10:45.039
for range in range epochs.

00:10:45.039 --> 00:10:47.179
So, defining running loss.

00:10:47.179 --> 00:10:51.889
So we're going to actually be printing out our loss as we're training so we can

00:10:51.889 --> 00:10:56.610
see that the training losses like the loss of our network is actually dropping.

00:10:56.610 --> 00:11:00.389
So, as our network learns and its predictions become better,

00:11:00.389 --> 00:11:02.299
then the loss is going to decrease and so we

00:11:02.299 --> 00:11:05.234
just want to see that's happening as we're training.

00:11:05.235 --> 00:11:10.685
So, we get our images and labels from the train loader.

00:11:10.684 --> 00:11:14.789
We're going to keep track of our steps,

00:11:14.789 --> 00:11:17.615
so how many times we've actually trained.

00:11:17.615 --> 00:11:21.585
So again, images.resize.

00:11:21.585 --> 00:11:24.040
Okay, so now our training pass.

00:11:24.039 --> 00:11:26.649
So, optimizer.zero_grad.

00:11:26.649 --> 00:11:28.379
Now our forward pass,

00:11:28.379 --> 00:11:34.080
output model.forward, loss equals criterion.

00:11:34.080 --> 00:11:39.025
Our backward pass and then our weight update step.

00:11:39.024 --> 00:11:46.429
Here, our loss is just a single number so what this means it's a scalar tensor.

00:11:46.429 --> 00:11:52.629
To actually get this number out of the scalar tensor and just summit with a zero here,

00:11:52.629 --> 00:11:55.620
then we actually need to do a loss.item.

00:11:55.700 --> 00:12:05.320
Every so often going to print out our training loss.

00:12:05.320 --> 00:12:09.260
Okay and so we can see, just scroll down,

00:12:09.259 --> 00:12:13.980
that our training loss is dropping as its training which is a good sign.

00:12:13.980 --> 00:12:16.370
Now with the network trained,

00:12:16.370 --> 00:12:18.965
we can see how it performs.

00:12:18.965 --> 00:12:22.820
Here, we are passing in the image of number six to

00:12:22.820 --> 00:12:26.750
our network and then our network has learned that this is a number six.

00:12:26.750 --> 00:12:30.879
We can look at different numbers so it knows that a two is a two.

00:12:30.879 --> 00:12:34.924
It knows that a three is a three, so it's really cool.

00:12:34.924 --> 00:12:36.924
Now using back propagation,

00:12:36.924 --> 00:12:43.194
we were able to train our neural network to identify these images of handwritten digits.

00:12:43.195 --> 00:12:46.930
This is useful for things like scanning documents, right?

00:12:46.929 --> 00:12:50.329
You can imagine that you can just take a whole bunch of images of a document,

00:12:50.330 --> 00:12:51.625
and it kind of scan over it,

00:12:51.625 --> 00:12:55.399
and then pass the images into a network,

00:12:55.399 --> 00:12:57.709
and the network is able to basically tell

00:12:57.710 --> 00:13:02.500
your computer what is in this document just from an image of a document.

00:13:02.500 --> 00:13:05.720
In the next part, you'll be able to, again,

00:13:05.720 --> 00:13:09.399
build your own network and train it this time. Cheers.

