WEBVTT
Kind: captions
Language: pt-BR

00:00:00.126 --> 00:00:01.801
Olá! Bem-vindo de volta.

00:00:01.835 --> 00:00:07.540
Neste vídeo, vou mostrar como
construir redes neurais com PyTorch.

00:00:07.574 --> 00:00:10.044
Ao final deste notebook,
que fornecerei para você,

00:00:10.077 --> 00:00:13.213
você construirá sua própria
rede neural. Vamos começar.

00:00:13.246 --> 00:00:16.149
Primeiro, importamos
as coisas normalmente.

00:00:16.182 --> 00:00:19.143
Para importar PyTorch,
escrevemos import torch.

00:00:19.177 --> 00:00:21.788
Também vamos importar o pacote
chamado "torchvision",

00:00:21.821 --> 00:00:23.390
que é associado com PyTorch,

00:00:23.423 --> 00:00:28.495
e nos permite baixar e usar
conjuntos de dados existentes.

00:00:28.528 --> 00:00:31.364
O conjunto que usaremos
é chamado de "MNIST".

00:00:31.398 --> 00:00:33.590
O MNIST é composto
por várias imagens

00:00:33.648 --> 00:00:37.631
de dígitos desenhados à mão.
De 0 a 9.

00:00:37.958 --> 00:00:40.399
Ele é usado
para treinar uma rede

00:00:40.441 --> 00:00:42.511
ou outro modelo
de aprendizado de máquina,

00:00:42.544 --> 00:00:47.180
assim eles podem classificar
as imagens nestes dígitos.

00:00:47.213 --> 00:00:52.952
Então uma imagem que mostra um 8
será classificada como o dígito 8.

00:00:52.986 --> 00:00:56.389
É isto que usaremos
para este exemplo.

00:00:56.717 --> 00:00:59.025
Aqui estou usando torchvision

00:00:59.059 --> 00:01:01.661
para baixar e carregar
os dados de treinamento.

00:01:01.695 --> 00:01:04.698
Este é o conjunto
de dados MNIST

00:01:04.731 --> 00:01:07.300
que podemos colocar
em trainset.

00:01:07.334 --> 00:01:08.902
Escrevi download=True,

00:01:08.935 --> 00:01:11.638
então se ele
já não existir no disco,

00:01:11.671 --> 00:01:13.606
ele vai baixar para nós.

00:01:13.640 --> 00:01:16.209
Também estou fornecendo
um transform.

00:01:16.242 --> 00:01:20.880
Ele basicamente lê estas imagens
e aplica as transformações

00:01:20.914 --> 00:01:24.818
para nos dar o conjunto de dados
que colocamos na nossa rede.

00:01:24.851 --> 00:01:27.654
O transform fica assim.

00:01:27.687 --> 00:01:33.493
Primeiro transformamos
estas imagens num tensor PyTorch

00:01:33.526 --> 00:01:35.862
e depois as normalizamos.

00:01:35.895 --> 00:01:39.278
Neste caso,
"normalizar" significa

00:01:39.311 --> 00:01:43.567
que queremos pegar
nossas imagens,

00:01:43.603 --> 00:01:46.072
que estão em escala de cinza,

00:01:46.106 --> 00:01:49.509
e cada valor, cada pixel é
um flutuante de 0 a 1,

00:01:49.542 --> 00:01:55.115
mas nós queremos que os pixels
variem de -1 até 1.

00:01:55.148 --> 00:01:58.685
Então o que ele faz
é subtrair 0,5.

00:01:58.718 --> 00:02:03.123
Ele subtrai 0,5
de cada canal de cor, cada pixel,

00:02:03.156 --> 00:02:05.892
e depois divide por 0,5.

00:02:05.925 --> 00:02:09.496
Então ele muda a extensão
de 0 até 1

00:02:09.529 --> 00:02:14.434
para -0,5 a 0,5. Depois divide
por 0,5 ou o multiplica por 2,

00:02:14.467 --> 00:02:19.272
então ele aumenta a variação
de -1 para 1.

00:02:19.305 --> 00:02:22.175
Isto facilita o aprendizado
das redes neurais,

00:02:22.208 --> 00:02:26.780
e você tem quer normalizar os dados
que entram numa rede neural.

00:02:26.813 --> 00:02:28.815
Outra coisa a se notar:

00:02:28.848 --> 00:02:31.251
aqui eu configurei um tamanho
de lote de 64.

00:02:31.284 --> 00:02:34.754
Assim, quando recebermos
os dados do trainloader,

00:02:34.788 --> 00:02:37.724
ele vai nos dar 64 imagens
por vez.

00:02:37.757 --> 00:02:39.859
Ok. Vou carregar isto.

00:02:39.893 --> 00:02:41.428
Carreguei os dados,

00:02:41.461 --> 00:02:45.632
agora podemos ver como é
uma destas imagens.

00:02:45.665 --> 00:02:48.735
Aqui está um exemplo,
e este é o número 7.

00:02:48.768 --> 00:02:50.537
É um dígito desenhado à mão,

00:02:50.570 --> 00:02:54.207
que vamos classificar
com nossa rede neural.

00:02:54.240 --> 00:02:59.579
Aqui está a rede que vamos construir
como exemplo com MNIST.

00:02:59.612 --> 00:03:03.083
No MNIST,
as imagens têm 28x28,

00:03:03.116 --> 00:03:08.722
então vamos transformá-las
em um vetor

00:03:08.755 --> 00:03:11.524
que tem 28x28 unidades
de comprimento.

00:03:11.558 --> 00:03:14.627
784, que é 28 vezes 28.

00:03:14.661 --> 00:03:16.329
É o tamanho
da camada de input.

00:03:16.363 --> 00:03:19.065
Vamos converter as imagens
em um vetor

00:03:19.099 --> 00:03:21.468
e passá-las aqui como inputs.

00:03:21.501 --> 00:03:24.404
Vamos usar duas
camadas ocultas,

00:03:24.437 --> 00:03:27.607
a 1ª com 128 unidades
e a 2ª com 64 unidades.

00:03:27.640 --> 00:03:30.744
Isto vai para uma camada de output
de 10 unidades.

00:03:30.777 --> 00:03:32.712
Esta camada tem 10 unidades

00:03:32.746 --> 00:03:35.949
porque queremos classificar
estes dígitos, e são 10 deles.

00:03:35.982 --> 00:03:40.587
Este corresponde aos dígitos
0, 1, 2 e por aí vai.

00:03:40.620 --> 00:03:44.624
Aqui está o número de unidades
em cada camada oculta

00:03:44.657 --> 00:03:49.062
e o número de camadas ocultas
que você usa é meio arbitrário.

00:03:49.095 --> 00:03:52.098
Quanto mais unidades você tiver
numa camada

00:03:52.132 --> 00:03:53.566
e mais camadas você tiver,

00:03:53.600 --> 00:03:56.963
melhor sua rede conseguirá
ajustar os dados.

00:03:57.005 --> 00:03:59.706
Parte do treinamento
de redes neurais

00:03:59.739 --> 00:04:03.223
é descobrir quantas unidades
e quantas camadas

00:04:03.276 --> 00:04:05.497
devem ser usadas na sua rede.

00:04:05.530 --> 00:04:07.414
Para funções de ativação:

00:04:07.447 --> 00:04:09.716
para camadas ocultas,
vamos usar ReLUs

00:04:09.749 --> 00:04:12.018
e para o output,
vamos usar softmax.

00:04:12.052 --> 00:04:15.075
Lembre-se de que a softmax

00:04:15.108 --> 00:04:19.592
pega o valor destas unidades
de output

00:04:19.626 --> 00:04:23.790
e os transforma
numa distribuição de probabilidade.

00:04:23.824 --> 00:04:29.436
Isto significa que ele esguicha
todos estes valores entre 0 e 1,

00:04:29.469 --> 00:04:31.538
depois divide pelo total.

00:04:31.571 --> 00:04:34.574
A soma dos valores de cada unidade
é igual a 1.

00:04:34.607 --> 00:04:38.345
Esta é uma distribuição
de probabilidade discreta

00:04:38.378 --> 00:04:42.015
que nos dá a probabilidade de que,
qualquer que seja o input,

00:04:42.048 --> 00:04:44.150
ele pertence a esta classe.

00:04:44.184 --> 00:04:47.454
Para treinar esta rede,
vamos precisar de um erro.

00:04:47.487 --> 00:04:49.556
Vamos usar
um erro de entropia cruzada.

00:04:49.589 --> 00:04:53.560
Ele compara a probabilidade,

00:04:53.593 --> 00:04:57.731
como a previsão da camada da softmax
com a categoria true,

00:04:57.764 --> 00:05:01.234
e você pode usar a diferença,
este erro,

00:05:01.267 --> 00:05:03.503
para atualizar os pesos
na sua rede.

00:05:03.536 --> 00:05:06.139
Certo, é hora
de construir a rede.

00:05:06.172 --> 00:05:09.723
Primeiro, precisamos importar

00:05:09.756 --> 00:05:13.113
alguns módulos do PyTorch.

00:05:13.146 --> 00:05:17.017
Escrevo from torch import nn,

00:05:17.050 --> 00:05:20.086
de funções da rede neural.

00:05:20.624 --> 00:05:26.459
Também podemos importar
functional as F.

00:05:26.493 --> 00:05:31.064
Estas são funções mais específicas
para redes neurais,

00:05:31.097 --> 00:05:33.333
e isto é
uma forma funcional delas.

00:05:33.897 --> 00:05:36.236
Pronto, vamos começar
a construir a rede.

00:05:36.269 --> 00:05:39.539
O jeito mais geral e básico

00:05:39.572 --> 00:05:43.043
de fazer isto no PyTorch
é criando uma classe.

00:05:43.076 --> 00:05:45.912
Não importa o nome.
Vamos chamá-la de "Network".

00:05:45.945 --> 00:05:49.249
Ela é subclasse de nn.Module.

00:05:49.660 --> 00:05:51.985
Agora na função init,

00:05:52.018 --> 00:05:54.287
primeiro precisamos usar
a super.

00:05:54.320 --> 00:05:59.926
A super chama funções
ou atributos

00:05:59.959 --> 00:06:03.740
de qualquer classe
da qual ela seja subclasse.

00:06:03.773 --> 00:06:05.683
Esta linha

00:06:05.716 --> 00:06:10.503
vai chamar
o método init de nn.Module.

00:06:10.718 --> 00:06:13.940
Agora vamos definir
as camadas que vamos usar,

00:06:13.973 --> 00:06:18.978
as operações que vão definir
a arquitetura da rede neural.

00:06:19.012 --> 00:06:21.414
Para isto,
chamaremos self.fc1,

00:06:21.448 --> 00:06:26.353
uma camada completamente conectada.
Então fc é linear.

00:06:26.386 --> 00:06:28.621
Então isto é...
Se olhar para isto,

00:06:28.655 --> 00:06:32.792
isto aplica uma transformação linear
em dados recebidos.

00:06:32.826 --> 00:06:37.773
Precisamos colocar aqui
o tamanho do input

00:06:37.806 --> 00:06:39.366
e o tamanho do output.

00:06:39.399 --> 00:06:43.003
Sabemos que temos 784 unidades
na camada de input

00:06:43.036 --> 00:06:46.206
e queremos que a 1ª camada oculta
tenha 128 unidades.

00:06:46.239 --> 00:06:47.540
Para a 2ª camada,

00:06:47.574 --> 00:06:51.211
podemos fazer algo similar:
128, 64.

00:06:51.244 --> 00:06:55.048
Agora a camada de output fc3.

00:06:55.081 --> 00:07:00.453
Então isto vai de 64 a 10.

00:07:00.823 --> 00:07:04.818
Estas são as operações
que precisamos definir nesta parte.

00:07:04.851 --> 00:07:07.994
Para que isto funcione
como uma rede neural,

00:07:08.028 --> 00:07:11.865
precisamos definir
uma função forward.

00:07:11.898 --> 00:07:15.802
Todas as redes PyTorch
que você constrói

00:07:15.835 --> 00:07:20.140
a partir de nn.Module precisam ter
esta função forward.

00:07:20.971 --> 00:07:25.645
É esperado que x seja
um tensor PyTorch,

00:07:25.679 --> 00:07:31.251
e você vai passar este tensor
por cada camada, cada operação.

00:07:31.284 --> 00:07:37.190
Por exemplo, para fazer
a primeira camada, self.fc1(x).

00:07:37.223 --> 00:07:42.295
Ele está passando o tensor x
pela primeira camada,

00:07:42.328 --> 00:07:46.433
pela primeira operação linear,
e isto nos dá outro tensor.

00:07:46.970 --> 00:07:48.802
E podemos pegar este tensor

00:07:48.835 --> 00:07:52.505
e colocar a função
de ativação ReLU nele

00:07:52.539 --> 00:07:55.175
e seguir para o próximo.

00:07:55.208 --> 00:07:57.444
Então continua assim.

00:07:57.897 --> 00:08:02.615
Finalmente colocamos
a função softmax.

00:08:03.233 --> 00:08:09.189
Precisamos definir a dimensão
na qual queremos calcular a softmax.

00:08:09.222 --> 00:08:14.794
Se você se lembra, eu disse
que as imagens serão lotes

00:08:14.828 --> 00:08:18.591
e serão 64 imagens diferentes
por lote.

00:08:19.086 --> 00:08:23.370
E o tamanho do lote vem antes
do tamanho do tensor.

00:08:23.403 --> 00:08:25.338
Então seria a dimensão zero.

00:08:25.372 --> 00:08:31.244
E o vetor que estamos passando
pela rede é a segunda dimensão.

00:08:31.277 --> 00:08:36.049
Este tensor aqui
vai acabar sendo 64,

00:08:36.082 --> 00:08:38.051
que é o tamanho
do lote, por 10.

00:08:38.084 --> 00:08:43.690
Queremos pegar a softmax
na dimensão que tem 10 valores,

00:08:43.723 --> 00:08:45.535
então é a dimensão 1 aqui.

00:08:45.568 --> 00:08:47.694
Por último, você retorna isto.

00:08:47.727 --> 00:08:51.798
Podemos criar nosso modelo,
nossa rede, assim,

00:08:51.831 --> 00:08:53.833
e ver como fica.

00:08:53.867 --> 00:08:57.103
Basicamente vai nos mostrar
todos os módulos,

00:08:57.137 --> 00:09:02.060
operações que registramos
na função init.

00:09:02.307 --> 00:09:06.913
Criamos estas operações lineares,
estas transformações lineares.

00:09:06.946 --> 00:09:10.250
Os pesos e vieses são criados
automaticamente para nós.

00:09:10.283 --> 00:09:15.388
Podemos ver isto
com model.fc1.weight.

00:09:15.422 --> 00:09:16.756
Vou pôr um print aqui.

00:09:16.790 --> 00:09:22.429
Também vou colocar
print(model.fc1.bias).

00:09:23.609 --> 00:09:25.832
Agora temos estes parâmetros

00:09:25.865 --> 00:09:29.769
que foram automaticamente criados
e inicializados para nós.

00:09:30.139 --> 00:09:33.640
Se você quiser reinicializar

00:09:33.673 --> 00:09:35.608
estes parâmetros
como pesos e vieses,

00:09:35.642 --> 00:09:39.813
podemos escrever o viés assim.

00:09:39.846 --> 00:09:42.882
E para obter os dados adjacentes,
você escreve .data,

00:09:42.916 --> 00:09:47.554
e agora podemos preencher
com zeros.

00:09:47.587 --> 00:09:50.390
O viés que está anexado
a esta operação linear

00:09:50.423 --> 00:09:52.392
na 1ª camada
totalmente conectada

00:09:52.425 --> 00:09:54.561
está preenchido com zeros.

00:09:54.594 --> 00:09:59.999
Da mesma forma, você pode olhar
seus pesos. Escrevo .data.

00:10:00.033 --> 00:10:04.037
Suponha que queiramos fazer
uma distribuição normal.

00:10:04.070 --> 00:10:08.341
Queremos inicializar
estes pesos aleatórios

00:10:08.375 --> 00:10:14.280
com uma distribuição normal
com um desvio padrão de 0,01.

00:10:15.772 --> 00:10:17.149
Pronto.

00:10:17.182 --> 00:10:20.587
É conveniente,
porque as matrizes de pesos e vieses

00:10:20.620 --> 00:10:23.556
estão sempre disponíveis
para você no seu modelo.

00:10:23.590 --> 00:10:25.425
Você escreve
model. qualquer camada,

00:10:25.458 --> 00:10:29.429
qualquer operação, e .weight
e você obtém o peso.

00:10:29.462 --> 00:10:30.897
Agora que temos uma rede,

00:10:30.930 --> 00:10:34.100
vou passar alguns dados por ela
e ver como fica.

00:10:34.470 --> 00:10:36.369
Recebemos os dados
do trainloader,

00:10:36.403 --> 00:10:40.473
e ele retorna imagens e rótulos
para cada uma dessas imagens.

00:10:40.507 --> 00:10:43.777
Rótulos são as categorias
às quais as imagens pertencem.

00:10:43.810 --> 00:10:46.546
Se tem uma imagem de 0,
então o rótulo é 0,

00:10:46.579 --> 00:10:49.549
se tem uma imagem de 5,
o rótulo é 5 e assim por diante.

00:10:49.582 --> 00:10:55.088
Em seguida, escrevemos
next(iter(trainloader)).

00:10:55.121 --> 00:10:58.658
O trainloader retorna
um gerador.

00:10:58.692 --> 00:11:02.028
Para transformá-lo em algo
em que você possa iterar,

00:11:02.062 --> 00:11:03.204
você escreve iter,

00:11:03.237 --> 00:11:06.399
e para obter o primeiro valor dele,
você escreve next.

00:11:06.433 --> 00:11:08.301
Sempre que você usar o next,

00:11:08.335 --> 00:11:13.373
ele vai fornecer o próximo lote
em seus dados de treinamento.

00:11:13.742 --> 00:11:17.043
E precisamos redimensionar
as imagens,

00:11:17.077 --> 00:11:19.512
pois elas vêm com 28x28
de tamanho

00:11:19.546 --> 00:11:22.525
e precisamos transformá-las
num vetor de 784 de comprimento.

00:11:22.559 --> 00:11:27.954
Então escrevemos images.resize,
o tamanho do lote -

00:11:27.987 --> 00:11:33.760
é tipo o número de canais de cor -
e depois 784.

00:11:33.793 --> 00:11:39.165
Um jeito melhor de fazer isto
é passar o tamanho do lote

00:11:39.199 --> 00:11:41.735
de onde você recebeu
os tensores de imagem.

00:11:41.768 --> 00:11:45.071
Então este é
o primeiro elemento de shape.

00:11:45.105 --> 00:11:49.009
Não faça isto, porque muitas vezes
não sabemos o tamanho do lote

00:11:49.042 --> 00:11:50.710
com antecedência.

00:11:50.744 --> 00:11:53.880
Em geral, você não quer fazer
a codificação rígida

00:11:53.913 --> 00:11:57.450
na sua rede, nos seus tensores
ou em qualquer lugar.

00:11:57.484 --> 00:11:59.619
Então você faz algo assim.

00:11:59.652 --> 00:12:01.921
Com as imagens
redimensionadas,

00:12:01.955 --> 00:12:03.689
agora podemos passá-las
na rede.

00:12:03.723 --> 00:12:06.592
Para fazer isto,
escreva model.forwards.

00:12:06.626 --> 00:12:10.062
Vou colocar isto em ps,
que significa probabilidades.

00:12:10.096 --> 00:12:12.431
Escrevo forward,
passo a imagem.

00:12:12.465 --> 00:12:14.367
Vamos pegar
esta primeira imagem.

00:12:14.401 --> 00:12:16.802
Isto vai calcular
nossas probabilidades.

00:12:16.836 --> 00:12:21.273
Agora vou usar esta função
e o arquivo helper.

00:12:21.307 --> 00:12:25.645
Helpers são os arquivos que escrevi
que facilitam as coisas.

00:12:25.679 --> 00:12:31.150
Quero ver como nossa imagem é,
e como a rede a classificou.

00:12:31.184 --> 00:12:33.819
Qual é a previsão dela
para a classe.

00:12:33.853 --> 00:12:38.157
A imagem que vamos usar aqui
é 0.

00:12:38.191 --> 00:12:41.894
Desta vez, vamos convertê-la
de volta

00:12:41.928 --> 00:12:43.404
para uma imagem de 28 por 28.

00:12:43.438 --> 00:12:44.797
Vou usar view aqui.

00:12:44.831 --> 00:12:48.868
View é basicamente
como a resize,

00:12:48.902 --> 00:12:51.771
exceto que ela retorna
um novo tensor.

00:12:51.805 --> 00:12:53.823
Resize tem um sublinhado
no final,

00:12:53.857 --> 00:12:55.658
o que significa
que é feito no lugar,

00:12:55.692 --> 00:12:58.177
mas view é como resizing
e reshaping,

00:12:58.211 --> 00:13:00.063
mas ela retorna
um novo tensor.

00:13:00.475 --> 00:13:02.181
Podemos passar nossa imagem

00:13:02.215 --> 00:13:04.550
e passamos a distribuição
de probabilidade.

00:13:04.584 --> 00:13:06.079
Digitei errado.

00:13:06.113 --> 00:13:09.655
Você pode ver aqui que a imagem
que passamos na rede é um 7,

00:13:09.689 --> 00:13:11.590
e está tentando fazer
previsões,

00:13:11.624 --> 00:13:14.760
mas como os pesos são aleatórios,
nós não a treinamos,

00:13:14.794 --> 00:13:17.897
a rede não tem ideia
de qual dígito é.

00:13:17.931 --> 00:13:19.899
Ela está dando
palpites aleatórios,

00:13:19.933 --> 00:13:23.336
onde as probabilidades são
praticamente as mesmas.

00:13:23.370 --> 00:13:27.306
A distribuição é uniforme
porque não a treinamos ainda.

00:13:27.340 --> 00:13:29.842
PyTorch fornece
um modo mais conveniente

00:13:29.876 --> 00:13:34.313
para construir um modelo,
chamado "nn.Sequential".

00:13:34.683 --> 00:13:37.216
Vou mostrar como fazer isto.

00:13:37.250 --> 00:13:40.720
Primeiro vamos definir
os hiperparâmetros,

00:13:40.754 --> 00:13:45.691
que são parâmetros que definem
a arquitetura da sua rede,

00:13:45.725 --> 00:13:48.928
e os parâmetros são os próprios
pesos e vieses.

00:13:48.962 --> 00:13:52.865
Por exemplo, o tamanho do seu input
será 784.

00:13:52.899 --> 00:13:55.801
Podemos definir o tamanho
das nossas camadas ocultas.

00:13:55.835 --> 00:13:59.005
Vai ser 128 e 64,

00:13:59.039 --> 00:14:03.209
e podemos definir que o tamanho
do output é igual a 10.

00:14:03.243 --> 00:14:06.712
Com o nn.Sequential, podemos dizer
que nosso modelo é isto.

00:14:06.746 --> 00:14:09.949
Então você passa as operações,

00:14:09.983 --> 00:14:12.084
transformações e módulos
que queira fazer.

00:14:12.118 --> 00:14:19.225
Escrevo nn.Linear
(input_size, hidden_size[0]))

00:14:19.259 --> 00:14:20.926
Este é o primeiro módulo.

00:14:20.960 --> 00:14:24.296
Queremos que ele passe
pela ativação ReLU.

00:14:24.330 --> 00:14:26.632
Escrevo nn.ReLU,

00:14:27.241 --> 00:14:30.870
e a segunda
transformação linear,

00:14:32.143 --> 00:14:34.015
outra ReLU

00:14:34.048 --> 00:14:36.739
e, finalmente, o output

00:14:37.479 --> 00:14:39.848
e a softmax.

00:14:40.380 --> 00:14:42.415
Nós imprimimos o modelo,

00:14:42.449 --> 00:14:47.019
vemos que é um modelo sequencial
e temos esta transformação linear,

00:14:47.053 --> 00:14:50.756
ReLU, transformação linear,
ReLU, linear e softmax.

00:14:50.790 --> 00:14:54.727
É praticamente o mesmo que a rede
que construímos antes,

00:14:54.761 --> 00:14:56.896
exceto que é
bem mais simples de fazer

00:14:56.930 --> 00:14:58.397
e em poucas linhas de código.

00:14:58.431 --> 00:15:02.768
Agora podemos usar a função
forward aqui e ver que é o mesmo.

00:15:02.802 --> 00:15:04.203
Colocamos outro 7.

00:15:04.237 --> 00:15:09.475
Como a rede ainda não está treinada,
ela não sabe o que é o 7.

00:15:09.509 --> 00:15:12.945
Outra coisa legal que pode ser feita
com Sequential

00:15:12.979 --> 00:15:16.215
é passar o OrderedDict,
"in order to dictionary",

00:15:16.249 --> 00:15:21.420
para nomear cada uma das camadas
em seu modelo sequencial.

00:15:21.454 --> 00:15:23.322
Para conseguir um OrderedDict,

00:15:23.356 --> 00:15:27.727
escreva from collections
import OrderedDict.

00:15:27.761 --> 00:15:32.565
Então podemos construir o modelo,
colocamos o nn.Sequencial.

00:15:32.599 --> 00:15:37.403
E o OrderedDict pega
uma lista de tuplas

00:15:37.437 --> 00:15:41.641
que constroem os valores
e chaves do dicionário.

00:15:41.675 --> 00:15:45.711
As chaves serão
os nomes das camadas,

00:15:45.745 --> 00:15:48.347
e os valores serão
as próprias operações.

00:15:48.381 --> 00:15:53.386
Podemos chamar nossa primeira
camada de "fc1", por exemplo,

00:15:53.420 --> 00:15:57.356
e dado o aspecto linear normal,
deduzi o resto da rede.

00:15:57.390 --> 00:16:01.694
Chamamos esta de fc1,
e o primeiro ReLU é relu1,

00:16:01.728 --> 00:16:04.530
fc2, relu2, output e softmax.

00:16:04.564 --> 00:16:06.632
Uma coisa a se notar
e lembrar é que,

00:16:06.666 --> 00:16:08.868
num dicionário,
as chaves têm que ser únicas.

00:16:08.902 --> 00:16:13.673
Então você tem que chamar
este de relu1 ou o que seja,

00:16:13.707 --> 00:16:18.344
e esta ativação ReLU
tem que ter um nome diferente.

00:16:18.378 --> 00:16:20.579
Agora podemos olhar
nosso modelo.

00:16:20.613 --> 00:16:26.285
Podemos ver que as camadas
têm um nome anexado a elas.

00:16:26.319 --> 00:16:29.288
Então, fc2
é esta operação linear,

00:16:29.321 --> 00:16:33.125
e podemos colocar fc2 aqui.

00:16:33.159 --> 00:16:34.894
E pronto. Estão nomeadas.

00:16:34.928 --> 00:16:37.797
Você pode acessá-las
como atributos,

00:16:37.831 --> 00:16:42.268
como definimos na rede original,
e seguir daí.

00:16:42.890 --> 00:16:47.173
Agora é sua vez de construir
uma rede.

00:16:47.207 --> 00:16:49.542
Use qualquer um dos métodos
que mencionei.

00:16:49.576 --> 00:16:52.078
Quero que você construa
uma rede

00:16:52.112 --> 00:16:55.214
para classificar imagens
com três camadas ocultas.

00:16:55.248 --> 00:17:00.553
A primeira deve ter 400 unidades,
a segunda, 200, e a última, 100.

00:17:00.587 --> 00:17:03.155
Use funções de ativação ReLU
em cada uma delas

00:17:03.189 --> 00:17:05.725
e softmax na camada de output.

00:17:05.759 --> 00:17:07.893
Você ainda não está treinando
a rede,

00:17:07.927 --> 00:17:10.062
então ela não vai poder
fazer previsões.

00:17:10.096 --> 00:17:13.632
No próximo vídeo,
você aprenderá a fazer isto,

00:17:13.666 --> 00:17:15.668
e será capaz de treinar
sua rede.

00:17:15.702 --> 00:17:18.138
Vejo você no próximo vídeo.
Até mais.

