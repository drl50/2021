WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.879
大家好 欢迎回来 在这个视频中

00:00:02.879 --> 00:00:07.580
我将演示如何使用 PyTorch 构建神经网络

00:00:07.580 --> 00:00:09.599
在此 notebook 的结尾 你将自己构建一个神经网络

00:00:09.599 --> 00:00:11.949
我会提供此 notebook

00:00:11.949 --> 00:00:13.259
我们开始吧

00:00:13.259 --> 00:00:16.214
首先像往常一样导入库

00:00:16.214 --> 00:00:19.245
要导入 PyTorch 输入 import torch

00:00:19.245 --> 00:00:21.179
还将导入这个软件包 称为 torchvision

00:00:21.179 --> 00:00:23.439
它与 PyTorch 相关

00:00:23.440 --> 00:00:28.530
使我们能够下载并使用一些现有的数据集

00:00:28.530 --> 00:00:31.425
在此视频中 我们将使用的数据集称为 MNIST

00:00:31.425 --> 00:00:35.600
MNIST 数据集是一堆手写数字的图像

00:00:35.600 --> 00:00:37.795
从 0 到 9

00:00:37.795 --> 00:00:42.234
此数据集用于训练网络或其他机器学习模型

00:00:42.234 --> 00:00:47.240
使它们能够将图像分类为数字

00:00:47.240 --> 00:00:53.015
显示 8 的图像将被分类为数字 8

00:00:53.015 --> 00:00:56.439
我们将在此示例中使用该数据集

00:00:56.439 --> 00:01:01.724
我使用 torchvision 下载并加载训练数据

00:01:01.725 --> 00:01:07.344
这是我们可以获取并放入 trainset 的 MNIST 数据集

00:01:07.344 --> 00:01:08.939
在这里输入 download=True

00:01:08.939 --> 00:01:11.694
如果磁盘上没有该数据集

00:01:11.694 --> 00:01:13.649
它将为我们下载该数据集

00:01:13.650 --> 00:01:16.264
在这里提供转换

00:01:16.263 --> 00:01:19.879
它的作用是读取这些图像

00:01:19.879 --> 00:01:24.864
然后应用这些转换 生成传入网络的数据集

00:01:24.864 --> 00:01:29.449
看起来这样

00:01:29.450 --> 00:01:35.900
首先将这些图像转换为 PyTorch 张量 然后标准化

00:01:35.900 --> 00:01:41.180
在此示例中 标准化是指

00:01:41.180 --> 00:01:47.110
对这些灰阶图像进行处理

00:01:47.109 --> 00:01:50.599
每个像素值是从 0 到 1 的浮点数

00:01:50.599 --> 00:01:55.179
但是我们希望像素值是从 -1 到 1

00:01:55.180 --> 00:01:58.750
这部分代码的作用是减 0.5

00:01:58.750 --> 00:02:00.105
这是均值

00:02:00.105 --> 00:02:02.359
从每个颜色通道 即每个像素减去 0.5

00:02:02.359 --> 00:02:05.935
然后除以 0.5

00:02:05.935 --> 00:02:10.015
这部分的作用是将从 0 到 1 的范围

00:02:10.014 --> 00:02:14.474
变成从 -0.5 到 0.5 然后除以 0.5 或乘以 2

00:02:14.474 --> 00:02:19.324
使方差范围扩展为 -1 到 1

00:02:19.324 --> 00:02:22.774
这样使神经网络更容易学习

00:02:22.775 --> 00:02:26.840
几乎每次将数据传入神经网络之前 都需要标准化数据

00:02:26.840 --> 00:02:28.849
需要注意的另一点是

00:02:28.849 --> 00:02:31.314
我将批次大小设为 64

00:02:31.314 --> 00:02:34.789
意思是当我们从 trainloader 获取数据时

00:02:34.789 --> 00:02:37.759
每次将获得 64 张图像

00:02:37.759 --> 00:02:39.794
加载这个 然后加载数据

00:02:39.794 --> 00:02:45.694
现在可以看看其中一张图像的样貌了

00:02:45.694 --> 00:02:48.799
这是一个示例 是一个数字 7

00:02:48.800 --> 00:02:51.340
这是一个手写数字

00:02:51.340 --> 00:02:54.245
我们将用神经网络对其进行分类

00:02:54.245 --> 00:02:59.634
我们将用 MNIST 构建这个示例网络

00:02:59.634 --> 00:03:03.125
对于 MNIST 图像是 28 x 28

00:03:03.125 --> 00:03:07.240
我们需要将它们转换为

00:03:07.240 --> 00:03:11.590
长度为 28 x 28 的向量

00:03:11.590 --> 00:03:14.860
即 28 x 28=784

00:03:14.860 --> 00:03:16.380
这是输入层的大小

00:03:16.379 --> 00:03:18.384
我们将接受图像 将它们转换为向量

00:03:18.384 --> 00:03:21.509
然后在这里作为输入传入网络中

00:03:21.509 --> 00:03:24.444
我们将使用两个隐藏层

00:03:24.444 --> 00:03:27.824
第一个有 128 个单元 第二个有 64 个单元

00:03:27.824 --> 00:03:30.639
这个将是输出层 有 10 个单元

00:03:30.639 --> 00:03:33.379
这个输出层有 10 个单元

00:03:33.379 --> 00:03:36.009
因为我们想要分类这些数字 共有 10 个数字

00:03:36.009 --> 00:03:40.649
这个对应数字 0 对应 1 2 等等

00:03:40.650 --> 00:03:44.689
每个隐藏层的单元数量

00:03:44.689 --> 00:03:49.120
以及隐藏层的数量是不定的

00:03:49.120 --> 00:03:53.620
通常 层级中的单元越多 层级越多

00:03:53.620 --> 00:03:56.719
网络就越能拟合数据

00:03:56.719 --> 00:04:00.500
训练神经网络的一大部分工作是

00:04:00.500 --> 00:04:05.294
确定最佳单元数量和最佳层级数量

00:04:05.294 --> 00:04:08.149
对于激活函数来说

00:04:08.150 --> 00:04:10.740
隐藏层将使用 ReLU

00:04:10.740 --> 00:04:12.064
输出层将使用 softmax

00:04:12.064 --> 00:04:15.449
softmax 接受这些输出单元的值

00:04:15.449 --> 00:04:23.560
并将它们转换为概率分布

00:04:23.560 --> 00:04:27.079
它会将所有这些值压扁到 0 和 1 之间

00:04:27.079 --> 00:04:31.350
然后除以所有值的和

00:04:31.350 --> 00:04:34.620
如果将每个单元的值相加 会等于 1

00:04:34.620 --> 00:04:39.170
这是一个离散概率分布

00:04:39.170 --> 00:04:44.210
表示输入属于这个类别的概率

00:04:44.209 --> 00:04:45.734
要训练此网络

00:04:45.735 --> 00:04:49.595
我们需要一个损失函数 我们将使用交叉熵损失函数

00:04:49.595 --> 00:04:53.600
它的作用是

00:04:53.600 --> 00:04:56.300
将 softmax 层的概率或预测

00:04:56.300 --> 00:05:00.180
与真实类别进行比较 然后使用二者之差（即损失）

00:05:00.180 --> 00:05:03.550
更新网络中的权重

00:05:03.550 --> 00:05:06.189
开始构建网络

00:05:06.189 --> 00:05:13.170
首先 我们需要从 PyTorch 导入一些模块

00:05:13.170 --> 00:05:17.060
输入 from torch import nn

00:05:17.060 --> 00:05:20.139
它是神经网络函数

00:05:20.139 --> 00:05:26.495
然后输入 import torch.nn.functional as F

00:05:26.495 --> 00:05:29.230
有更多专门针对神经网络的函数

00:05:29.230 --> 00:05:33.390
这是这些函数的函数形式

00:05:33.389 --> 00:05:36.294
导入这些内容后 可以开始构建网络了

00:05:36.295 --> 00:05:38.585
在 PyTorch 中的基本方式是

00:05:38.584 --> 00:05:43.094
创建一个类

00:05:43.095 --> 00:05:49.295
并不重要 因为这个类 Network 是 nn.Module 的子类

00:05:49.295 --> 00:05:52.020
然后在 init 函数中

00:05:52.019 --> 00:05:54.329
首先需要调用 super()

00:05:54.329 --> 00:05:56.449
super() 的作用是

00:05:56.449 --> 00:06:03.594
调用此子类所继承的类的函数或属性

00:06:03.595 --> 00:06:06.745
这行代码的作用是

00:06:06.745 --> 00:06:10.569
调用 nn.Module 的 init 方法

00:06:10.569 --> 00:06:13.985
完成后 我们将定义要使用的层级

00:06:13.985 --> 00:06:19.045
定义神经网络架构的运算

00:06:19.045 --> 00:06:21.300
为此 我们将称之为 self.fc1

00:06:21.300 --> 00:06:22.889
表示全连接层

00:06:22.889 --> 00:06:26.389
fc 是线性的

00:06:26.389 --> 00:06:28.680
看看这里的解释

00:06:28.680 --> 00:06:32.855
它会向输入数据应用线性转换

00:06:32.855 --> 00:06:36.379
我们需要在里面设定输入大小

00:06:36.379 --> 00:06:39.415
然后设定输出大小

00:06:39.415 --> 00:06:42.340
我们的输入层有 784 个单元

00:06:42.339 --> 00:06:46.264
我们希望第一个隐藏层有 128 个单元

00:06:46.264 --> 00:06:51.245
第二个层级相似 128 到 64

00:06:51.245 --> 00:06:55.084
输出层称为 fc3

00:06:55.084 --> 00:07:00.489
从 64 到 10

00:07:00.490 --> 00:07:04.590
这些差不多就是在这部分需要定义的所有运算

00:07:04.589 --> 00:07:08.049
要使这个成为神经网络

00:07:08.050 --> 00:07:11.915
我们需要定义 forward 函数

00:07:11.915 --> 00:07:15.310
从 nn.Module 构建的所有 PyTorch 网络

00:07:15.310 --> 00:07:20.180
必须具有这个 forward 函数

00:07:20.180 --> 00:07:26.030
该函数要求 x 是一个 PyTorch 张量

00:07:26.029 --> 00:07:28.549
你将这个张量传入每个层级

00:07:28.550 --> 00:07:31.305
即传入每个运算

00:07:31.305 --> 00:07:37.250
例如 要传入第一层级 输入 self.fc1(x)

00:07:37.250 --> 00:07:42.350
将张量 x 传入第一层级

00:07:42.350 --> 00:07:46.470
传入这个线性运算 生成另一个张量

00:07:46.470 --> 00:07:50.390
对该张量应用 ReLu 激活函数

00:07:50.389 --> 00:07:55.209
然后进入另一层级

00:07:55.209 --> 00:07:57.734
差不多就是这么持续下去

00:07:57.735 --> 00:08:02.670
最终 定义 softmax 函数

00:08:02.670 --> 00:08:06.280
我们需要定义

00:08:06.279 --> 00:08:09.224
计算 softmax 所在的维度

00:08:09.225 --> 00:08:13.540
之前我提到 图像将批次处理

00:08:13.540 --> 00:08:18.500
每批将有 64 张不同的图像

00:08:18.500 --> 00:08:23.420
对于张量大小来说 批次大小是第一个

00:08:23.420 --> 00:08:25.395
即维度 0

00:08:25.394 --> 00:08:31.299
传入网络的实际向量是第二个维度

00:08:31.300 --> 00:08:38.085
这个张量最终将是 64x10 64 是批次大小

00:08:38.085 --> 00:08:43.740
softmax 所在的维度有 10 个值

00:08:43.740 --> 00:08:45.375
因此这里是维度 1

00:08:45.375 --> 00:08:47.730
最后输入 return x

00:08:47.730 --> 00:08:49.909
可以像这样创建模型了 创建网络了

00:08:49.909 --> 00:08:53.894
看看结果如何

00:08:53.894 --> 00:08:57.159
这里显示的是我们在 init 函数中

00:08:57.159 --> 00:09:01.904
注册的这些模块 这些运算

00:09:01.904 --> 00:09:06.954
我们创建了这些线性运算 这些线性转换

00:09:06.955 --> 00:09:10.285
系统自动为我们创建了权重和偏差

00:09:10.284 --> 00:09:16.794
输出 model.fc1.weight

00:09:16.794 --> 00:09:22.474
然后还输出 fc1.bias

00:09:22.475 --> 00:09:25.250
可以看到这些参数

00:09:25.250 --> 00:09:29.815
系统自动为我们创建并初始化了这些参数

00:09:29.815 --> 00:09:35.665
如果我们想重新初始化这些参数 这些偏差和权重

00:09:35.664 --> 00:09:39.855
可以这样获得偏差

00:09:39.855 --> 00:09:41.889
然后获得底层数据

00:09:41.889 --> 00:09:47.595
输入 .data 并用 0 填充

00:09:47.595 --> 00:09:50.800
现在 与第一个全连接层中的线性运算

00:09:50.799 --> 00:09:54.620
相关的偏差被 0 填充

00:09:54.620 --> 00:09:59.080
同样 可以查看权重 输入 .data

00:09:59.090 --> 00:10:04.100
假设我们想要获得正态分布

00:10:04.100 --> 00:10:07.555
我们想要用正态分布初始化这些随机权重

00:10:07.554 --> 00:10:14.309
标准偏差为 0.01

00:10:15.120 --> 00:10:19.164
搞定 很方便

00:10:19.164 --> 00:10:23.605
因为始终可以在模型中查看偏差和权重指标

00:10:23.605 --> 00:10:25.470
只需输入 model. 然后是任何层级名称

00:10:25.470 --> 00:10:29.490
任何运算 然后是 .weight 获得权重

00:10:29.490 --> 00:10:31.480
我们已经有了一个网络

00:10:31.480 --> 00:10:34.164
我将传入一些数据 看看效果如何

00:10:34.164 --> 00:10:37.549
从 trainloader 获取数据

00:10:37.549 --> 00:10:40.519
它会返回图像和每个图像的标签

00:10:40.519 --> 00:10:43.840
标签是指图像所属的类别

00:10:43.840 --> 00:10:46.610
如果图像是 0 则标签是 0

00:10:46.610 --> 00:10:49.610
如果图像是 5 则标签是 5 等等

00:10:49.610 --> 00:10:55.134
调用 next(iter(trainloader))

00:10:55.134 --> 00:10:58.710
trainloader 返回一个生成器

00:10:58.710 --> 00:11:02.065
要使其变成可以遍历的数据

00:11:02.065 --> 00:11:06.455
调用 iter 要获得第一个值 调用 next

00:11:06.455 --> 00:11:08.360
每次调用 next

00:11:08.360 --> 00:11:13.430
它都会返回训练数据中的下个批次

00:11:13.429 --> 00:11:17.779
对于图像 我们需要调整图像尺寸

00:11:17.779 --> 00:11:22.089
因为它们是 28 x 28 图像 我们需要使其变成 784 长的向量

00:11:22.090 --> 00:11:28.019
输入 images.resize 这是批次大小

00:11:28.019 --> 00:11:33.824
这是颜色通道 然后输入 784

00:11:33.825 --> 00:11:37.855
更好的处理方式是

00:11:37.855 --> 00:11:41.779
传入从图像张量本身获取的批次大小

00:11:41.779 --> 00:11:45.214
这是 shape 的第一个元素

00:11:45.215 --> 00:11:47.820
但是不会经常这么做

00:11:47.820 --> 00:11:50.754
因为提前不知道批次大小是多少

00:11:50.754 --> 00:11:53.929
通常 不能将此数字硬编码到

00:11:53.929 --> 00:11:57.500
网络 张量或任何位置

00:11:57.500 --> 00:11:59.664
不能这么做

00:11:59.664 --> 00:12:01.959
调整图像大小后

00:12:01.960 --> 00:12:03.750
现在可以传入网络中了

00:12:03.750 --> 00:12:06.639
为此 输入 model.forward

00:12:06.639 --> 00:12:07.970
将这个放在 ps 中

00:12:07.970 --> 00:12:10.125
ps 表示概率

00:12:10.125 --> 00:12:12.475
输入 forward 传入图像

00:12:12.475 --> 00:12:14.409
获取第一个图像

00:12:14.409 --> 00:12:16.839
这样就可以计算概率

00:12:16.840 --> 00:12:21.310
现在将使用这个函数 使用帮助程序文件

00:12:21.309 --> 00:12:25.699
我写的帮助程序可以使代码编写起来更轻松

00:12:25.700 --> 00:12:31.210
我要查看下图像是什么 网络是如何对其分类的

00:12:31.210 --> 00:12:33.884
类别预测是什么

00:12:33.884 --> 00:12:38.220
输入 images[0]

00:12:38.220 --> 00:12:43.180
这次需要将其变回 28x28 的图像

00:12:43.179 --> 00:12:44.849
在这里使用 view

00:12:44.850 --> 00:12:51.925
view 和 resize 很像 但是它返回一个新的张量

00:12:51.924 --> 00:12:55.574
这个 resize_ 有个下划线 表示原地操作

00:12:55.575 --> 00:12:57.600
view 和 resize reshape 相似

00:12:57.600 --> 00:13:00.045
但是它返回一个新的张量

00:13:00.044 --> 00:13:02.659
可以传入图像 传入概率分布

00:13:02.659 --> 00:13:05.839
这里输错了

00:13:05.840 --> 00:13:07.384
可以在这里看出

00:13:07.384 --> 00:13:10.960
我们传入网络中的图像是数字 7

00:13:10.960 --> 00:13:14.530
它尝试做出一些预测 但是因为权重是随机的 我们尚未训练网络

00:13:14.529 --> 00:13:17.949
网络根本不知道该数字是什么

00:13:17.950 --> 00:13:20.440
它只是进行随机猜测

00:13:20.440 --> 00:13:23.380
所有概率都差不多

00:13:23.379 --> 00:13:27.360
现在是一个均匀分布 因为尚未训练网络

00:13:27.360 --> 00:13:34.355
PyTorch 提供了构建模型的更方便方式 叫做 nn.Sequential

00:13:34.355 --> 00:13:37.254
我将演示下如何使用此方式

00:13:37.254 --> 00:13:40.759
首先需要定义超参数

00:13:40.759 --> 00:13:45.754
超参数是用来定义网络架构的参数

00:13:45.754 --> 00:13:48.985
参数是权重和偏差本身

00:13:48.985 --> 00:13:52.925
例如 输入大小是 784

00:13:52.924 --> 00:13:55.849
定义隐藏层的大小

00:13:55.850 --> 00:14:03.264
输入 128 和 64 然后定义输出大小 等于 10

00:14:03.264 --> 00:14:06.754
输入 model = nn.Sequential

00:14:06.754 --> 00:14:10.000
传入运算

00:14:10.000 --> 00:14:12.120
即要执行的转换 模块

00:14:12.120 --> 00:14:20.970
输入 nn.Linear(input_size, hidden_size[0] 这是第一个模块

00:14:20.970 --> 00:14:24.335
我们希望应用 ReLu 激活

00:14:24.335 --> 00:14:33.730
输入 nn.ReLU 然后是第二个线性转换 然后是另一个 ReLU

00:14:33.730 --> 00:14:40.394
最终是输出 然后是 softmax

00:14:40.394 --> 00:14:42.475
输出模型

00:14:42.475 --> 00:14:47.055
可以看出它是一个序列模型 这里是线性转换

00:14:47.054 --> 00:14:48.875
ReLU 线性转换 ReLU

00:14:48.875 --> 00:14:50.820
线性转换 然后是 softmax

00:14:50.820 --> 00:14:54.770
基本上和之前构建的网络相同

00:14:54.769 --> 00:14:58.345
但是创建起来更简单 代码行更少

00:14:58.345 --> 00:15:02.825
我们可以前向传播此网络 看到结果一样

00:15:02.825 --> 00:15:04.265
输入另一个数字 7

00:15:04.264 --> 00:15:06.009
因为网络尚未训练

00:15:06.009 --> 00:15:09.534
不知道 7 到底什么样

00:15:09.534 --> 00:15:14.615
对于序列模型 有一个很酷的功能是可以传入 OrderedDict

00:15:14.615 --> 00:15:16.269
即有序字典

00:15:16.269 --> 00:15:21.475
用于命名序列模型中的每个层级

00:15:21.475 --> 00:15:23.370
要获取 OrderedDict

00:15:23.370 --> 00:15:27.794
输入 from collections import OrderedDict

00:15:27.794 --> 00:15:32.615
然后可以构建序列模型

00:15:32.615 --> 00:15:41.685
OrderedDict 接受一个元组列表 用于构建字典键值

00:15:41.684 --> 00:15:44.234
键将是层级名称

00:15:44.235 --> 00:15:48.269
值将是运算本身

00:15:48.269 --> 00:15:52.289
例如 可以将第一个层级命名为 fc1

00:15:52.289 --> 00:15:57.414
然后是正常的线性转换和后续网络运算

00:15:57.414 --> 00:16:01.750
这个称为 fc1 第一个 ReLU 是 relu1

00:16:01.750 --> 00:16:04.568
fc2 relu2 output softmax

00:16:04.568 --> 00:16:08.934
需要注意的一点是 在字典中 键必须是唯一的

00:16:08.934 --> 00:16:12.939
将这个命名为 relu1 或任何其他名称

00:16:12.940 --> 00:16:18.380
这个 ReLU 激活函数必须具有不同的名称

00:16:18.379 --> 00:16:20.625
现在看看我们的模型

00:16:20.625 --> 00:16:26.330
可以看出 现在每个层级都有一个关联的名称

00:16:26.330 --> 00:16:33.165
fc2 是这个线性运算 我们可以在这里获取 fc2

00:16:33.164 --> 00:16:34.939
就这样 它们都有了名称

00:16:34.940 --> 00:16:37.840
你可以访问它们

00:16:37.840 --> 00:16:42.230
就像在原始网络中定义的属性一样 然后继续操作

00:16:42.419 --> 00:16:47.209
现在该你来构建网络了

00:16:47.210 --> 00:16:49.605
你可以使用我在本视频中讲解的任何方法

00:16:49.605 --> 00:16:54.105
请构建一个分类 MNIST 图像的网络

00:16:54.105 --> 00:16:57.289
必须有三个隐藏层 第一个隐藏层应该有 400 个单元

00:16:57.289 --> 00:17:00.610
第二个应该有 200 个单元 最后一个应该有 100 个单元

00:17:00.610 --> 00:17:02.860
确保在每个隐藏层上使用 ReLU 激活函数

00:17:02.860 --> 00:17:05.765
在输出层使用 softmax

00:17:05.765 --> 00:17:07.940
暂时网络还没有训练

00:17:07.940 --> 00:17:10.120
因此无法做出预测

00:17:10.119 --> 00:17:11.879
但是在下个视频 notebook 中

00:17:11.880 --> 00:17:13.700
你将学习这方面的知识

00:17:13.700 --> 00:17:15.730
使你能够训练网络

00:17:15.730 --> 00:17:18.000
下个视频见 加油

