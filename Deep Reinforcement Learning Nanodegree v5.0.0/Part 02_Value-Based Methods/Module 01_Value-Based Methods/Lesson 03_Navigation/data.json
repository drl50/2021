{
  "data": {
    "lesson": {
      "id": 616711,
      "key": "69bd42c6-b70e-4866-9764-9bfa8c03cdea",
      "title": "Navigation",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Train an agent to navigate a large world and collect yellow bananas, while avoiding blue bananas.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/69bd42c6-b70e-4866-9764-9bfa8c03cdea/616711/1544455873992/Navigation+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/69bd42c6-b70e-4866-9764-9bfa8c03cdea/616711/1544455871839/Navigation+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": {
        "key": "f1fbcd5b-47ec-4ef1-9c93-4cec919147fd",
        "version": "1.0.0",
        "locale": "en-us",
        "duration": 70560,
        "semantic_type": "Project",
        "title": "Navigation",
        "description": "## Project Overview\n---\nFor this project, you will train an agent to navigate (and collect bananas!) in a large, square world. \n\nA reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.  \n\nThe state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction.  Given this information, the agent has to learn how to best select actions.  Four discrete actions are available, corresponding to:\n- **`0`** - move forward.\n- **`1`** - move backward.\n- **`2`** - turn left.\n- **`3`** - turn right.\n\nThe task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes.\n\n## Evaluation\n---\nYour project will be reviewed by a Udacity reviewer against the [project rubric](https://review.udacity.com/#!/rubrics/1889/view). Review this rubric thoroughly, and self-evaluate your project before submission. All criteria found in the rubric must meet specifications for you to pass.\n\n## Project Submission\n--\n\nWe suggest that you submit a Github repository, but if you have trouble with that submission or want to submit a zip file instead, continue with the following instructions:\n\nWhen you are ready to submit your project, collect the following files and compress them into a single zip archive for upload:\n- The `Navigation.ipynb` file with fully functional code, **all code cells executed and displaying output**, and **all questions answered**. You can also download this via your workspace by clicking download as..\n- A `README.md` markdown file with a description of your code, much like [this one](https://github.com/udacity/deep-reinforcement-learning/blob/master/p1_navigation/README.md).\n- An HTML or PDF export of the project report with the name `Report.html` or `Report.pdf`.\n- A file with the saved model weights of the successful agent, can be named something like `model.pt`.\n- Any additional images used for the project that were not supplied to you for the project. __Please do not include the large banana, project data sets that you may download to work with.  These files will make your project too large to submit.__\n\nAgain, alternatively, your submission could consist of the GitHub link to your repository.\n\n## Project Submission Checklist\n\n**Before submitting your project, please review and confirm the following items.** \n<input type=\"checkbox\"> I am confident all rubric items have been met and my project will pass as submitted. \n<input type=\"checkbox\"> Project builds correctly without errors and runs.\n<input type=\"checkbox\"> All required functionality exists and my project behaves as expected per the project's specifications.\n\n**Once you have checked all these items, you are ready to submit!**\n\n\n\n## Ready to submit your project?\n---\nClick on the \"Submit Project\" button and follow the instructions to submit!",
        "is_public": true,
        "summary": null,
        "forum_path": "",
        "rubric_id": "1889",
        "terminal_project_id": null,
        "resources": null,
        "image": {
          "url": "https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5af79225_banana/banana.jpg",
          "width": 1162,
          "height": 1024
        }
      },
      "lab": null,
      "concepts": [
        {
          "id": 613315,
          "key": "9812fef1-aa14-414e-a86c-fabac3585387",
          "title": "Unity ML-Agents",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9812fef1-aa14-414e-a86c-fabac3585387",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 613317,
              "key": "94747266-e6fc-4614-973c-2bfb097deed1",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ad8aca0_unity-wide/unity-wide.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/94747266-e6fc-4614-973c-2bfb097deed1",
              "caption": "",
              "alt": "",
              "width": 3000,
              "height": 500,
              "instructor_notes": null
            },
            {
              "id": 613320,
              "key": "e276e8ee-acb5-43e5-b1f6-bedf87062c0e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Unity ML-Agents",
              "instructor_notes": ""
            },
            {
              "id": 613316,
              "key": "855e6150-00fd-4b48-8538-90bb6a8e110b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "**Unity Machine Learning Agents (ML-Agents)** is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents. \n\nFor game developers, these trained agents can be used for multiple purposes, including controlling [NPC](https://en.wikipedia.org/wiki/Non-player_character) behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. \n\nIn this course, you will use Unity's rich environments to design, train, and evaluate your own deep reinforcement learning algorithms.  You can read more about ML-Agents by perusing the [GitHub repository](https://github.com/Unity-Technologies/ml-agents).\n\n> **Note: The Unity ML-Agent team frequently releases updated versions of their environment. We are using the v0.4 interface.  To avoid any confusion, please use the workspace we provide here or work with v0.4 locally.**",
              "instructor_notes": ""
            },
            {
              "id": 613319,
              "key": "09855c95-a395-4ebc-ab83-a2afa11a6ad5",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ad8b114_2018-02-27-16-05-37/2018-02-27-16-05-37.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/09855c95-a395-4ebc-ab83-a2afa11a6ad5",
              "caption": "Winner of the Unity ML-Agents Challenge: A robotic arm that can make pancakes!",
              "alt": "Winner of the Unity ML-Agents Challenge: A robotic arm that can make pancakes!",
              "width": 480,
              "height": 286,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 616987,
          "key": "7098bc65-4e08-4d87-a5c6-57d78b5b3288",
          "title": "The Environment - Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7098bc65-4e08-4d87-a5c6-57d78b5b3288",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 616997,
              "key": "778b2793-dec5-49aa-938e-b427a987f421",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# The Environment",
              "instructor_notes": ""
            },
            {
              "id": 617006,
              "key": "63780c4d-81c4-44b6-83c0-140b5646022f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "For this project, you will train an agent to navigate (and collect bananas!) in a large, square world.  ",
              "instructor_notes": ""
            },
            {
              "id": 644419,
              "key": "e08cb95d-b0a4-471e-ab18-a9fe1eb1d9f2",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b1ab4b0_banana/banana.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e08cb95d-b0a4-471e-ab18-a9fe1eb1d9f2",
              "caption": "",
              "alt": "",
              "width": 400,
              "height": 269,
              "instructor_notes": null
            },
            {
              "id": 617007,
              "key": "02e0ea38-3c11-48ba-bdc0-bad0dc791f68",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana.  Thus, the goal of your agent is to collect as many yellow bananas as possible while avoiding blue bananas.  \n\nThe state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction.  Given this information, the agent has to learn how to best select actions.  Four discrete actions are available, corresponding to:\n- **`0`** - move forward.\n- **`1`** - move backward.\n- **`2`** - turn left.\n- **`3`** - turn right.\n\nThe task is episodic, and in order to solve the environment, your agent must get an average score of +13 over 100 consecutive episodes.\n\n## Note\n---\nThe project environment is similar to, but **not identical to** the Banana Collector environment on the [Unity ML-Agents GitHub page](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector).  \n> **You are *required* to work with the environment that we will provide as part of the project.**  \n\nIn particular, your project submission should **not** use the environment on the ML-Agents GitHub page.",
              "instructor_notes": ""
            },
            {
              "id": 617017,
              "key": "6bbf2c80-2d7a-4b41-b957-997315b789ec",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Remark\n---\n\nPlease exercise extreme caution in robotic applications!",
              "instructor_notes": ""
            },
            {
              "id": 617016,
              "key": "0e58c6bc-90bc-48df-befd-b6d758636c77",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ae1ee84_bananas-save/bananas-save.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0e58c6bc-90bc-48df-befd-b6d758636c77",
              "caption": "Found on Twitter",
              "alt": "Found on Twitter",
              "width": 388,
              "height": 300,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 651178,
          "key": "5c32765b-2f16-4a64-b76f-c56ea154748b",
          "title": "The Environment - Play",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5c32765b-2f16-4a64-b76f-c56ea154748b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 651179,
              "key": "fc212723-9299-4d9f-adcd-5d7ae4fc96f3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# The Environment",
              "instructor_notes": ""
            },
            {
              "id": 651181,
              "key": "7029d315-ff34-4247-a30d-861fd02b1260",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Follow the instructions in the Jupyter notebook below to play the game, as a human agent!\n\nThe **available controls** are:\n- `W` - move forward. (_Note: when playing the game, the agent will move forward, if you don't select a different action in time, so you can also think of this action as the \"do nothing\" action._)\n- `S` - move backward.\n- `A` - turn left.\n- `D` - turn right.\n\nSpend a couple of minutes to move around and collect some yellow bananas.  Once you feel like you understand the agent's task, feel free to move on to the next part of the lesson!",
              "instructor_notes": ""
            },
            {
              "id": 651180,
              "key": "abf1ea82-3368-428f-ab8b-6d460aa39bc1",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view5b2a5ff4",
              "pool_id": "jupyter",
              "view_id": "5b2a5ff4-30fa-4d8d-9c9f-cc6a1d5ab0a2",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/play.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 617009,
          "key": "319dc918-bd2c-4d3b-80a5-063bb5f1905a",
          "title": "The Environment - Explore",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "319dc918-bd2c-4d3b-80a5-063bb5f1905a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617010,
              "key": "99dce149-6dac-41dd-9db2-3d727dd728ca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# The Environment",
              "instructor_notes": ""
            },
            {
              "id": 617012,
              "key": "123a0c46-8529-49e1-80dc-6d761d0a1ccd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Follow the instructions below to explore the environment on your own machine!  You will also learn how to use the Python API to control your agent.\n\n## Step 1: Clone the DRLND Repository\n---\nIf you haven't already, please follow the [instructions in the DRLND GitHub repository](https://github.com/udacity/deep-reinforcement-learning#dependencies) to set up your Python environment.  These instructions can be found in `README.md` at the root of the repository.  By following these instructions, you will install PyTorch, the ML-Agents toolkit, and a few more Python packages required to complete the project.\n\n(_For Windows users_) The ML-Agents toolkit supports Windows 10. While it might be possible to run the ML-Agents toolkit using other versions of Windows, it has not been tested on other versions. Furthermore, the ML-Agents toolkit has not been tested on a Windows VM such as Bootcamp or Parallels.  \n\n## Step 2: Download the Unity Environment\n---\nFor this project, you will **not** need to install Unity - this is because we have already built the environment for you, and you can download it from one of the links below.  You need only select the environment that matches your operating system:\n- Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n- Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n- Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n- Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n\nThen, place the file in the `p1_navigation/` folder in the DRLND GitHub repository, and unzip (or decompress) the file.\n\n(_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.\n\n(_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)), then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip) to obtain the \"headless\" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen, but you will be able to train the agent.  (_To watch the agent, you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the **Linux** operating system above._)\n\n## Step 3: Explore the Environment\n---\nAfter you have followed the instructions above, open `Navigation.ipynb` (located in the `p1_navigation/` folder in the DRLND GitHub repository) and follow the instructions to learn how to use the Python API to control the agent.\n\nWatch the (_silent_) video below to see what kind of output to expect from the notebook, if everything is working properly!",
              "instructor_notes": ""
            },
            {
              "id": 654003,
              "key": "8a80a9cf-593f-47b5-b0ea-2226cf4f177a",
              "title": "Getting Started",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "ltz2GhFv04A",
                "china_cdn_id": "ltz2GhFv04A.mp4"
              }
            },
            {
              "id": 617022,
              "key": "fcf6b7cd-c3b8-44e8-89c6-3d2530a53bd2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the last code cell of the notebook, you'll learn how to design and observe an agent that always selects random actions at each timestep.  Your goal in this project is to create an agent that performs much better!\n\n## (Optional) Build your Own Environment\n---\nFor this project, we have built the Unity environment for you, and you must use the environment files that we have provided.  \n\nIf you are interested in learning to build your own Unity environments **after completing the project**, you are encouraged to follow the instructions [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md), which walk you through all of the details of building an environment from a Unity scene.  ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 673194,
          "key": "eb5d8df0-eef2-4a41-bf7f-8f688534da35",
          "title": "Project Instructions",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "eb5d8df0-eef2-4a41-bf7f-8f688534da35",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 673196,
              "key": "51ffba2e-da9a-4b95-bae1-916ad9fda017",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Project Instructions",
              "instructor_notes": ""
            },
            {
              "id": 673195,
              "key": "b176f4ef-1899-4758-90a1-aa81e180ae54",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "For this project, you will train an agent to solve the provided environment. \n\nTo submit the project, you will provide a link to a GitHub repository with your implementation.  If you would like a refresher on GitHub, please check out the lessons on GitHub in the extracurricular content.\n\nTo review the detailed project requirements, please read the [project rubric](https://review.udacity.com/#!/rubrics/1889/view).  \n\nThe format of this project is largely open-ended; you need only satisfy the points in the rubric.  For instance, while we suspect that the majority of students will train the agent in a Jupyter notebook, you are welcome to instead structure your repository so that your Python code is run from the command line instead.  \n\n## Your GitHub Submission\n---\nAs described in the rubric, your GitHub submission should contain:\n- a **README** that describes how someone not familiar with this project should use your repository.  The README should be designed for a general audience that may not be familiar with the Nanodegree program; you should describe the environment that you solved, along with how to install the requirements before running the code in your repository.\n- the **code** that you use for training the agent, along with the trained model weights.\n- a **report** describing your learning algorithm.  This is where you will describe the details of your implementation, along with ideas for future work.\n  \nThis GitHub repository will serve as a portfolio piece to share your new skills with the global community of reinforcement learning students and practitioners, along with potential employers!\n\n## Project Workspace\n---\nWhile you are welcome to train the agent locally on your own machine, you can also complete the project in the **Workspace** that appears towards the end of this lesson.  Note that the Workspace does not allow you to see the simulator of the environment; so, if you want to watch the agent while it is training, you should train locally.\n\nThe Workspace provides a Jupyter server directly in your browser and has GPU support.  You can learn more about the Workspace by perusing the **Udacity Workspaces** lesson in the extracurricular content.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617014,
          "key": "0df81599-d934-4dfb-860c-22f723129795",
          "title": "Benchmark Implementation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "0df81599-d934-4dfb-860c-22f723129795",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617018,
              "key": "cea02be9-e113-4171-b137-d172caf741e2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Benchmark Implementation",
              "instructor_notes": ""
            },
            {
              "id": 617015,
              "key": "cec94e41-f57c-45bb-83b4-ce1d6489d821",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "For this project, you can use any algorithm of your choosing to solve the task. You are strongly encouraged to do your own research, to devise your own approach towards solving this problem. \n\nIn case you get stuck, note that you should be able to solve the project by making only minor modifications to the DQN code provided as part of the **Deep Q-Networks** lesson.  Please see the image below for an example of how you might expect your agent's score to evolve.  If you're interested in about how long it should take, in the solution code for the project, we were able to solve the project in fewer than 1800 episodes.  ",
              "instructor_notes": ""
            },
            {
              "id": 644420,
              "key": "32a135a3-d6bb-4b77-b0ff-eee372be5309",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b1ab750_screen-shot-2018-06-08-at-1.04.47-pm/screen-shot-2018-06-08-at-1.04.47-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/32a135a3-d6bb-4b77-b0ff-eee372be5309",
              "caption": "(Example) Plotted Rewards for Project 1",
              "alt": "(Example) Plotted Rewards for Project 1",
              "width": 350,
              "height": 1256,
              "instructor_notes": null
            },
            {
              "id": 626267,
              "key": "734872d1-b08e-430a-af96-659adbccc87d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "_Your agent might take longer (or solve the task much faster!) -- this is perfectly fine, and we provide this estimate only as an estimate for how long you should wait before assessing if your agent is learning._",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617033,
          "key": "b2d9818b-b010-439e-bb23-47c49f8ddc2d",
          "title": "Not sure where to start?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b2d9818b-b010-439e-bb23-47c49f8ddc2d",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 620850,
              "key": "aa04d527-c3d2-4092-b29e-ca2e94703435",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aeb3f3f_idea-2579308-640/idea-2579308-640.jpg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/aa04d527-c3d2-4092-b29e-ca2e94703435",
              "caption": "",
              "alt": "",
              "width": 200,
              "height": 200,
              "instructor_notes": null
            },
            {
              "id": 617035,
              "key": "1fc1b407-1a7a-412e-b5d1-d330d33b5314",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Not sure where to start?",
              "instructor_notes": ""
            },
            {
              "id": 617043,
              "key": "9cdf4e17-37c9-4877-a217-637c16b703de",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If you're not sure where to start, here are some suggestions for how to make some progress with the project.  You need not follow this advice; these are only suggestions, and you should follow whatever path works best for you!\n\n## Step 1: Master the details of Deep Q-Networks (DQN).\n---\nRead the [DQN paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) to master all of the details.  Refer to the lesson on **Deep Q-Networks** to cement your understanding. If you have any questions, post them in Slack or Knowledge!\n\n## Step 2: Study the coding exercise from the lesson.\n---\n\nIn the **Deep Q-Networks** lesson, you applied a DQN implementation to an OpenAI Gym task. Take the time to understand this code in great detail. Tweak the various hyperparameters and settings to build your intuition for what should work well (_and what doesn't!_).\n\n## Step 3: Adapt the code from the lesson to the project.\n---\nAdapt the code from the exercise to the project, while making as few modifications as possible. (_Remember that the code that you use to interact with the Unity environment is different from the OpenAI gym interface._) Don't worry about efficiency, and just make sure the code runs. Don't worry about modifying hyperparameters, optimizers, or anything else of that nature just yet. \n\nFor this step, you do not need to run your code on a GPU. In particular, if working in the Udacity Workspace, GPU should not be enabled. Save your GPU hours (_if needed_) for the next step!\n\n## Step 4: Optimize the hyperparameters.\n---\nAfter you have verified that your DQN code runs, try a few long training sessions while running your code on CPU. If your agent fails to learn, try out a few potential solutions by modifying your code.  Once you're feeling confident (_or impatient :)_) try out your implementation with GPU support!  (_Note that you may not need GPU at all. For instance, we found that our implementation trained quickly enough on CPU._)",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617023,
          "key": "f8581138-d0a6-44ef-92ea-dfe7e9ecb5a3",
          "title": "Collaborate!",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f8581138-d0a6-44ef-92ea-dfe7e9ecb5a3",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626248,
              "key": "f828b1d2-b9be-47fb-910b-9b33aefe58ce",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5af1d1ae_849-1234251683jkyt/849-1234251683jkyt.jpg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f828b1d2-b9be-47fb-910b-9b33aefe58ce",
              "caption": "",
              "alt": "",
              "width": 400,
              "height": 700,
              "instructor_notes": null
            },
            {
              "id": 617024,
              "key": "0752ae45-999d-45e6-ac4a-eb96911b1fbb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Collaborate!",
              "instructor_notes": ""
            },
            {
              "id": 617025,
              "key": "bc3e7151-590c-4aaf-bd49-d7064e06f908",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If at any point you get stuck, remember you are part of a strong learning community!  Please do not hesitate to reach out to your instructors and fellow students by posting in Study Groups or Knowledge.  You may also find it useful to post after you've solved the environment, to get more ideas for further improving your agent! :)",
              "instructor_notes": ""
            },
            {
              "id": 662932,
              "key": "ab235c84-8595-404b-b115-1ced5629b65d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Share your project success!\n---\nPassed your project? Share the good news!\n\nWhat youâ€™ve accomplished is no small feat. Give yourself a pat on the back and some well-deserved recognition by sharing your success with your network.\n\n<iframe\n  src=\"https://platform.twitter.com/widgets/tweet_button.html?size=l&url=www.udacity.com&text=I%20trained%20my%20own%20intelligent%20agent%20in%20the%20Deep%20Reinforcement%20Learning%20Nanodegree!%20@udacity\n[Insert%20your%20Github%20repository%20url%20here]&hashtags=udacityDeepRL\"\nwidth=\"140\"\nheight=\"28\"\nscrolling=\"no\">\n</iframe>\n\nMake sure to use **@udacity** and **#udacityDeepRL** in your posts!\n\n<html>\n\n<head>\n<style>\n.fb {color: white;\n  background-color: #4661b0;\n  border-radius: 4px;\n  font-weight: bold;\n  height: 28px;\n  font-size: 14px;}\n.fb:hover {color: #4661b0;\n  background-color: white;\n  border-color: #4661b0;\n  transition: background-color 0.4s}\n.linkedin {color: white;\n  background-color: #0077B5;\n  border-radius: 4px;\n  font-weight: bold;\n  height: 28px;\n  font-size: 14px;}\n.linkedin:hover {color: #0077B5;\n  background-color: white;\n  border-color: #0077B5;\n  transition: background-color 0.4s}\n</style>\n\n</head>\n\n<body>\n<form action=\"https://www.facebook.com/sharer.php?\">\n  Enter the full URL of your GitHub repository or a YouTube video of your trained agent:<br>\n  <input type=\"url\" name=\"u\" placeholder=\"Paste URL here\">\n  <input type=\"hidden\" name=\"hashtag\" value=\"#udacityDeepRL\">\n  <button class=\"fb\">Share on Facebook</button>\n</form>\n\n<form action=\"https://www.linkedin.com/shareArticle?mini=true\">\n  <input type=\"url\" name=\"url\" placeholder=\"Paste URL here\">\n  <input type=\"hidden\" name=\"title\" value=\"Train your own AI agent!\">\n  <input type=\"hidden\" name=\"summary\" value=\"I trained my own intelligent agent in the Deep Reinforcement Learning Nanodegree @udacity #udacityDeepRL\">\n  <button class=\"linkedin\">Share on LinkedIn</button>\n</form>\n\n</body>\n\n</html>",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 671745,
          "key": "b6729be0-4a97-4c8d-b2c9-9eafe47939ac",
          "title": "Workspace",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b6729be0-4a97-4c8d-b2c9-9eafe47939ac",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 671987,
              "key": "fb371d8f-4e3c-4dd5-ad9e-b23828397635",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewB1e3Efhpzm",
              "pool_id": "jupytergpu",
              "view_id": "jupyter-H12NznpfQ",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": {
                      "id": "mldatasets",
                      "paths": [
                        {
                          "src": "/drlnd_projects_v4/Banana_Linux_NoVis",
                          "dest": "/data/Banana_Linux_NoVis"
                        }
                      ]
                    },
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Navigation.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 626252,
          "key": "80164380-a9ad-460d-bee7-59fe2d776036",
          "title": "(Optional) Challenge: Learning from Pixels",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "80164380-a9ad-460d-bee7-59fe2d776036",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 626253,
              "key": "8fecef20-f495-4f44-b742-f2b43f3d6da9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# (Optional) Challenge: Learning from Pixels",
              "instructor_notes": ""
            },
            {
              "id": 626254,
              "key": "9467bbfd-bc49-441e-8304-638317128c3b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "After you have successfully completed the project, if you're looking for an additional challenge, you have come to the right place!\n\nIn the project, your agent learned from information such as its velocity, along with ray-based perception of objects around its forward direction.  **A more challenging task would be to learn directly from pixels!**\n\nThis environment is _almost_ identical to the project environment, where the only difference is that the state is an 84 x 84 RGB image, corresponding to the agent's first-person view of the environment.\n\n## Download the Unity Environment\n---\nTo solve this harder task, you'll need to download a new Unity environment.  \n\nYou need only select the environment that matches your operating system:\n- Linux: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana_Linux.zip)\n- Mac OSX: [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana.app.zip)\n- Windows (32-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana_Windows_x86.zip)\n- Windows (64-bit): [click here](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/VisualBanana_Windows_x86_64.zip)\n\nThen, place the file in the `p1_navigation/` folder in the DRLND GitHub repository, and unzip (or decompress) the file.\n\n(_For AWS_) If you'd like to train the agent on AWS, you must follow the instructions to [set up X Server](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the **Linux** operating system above.\n\n> Please do not submit a project with this new environment.  You are **required** to complete the project with the version of the environment that was provided earlier in this lesson, in **The Environment - Explore**.\n\n## Explore the Environment\n---\nAfter you have followed the instructions above, open `Navigation_Pixels.ipynb` (located in the `p1_navigation/` folder in the DRLND GitHub repository) and follow the instructions to learn how to use the Python API to control the agent.\n\n## Important Note\n---\nTo solve this environment, you will need to design a convolutional neural network as the DQN architecture.  For inspiration about how to set up this architecture, please refer to the [DQN paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).\n\nThis task will take much longer to train than the project, and (_unless you are very patient :D_) you're encouraged to use a GPU to train.  If you don't have a local GPU setup, you can learn how to train the project on AWS by following the instructions [here](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md).  Note that it is not possible to train this agent in the provided Udacity Workspace.",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}