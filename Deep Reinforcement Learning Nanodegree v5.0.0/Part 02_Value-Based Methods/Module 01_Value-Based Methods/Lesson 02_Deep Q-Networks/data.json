{
  "data": {
    "lesson": {
      "id": 616710,
      "key": "a6829f14-5ef0-4b4a-83ed-234029c5cc60",
      "title": "Deep Q-Networks",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Extend value-based reinforcement learning methods to complex problems using deep neural networks.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/a6829f14-5ef0-4b4a-83ed-234029c5cc60/616710/1544455643355/Deep+Q-Networks+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/a6829f14-5ef0-4b4a-83ed-234029c5cc60/616710/1544455638150/Deep+Q-Networks+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 645574,
          "key": "804805cb-e83f-4beb-b07e-17888e718b62",
          "title": "From RL to Deep RL",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "804805cb-e83f-4beb-b07e-17888e718b62",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 645575,
              "key": "2d46d119-c4ac-4f23-b59d-46c07ff12063",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# From RL to Deep RL",
              "instructor_notes": ""
            },
            {
              "id": 645584,
              "key": "cdf0161f-e36e-473f-a2de-7eca3962e308",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So far, you've solved many of your own reinforcement learning problems, using solution methods that represent the action values in a small table.  Earlier in the nanodegree, we referred to this table as a **Q-table**. \n\nIn the video below, **Kelvin Lwin** will introduce you to the idea of using neural networks to expand the size of the problems that we can solve with reinforcement learning.  This context is useful preparation for exploring the details behind the Deep Q-Learning algorithm later in this lesson!\n\n_Kelvin is a Senior Deep Learning Instructor at the [NVIDIA Deep Learning Institute](https://www.nvidia.com/en-us/deep-learning-ai/education)._",
              "instructor_notes": ""
            },
            {
              "id": 645576,
              "key": "218d814b-cfc2-435a-9ac6-3962a4a1d438",
              "title": "From RL to Deep RL",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "7HLJ0uaR1F0",
                "china_cdn_id": "7HLJ0uaR1F0.mp4"
              }
            },
            {
              "id": 652529,
              "key": "850a5737-8635-4ac6-853b-fc75c62cfdaf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Stabilizing Deep Reinforcement Learning\n---\nAs you'll learn in this lesson, the Deep Q-Learning algorithm represents the optimal action-value function <span class=\"mathquill\">q_*</span> as a neural network (instead of a table).\n\nUnfortunately, reinforcement learning is [notoriously unstable](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.3097&rep=rep1&type=pdf) when neural networks are used to represent the action values.  In this lesson, you'll learn all about the Deep Q-Learning algorithm, which addressed these instabilities by using **two key features**:\n- Experience Replay\n- Fixed Q-Targets\n\nWatch the video below to learn more! ",
              "instructor_notes": ""
            },
            {
              "id": 652531,
              "key": "29bf7471-cd6e-4241-8a16-29cf861988f6",
              "title": "DQN Overview",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "WgiAvr7COR0",
                "china_cdn_id": "WgiAvr7COR0.mp4"
              }
            },
            {
              "id": 652530,
              "key": "f14cfbbf-86ce-46aa-abce-e4374a7bf541",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Additional References\n---\n\n* Riedmiller, Martin. \"Neural fitted Q iterationâ€“first experiences with a data efficient neural reinforcement learning method.\" European Conference on Machine Learning. Springer, Berlin, Heidelberg, 2005.\nhttp://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf\n\n* Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature518.7540 (2015): 529.\nhttp://www.davidqiu.com:8888/research/nature14236.pdf ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 624436,
          "key": "71a2042a-da05-4adc-bcca-4625bccfae05",
          "title": "Deep Q-Networks",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "71a2042a-da05-4adc-bcca-4625bccfae05",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 631332,
              "key": "bfc815e2-d50b-43cc-8855-779abc9b18d6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Deep Q-Networks",
              "instructor_notes": ""
            },
            {
              "id": 631333,
              "key": "2a90b949-735d-4362-baea-8fb1e8a7e785",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the video below, Arpan will tell you all about how DeepMind leveraged a **Deep Q-Network (DQN)** to build the Deep Q-Learning algorithm that learned to play many Atari video games better than humans.",
              "instructor_notes": ""
            },
            {
              "id": 624461,
              "key": "7853922c-4276-4446-9814-423905b2ad6e",
              "title": "Deep Q-Networks",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "GgtR_d1OB-M",
                "china_cdn_id": "GgtR_d1OB-M.mp4"
              }
            },
            {
              "id": 631896,
              "key": "d584e89a-42f0-42aa-9ad5-b49829df7936",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 631897,
              "key": "81eb123e-f6d3-494e-a160-1283e4876b8c",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "81eb123e-f6d3-494e-a160-1283e4876b8c",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Select all that are true.",
                "answers": [
                  {
                    "id": "a1527605040157",
                    "text": "The Deep Q-Network (DQN) receives the most recent state of the game as input.  After some preprocessing, this is the 84x84x1 (grayscale) game screen.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527605133053",
                    "text": "In order to capture temporal information, a stack of 4 preprocessed frames are used as the state that is passed to the DQN.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527605188331",
                    "text": "The DQN takes a state and action as input, and returns the corresponding predicted action value.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527605188746",
                    "text": "The DQN takes the state as input, and returns the corresponding predicted action values for each possible game action.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527605304420",
                    "text": "The DQN was provided information about the game of pong - before the agent started learning, the researchers wrote helper functions to ensure the agent always keeps the paddle close to the ball.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527605447890",
                    "text": "The DQN architecture included a combination of convolutional and recurrent layers.  The CNN component captures spatial information in the state, and the RNN component detects temporal patterns.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527605465804",
                    "text": "The DQN architecture is fully convolutional: it has only convolutional layers and no fully-connected layers.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527605596955",
                    "text": "The DQN architecture is composed of a couple of convolutional layers, followed by a couple of fully connected layers.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527605641421",
                    "text": "For each Atari game, the DQN was trained from scratch on that game.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527605658386",
                    "text": "The DQN was trained on data from the first several games, and then with that knowledge (and no additional training), it could beat the remaining Atari games.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 624437,
          "key": "fa0b054f-cbf6-45c3-9fcf-7e81c0441a9e",
          "title": "Experience Replay",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "fa0b054f-cbf6-45c3-9fcf-7e81c0441a9e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 631316,
              "key": "8a044b31-dcd0-4a61-af64-a386d8a1d6b4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Experience Replay",
              "instructor_notes": ""
            },
            {
              "id": 624462,
              "key": "a5f9da63-a6ee-40b2-a414-c061d665a167",
              "title": "Experience Replay",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "wX_-SZG-YMQ",
                "china_cdn_id": "wX_-SZG-YMQ.mp4"
              }
            },
            {
              "id": 631322,
              "key": "fcac6d0c-abec-4e56-bd9a-bbb8ac795fc0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Summary\n---\n\nWhen the agent interacts with the environment, the sequence of experience tuples can be highly correlated.  The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation.  By instead keeping track of a **replay buffer** and using **experience replay** to sample from the buffer at random, we can prevent action values from oscillating or diverging catastrophically.\n\nThe **replay buffer** contains a collection of experience tuples (<span class=\"mathquill\">S</span>, <span class=\"mathquill\">A</span>, <span class=\"mathquill\">R</span>, <span class=\"mathquill\">S'</span>).  The tuples are gradually added to the buffer as we are interacting with the environment.\n\nThe act of sampling a small batch of tuples from the replay buffer in order to learn is known as **experience replay**.  In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.\n\n## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 631335,
              "key": "1c07bfdf-6543-4041-a7a2-fb86105f7ccd",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "1c07bfdf-6543-4041-a7a2-fb86105f7ccd",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following are true?  Select all that apply.",
                "answers": [
                  {
                    "id": "a1527296926624",
                    "text": "Experience replay is based on the idea that we can learn better, if we do multiple passes over the same experience.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527297063469",
                    "text": "Experience replay causes harmful correlations and can cause lead to action-value estimates that fail to converge while training.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527297193365",
                    "text": "Experience replay is used to generate uncorrelated experience data for online training of deep RL agents.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527297263482",
                    "text": "Once an experience tuple is randomly sampled from the replay buffer, the agent learns from it, and then it is discarded from the buffer.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 624438,
          "key": "22d7ab96-d6d2-4605-8cb6-2a8b5bd087f2",
          "title": "Fixed Q-Targets",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "22d7ab96-d6d2-4605-8cb6-2a8b5bd087f2",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 631327,
              "key": "fe387e42-4b63-4855-b326-45edb40fc18e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Fixed Q-Targets",
              "instructor_notes": ""
            },
            {
              "id": 624463,
              "key": "6f1bcb55-8c36-434b-b3a7-b894bd15ad77",
              "title": "Fixed Q-Targets",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "SWpyiEezfp4",
                "china_cdn_id": "SWpyiEezfp4.mp4"
              }
            },
            {
              "id": 624464,
              "key": "4044f7ba-1b07-4d7a-a7d5-ef31eb076854",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Summary\n---\nIn Q-Learning, we **_update a guess with a guess_**, and this can potentially lead to harmful correlations.  To avoid this, we can update the parameters <span class=\"mathquill\">w</span> in the network <span class=\"mathquill\">\\hat{q}</span> to better approximate the action value corresponding to state <span class=\"mathquill\">S</span> and action <span class=\"mathquill\">A</span> with the following update rule:\n\n<div class=\"mathquill\">\\Delta w = \\alpha \\cdot \\overbrace{( \\underbrace{R + \\gamma \\max_a\\hat{q}(S', a, w^-)}_{\\rm {TD~target}} - \\underbrace{\\hat{q}(S, A, w)}_{\\rm {old~value}})}^{\\rm {TD~error}} \\nabla_w\\hat{q}(S, A, w)</div>\nwhere <span class=\"mathquill\">w^-</span> are the weights of a separate target network that are not changed during the learning step, and (<span class=\"mathquill\">S</span>, <span class=\"mathquill\">A</span>, <span class=\"mathquill\">R</span>, <span class=\"mathquill\">S'</span>) is an experience tuple.\n\n**Note**: Ever wondered how the example in the video would look in real life? See: [Carrot Stick Riding](https://www.youtube.com/watch?v=-PVFBGN_zoM).\n\n## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 631336,
              "key": "a5306b6b-0444-46a1-9e7e-2ee60b25e444",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "a5306b6b-0444-46a1-9e7e-2ee60b25e444",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following are true?  Select all that apply.",
                "answers": [
                  {
                    "id": "a1527297466242",
                    "text": "The Deep Q-Learning algorithm uses two separate networks with identical architectures.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527297535473",
                    "text": "The Deep Q-Learning algorithm uses two separate networks with different architectures.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527297640119",
                    "text": "Every time we update the primary Q-Network, we immediately update the target Q-Network weights, so that they match after each learning step.",
                    "is_correct": false
                  },
                  {
                    "id": "a1527297732285",
                    "text": "The target Q-Network's weights are updated less often (or more slowly) than the primary Q-Network.",
                    "is_correct": true
                  },
                  {
                    "id": "a1527297951276",
                    "text": "Without fixed Q-targets, we would encounter a harmful form of correlation, whereby we shift the parameters of the network based on a constantly moving target.",
                    "is_correct": true
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 624439,
          "key": "637ff801-c1e1-4eb8-90cb-c9bcda92ca77",
          "title": "Deep Q-Learning Algorithm",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "637ff801-c1e1-4eb8-90cb-c9bcda92ca77",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624477,
              "key": "b81f8020-b8b8-4924-b4eb-76c373049a89",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aef2add_dqn/dqn.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/b81f8020-b8b8-4924-b4eb-76c373049a89",
              "caption": "Illustration of DQN Architecture ([Source](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf))",
              "alt": "Illustration of DQN Architecture",
              "width": 500,
              "height": 804,
              "instructor_notes": null
            },
            {
              "id": 624476,
              "key": "258433fc-e6fe-4559-b8d6-8ce4ccd1d1cf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Deep Q-Learning Algorithm",
              "instructor_notes": ""
            },
            {
              "id": 624475,
              "key": "cfa1067f-5cf5-4f99-b697-e2028ef1b36b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Please take the time now to read the [research paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) that introduces the Deep Q-Learning algorithm.\n\n## Reading Scientific Papers\n---\nAs part of this nanodegree, you will learn about many of the most recent, cutting-edge algorithms!  Because of this, it will prove useful to learn how to read the original research papers.  Here are some [excellent tips](https://www.huffingtonpost.com/jennifer-raff/how-to-read-and-understand-a-scientific-paper_b_5501628.html).  Some of the best advice is:\n- Take notes.\n\n- Read the paper multiple times.  On the first couple readings, try to focus on the main points: \n 1.  What kind of tasks are the authors using deep reinforcement learning (RL) to solve?  What are the states, actions, and rewards?\n 2. What neural network architecture is used to approximate the action-value function?\n 3. How are experience replay and fixed Q-targets used to stabilize the learning algorithm?\n 4. What are the results? \n- Understanding the paper will probably take you longer than you think.  Be patient, and reach out to the Udacity community with any questions.\n\n## Check Your Understanding\n---\nAfter you have read the paper, use the video below to check your understanding.",
              "instructor_notes": ""
            },
            {
              "id": 624478,
              "key": "fb4cfd83-bb36-44ff-94a1-248109eaa482",
              "title": "Deep Q-Learning Algorithm",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "MqTXoCxQ_eY",
                "china_cdn_id": "MqTXoCxQ_eY.mp4"
              }
            }
          ]
        },
        {
          "id": 624441,
          "key": "02a1cc37-a38b-4159-8a4e-4abc4db8cb87",
          "title": "Coding Exercise",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "02a1cc37-a38b-4159-8a4e-4abc4db8cb87",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624459,
              "key": "4db7b4b6-93d4-475a-8b05-560a1b8d9516",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/September/59c73b1b_jupyter/jupyter.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/4db7b4b6-93d4-475a-8b05-560a1b8d9516",
              "caption": "",
              "alt": "Jupyter logo",
              "width": 100,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 689129,
              "key": "41ebdc45-5d96-4594-8e0d-2f0a0b6c3d43",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Coding Exercise",
              "instructor_notes": ""
            },
            {
              "id": 624460,
              "key": "408ce2da-fc98-4007-9056-c5450366b4d9",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this exercise, you will implement Deep Q-Learning to solve OpenAI Gym's LunarLander environment.  To begin, open the Workspace in the next concept, navigate to the `exercise/` folder, and follow the instructions in `Deep_Q_Network.ipynb`.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 663228,
              "key": "1c1acc73-806d-431a-a36d-b17cfc6d6eac",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "(*Alternatively, if you'd prefer to explore a complete implementation, enter the `solution/` folder, and run the code in `Deep_Q_Network_Solution.ipynb`.*)\n\nAfter you are able to get the code working, try to change the hyperparameters and architecture, to see if you can get the agent to train faster! \n\n## PyTorch\n---\nThe implementation is written in PyTorch.  If the PyTorch framework is new to you, please take a look at our **Deep Learning with PyTorch** lesson in the extracurricular content. \n\n## <span style=\"color:red;\">Note</span>\n---\nIn the Workspace in the following concept, you will have the option to **ENABLE GPU** to accelerate training.  After training, you can use the provided code in the Jupyter notebook to watch your agent's performance.  Note that if visualizing the trained agent in the Workspace, GPU should be **disabled** -- otherwise, the notebook will return an error.  \n\n><span style=\"color:red;\">Thus, you are encouraged to follow the following workflow:  </span>\n1. Train the agent with GPU **Enabled**, and save the trained model weights, \n2. **Disable GPU**, load the trained weights from the file, and watch the trained agent.\n\n## DQN Improvements\n---\nLater in this lesson, you will learn about three different improvements you can make to your algorithm:\n- Double DQN\n- Prioritized Experience Replay\n- Dueling DQN\n\nIf you'd like to implement any of these, you're encouraged to use the provided notebook as a starting point.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 624442,
          "key": "b3d7d025-6699-4f77-9b8b-8cb45042d14a",
          "title": "Workspace",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b3d7d025-6699-4f77-9b8b-8cb45042d14a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 671982,
              "key": "167c6d5a-513d-4509-b950-a3efde18e5f8",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewS1gtET5aGm",
              "pool_id": "jupytergpu",
              "view_id": "jupyter-rktEaqaf7",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 624443,
          "key": "9cf78c93-e31a-40a1-bea3-c0f3509fd2f3",
          "title": "Deep Q-Learning Improvements",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9cf78c93-e31a-40a1-bea3-c0f3509fd2f3",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624468,
              "key": "9254d868-c4a2-4c28-a29d-c647f0a17de2",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aef27bf_dueling-q-network/dueling-q-network.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/9254d868-c4a2-4c28-a29d-c647f0a17de2",
              "caption": "Dueling Q-Network ([Source](https://arxiv.org/pdf/1511.06581.pdf))",
              "alt": "Dueling Q-Network",
              "width": 500,
              "height": 510,
              "instructor_notes": null
            },
            {
              "id": 624458,
              "key": "016db861-0790-4e41-a081-d70eaafaff56",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Deep Q-Learning Improvements",
              "instructor_notes": ""
            },
            {
              "id": 624466,
              "key": "4460bbaf-0ffa-4d8b-9de9-5690434e2683",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Several improvements to the original Deep Q-Learning algorithm have been suggested.  Over the next several videos, we'll look at three of the more prominent ones.",
              "instructor_notes": ""
            },
            {
              "id": 624467,
              "key": "bca50571-37d3-42b6-b475-35a44134573e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Double DQN\n---\nDeep Q-Learning [tends to overestimate](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf) action values.  [Double Q-Learning](https://arxiv.org/abs/1509.06461) has been shown to work well in practice to help with this. \n\n## Prioritized Experience Replay\n---\nDeep Q-Learning samples experience transitions _uniformly_ from a replay memory.  [Prioritized experienced replay](https://arxiv.org/abs/1511.05952) is based on the idea that the agent can learn more effectively from some transitions than from others, and the more important transitions should be sampled with higher probability. \n\n## Dueling DQN\n---\nCurrently, in order to determine which states are (or are not) valuable, we have to estimate the corresponding action values _for each action_.  However, by replacing the traditional Deep Q-Network (DQN) architecture with a [dueling architecture](https://arxiv.org/abs/1511.06581), we can assess the value of each state, without having to learn the effect of each action.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 624444,
          "key": "7c3a1bb9-c0c9-4cf6-b4c6-ae962d91bd97",
          "title": "Double DQN",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7c3a1bb9-c0c9-4cf6-b4c6-ae962d91bd97",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624454,
              "key": "ca0ee442-90bb-4e80-9d0f-436cb12cb9ad",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Double DQN",
              "instructor_notes": ""
            },
            {
              "id": 671693,
              "key": "a5b6deea-b776-422e-b731-c0ea0359d7f7",
              "title": "10 Double DQN V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "PGCEMLujiGI",
                "china_cdn_id": "PGCEMLujiGI.mp4"
              }
            },
            {
              "id": 624456,
              "key": "71b31a47-1c17-4e91-a3ea-7d36b867ad4d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Notes\n---\nYou can read more about Double DQN (DDQN) by perusing this [research paper](https://arxiv.org/abs/1509.06461).\n\nIf you'd like to dig deeper into how Deep Q-Learning overestimates action values, please read this [research paper](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 624445,
          "key": "64bb7ff1-ae90-42e5-9caa-7b5e28c066a8",
          "title": "Prioritized Experience Replay",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "64bb7ff1-ae90-42e5-9caa-7b5e28c066a8",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624451,
              "key": "c6fd33e3-67ae-4115-8eb8-53c9be96f07a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Prioritized Experience Replay",
              "instructor_notes": ""
            },
            {
              "id": 671694,
              "key": "3d315309-1b73-423f-b669-e381cd7e21d0",
              "title": "10 Prioritized Experience Replay V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "cN8z-7Ze9L8",
                "china_cdn_id": "cN8z-7Ze9L8.mp4"
              }
            },
            {
              "id": 624453,
              "key": "5d216ac8-beb0-459c-8489-607b5c17e6a7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Notes\n---\nYou can read more about prioritized experience replay by perusing this [research paper](https://arxiv.org/abs/1511.05952).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 624446,
          "key": "fce018b1-d0a4-4b61-8634-fec484e8c35f",
          "title": "Dueling DQN",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "fce018b1-d0a4-4b61-8634-fec484e8c35f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624448,
              "key": "cfecb883-3f34-4e84-8cc9-bab36f9b6b55",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Dueling DQN",
              "instructor_notes": ""
            },
            {
              "id": 671695,
              "key": "d198e6dc-7f92-45e7-8db6-7aa2d834c9ca",
              "title": "10 Dueling DQN V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "zZeHbPs39Ls",
                "china_cdn_id": "zZeHbPs39Ls.mp4"
              }
            },
            {
              "id": 624450,
              "key": "8fbca6a5-957c-40ab-a485-11ff5fbadf11",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Notes\n---\nYou can read more about Dueling DQN by perusing this [research paper](https://arxiv.org/abs/1511.06581).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 668569,
          "key": "40f998d4-7495-4e43-bda2-5f42829a4e4a",
          "title": "Rainbow",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "40f998d4-7495-4e43-bda2-5f42829a4e4a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 668597,
              "key": "cec37415-62cf-42c2-b71e-12f8eef4677f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b38622c_rainbow-1445337690d8q/rainbow-1445337690d8q.jpg",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/cec37415-62cf-42c2-b71e-12f8eef4677f",
              "caption": "",
              "alt": "",
              "width": 300,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 668570,
              "key": "28978cf0-8b98-40e0-adc9-ad004c3d97a4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Rainbow",
              "instructor_notes": ""
            },
            {
              "id": 668572,
              "key": "7e02941b-5833-47d3-85e1-c5abc8a2346a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So far, you've learned about three extensions to the Deep Q-Networks (DQN) algorithm:\n- Double DQN (DDQN)\n- Prioritized experience replay \n- Dueling DQN \n\nBut these aren't the only extensions to the DQN algorithm!  Many more extensions have been proposed, including:\n- Learning from [multi-step bootstrap targets](https://arxiv.org/abs/1602.01783) (as in A3C - *you'll learn about this in the next part of the nanodegree*)\n- [Distributional DQN](https://arxiv.org/abs/1707.06887)\n- [Noisy DQN](https://arxiv.org/abs/1706.10295)\n\nEach of the six extensions address a **_different_** issue with the original DQN algorithm.\n\nResearchers at Google DeepMind recently tested the performance of an agent that incorporated all six of these modifications.  The corresponding algorithm was termed [Rainbow](https://arxiv.org/abs/1710.02298).\n\nIt outperforms each of the individual modifications and achieves state-of-the-art performance on Atari 2600 games!",
              "instructor_notes": ""
            },
            {
              "id": 668573,
              "key": "5e78d636-4404-484f-ad2a-718afd93601e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b3814f1_screen-shot-2018-06-30-at-6.40.09-pm/screen-shot-2018-06-30-at-6.40.09-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/5e78d636-4404-484f-ad2a-718afd93601e",
              "caption": "Performance on Atari games: comparison of Rainbow to six baselines.",
              "alt": "Performance on Atari games: comparison of Rainbow to six baselines.",
              "width": 400,
              "height": 400,
              "instructor_notes": null
            },
            {
              "id": 668575,
              "key": "f1e46010-2352-4e55-858d-cebd78923482",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## In Practice\n---\nIn mid-2018, OpenAI held [a contest](https://contest.openai.com), where participants were tasked to create an algorithm that could learn to play the [Sonic the Hedgehog](https://en.wikipedia.org/wiki/Sonic_the_Hedgehog) game.  The participants were tasked to train their RL algorithms on provided game levels; then, the trained agents were ranked according to their performance on previously unseen levels.\n\nThus, the contest was designed to assess the ability of trained RL agents to generalize to new tasks.  \n",
              "instructor_notes": ""
            },
            {
              "id": 668576,
              "key": "8715e108-d70f-4e97-8585-68e9850f6a0a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b381932_sonic/sonic.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8715e108-d70f-4e97-8585-68e9850f6a0a",
              "caption": "Sonic The Hedgehog ([Source](https://contest.openai.com))",
              "alt": "Sonic The Hedgehog",
              "width": 500,
              "height": 350,
              "instructor_notes": null
            },
            {
              "id": 668577,
              "key": "1bcd94dc-9f44-435e-b9f2-c2220f574299",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "One of the provided baseline algorithms was **Rainbow DQN**.  If you'd like to play with this dataset and run the baseline algorithms, you're encouraged to follow the [setup instructions](https://contest.openai.com/2018-1/details/).  ",
              "instructor_notes": ""
            },
            {
              "id": 668578,
              "key": "f89228d3-abbe-424f-83fd-c403e51891e0",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b381a72_screen-shot-2018-06-30-at-7.03.40-pm/screen-shot-2018-06-30-at-7.03.40-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f89228d3-abbe-424f-83fd-c403e51891e0",
              "caption": "Baseline results on the Retro Contest (test set) ([Source](https://blog.openai.com/retro-contest/))",
              "alt": "Baseline results on the Retro Contest (test set)",
              "width": 500,
              "height": 500,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 624447,
          "key": "3a07c664-b374-4a48-8726-67c452cfa775",
          "title": "Summary",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "3a07c664-b374-4a48-8726-67c452cfa775",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624457,
              "key": "21cf7b6a-8c74-40be-8d5d-09dfb09fad9a",
              "title": "Summary",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "x6JggcDTcys",
                "china_cdn_id": "x6JggcDTcys.mp4"
              }
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}