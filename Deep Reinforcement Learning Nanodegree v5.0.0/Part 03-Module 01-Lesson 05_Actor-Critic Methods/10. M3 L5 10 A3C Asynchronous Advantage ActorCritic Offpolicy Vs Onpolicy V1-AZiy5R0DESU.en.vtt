WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.025
In case you're not clear on what on-policy versus off-policy learning is,

00:00:05.025 --> 00:00:07.080
let me explain that real quick.

00:00:07.080 --> 00:00:10.365
On-policy learning is when the policy use for

00:00:10.365 --> 00:00:15.120
interacting with the environment is also the policy being learned.

00:00:15.119 --> 00:00:18.765
Off-policy learning is when the policy used for

00:00:18.765 --> 00:00:23.714
interacting with the environment is different than the policy being learned.

00:00:23.714 --> 00:00:28.740
Sarsa is a good example of an on-policy learning agent.

00:00:28.739 --> 00:00:32.254
A sarsa agent uses the same policy to

00:00:32.255 --> 00:00:36.429
interact with the environment as the policy it is learning.

00:00:36.429 --> 00:00:37.950
On the other hand,

00:00:37.950 --> 00:00:42.330
Q-learning is a good example of a off-policy learning agent.

00:00:42.329 --> 00:00:46.854
A Q-learning agent learns about the optimal policy,

00:00:46.854 --> 00:00:53.734
though the policy that generates behavior is an exploratory policy, often epsilon greedy.

00:00:53.734 --> 00:00:57.439
Looking at the update equations for these two methods,

00:00:57.439 --> 00:00:59.644
helps us understand them better.

00:00:59.645 --> 00:01:06.280
As you can see in Sarsa the action used for calculating the TD target and

00:01:06.280 --> 00:01:13.790
the TD error is the action the agent will take in the following time step A prime.

00:01:13.790 --> 00:01:17.690
In Q-learning however, the action used for

00:01:17.689 --> 00:01:22.655
calculating the target is the action with the highest value.

00:01:22.655 --> 00:01:26.060
But this action is not guaranteed to be used by

00:01:26.060 --> 00:01:30.409
the agent for interaction with the environment in the following time step.

00:01:30.409 --> 00:01:35.244
In other words, this is not necessarily A prime.

00:01:35.245 --> 00:01:40.370
The Q-learning agent may choose an exploratory action in the next step.

00:01:40.370 --> 00:01:47.005
In Sarsa that action exploratory or not is already been chosen.

00:01:47.004 --> 00:01:51.379
Q-learning learns the deterministic optimal policy

00:01:51.379 --> 00:01:55.209
even if its behaviour policy is totally random,

00:01:55.209 --> 00:01:59.759
Sarsa learns the best exploratory policy,

00:01:59.760 --> 00:02:03.395
that is the best policy that still explores.

00:02:03.394 --> 00:02:07.459
DQN is also an off-policy learning method,

00:02:07.459 --> 00:02:11.389
your agent behaves with some exploratory policy

00:02:11.389 --> 00:02:16.119
say epsilon greedy where they learns about the optimal policy.

00:02:16.120 --> 00:02:18.610
When using off-policy learning,

00:02:18.610 --> 00:02:23.825
agents are able to learn from many different sources including experiences

00:02:23.824 --> 00:02:29.884
generated by all versions of the agent itself thus the replay buffer.

00:02:29.884 --> 00:02:33.349
However, off-policy learning is known to be

00:02:33.349 --> 00:02:38.000
unstable and often diverge with deep neural networks.

00:02:38.000 --> 00:02:42.425
A3C on the other hand is a non-policy learning method.

00:02:42.425 --> 00:02:45.695
With on-policy learning, you only use

00:02:45.694 --> 00:02:49.924
the data generated by the policy currently being learned about,

00:02:49.925 --> 00:02:52.594
and anytime you improve the policy,

00:02:52.594 --> 00:02:56.590
you toss out old data and go out collect some more.

00:02:56.590 --> 00:03:01.340
On-policy learning is a bit inefficient in the use of experiences,

00:03:01.340 --> 00:03:06.215
but it often has more stable and consistent convergence properties.

00:03:06.215 --> 00:03:10.444
A simple analogy of on and off-policy goes this way.

00:03:10.444 --> 00:03:15.379
On-policy is learning from your own hands-on experience for example,

00:03:15.379 --> 00:03:17.585
the projects in this nano degree.

00:03:17.585 --> 00:03:21.034
As you can imagine that is a pretty good way of learning,

00:03:21.034 --> 00:03:23.495
but it is somewhat data inefficient,

00:03:23.495 --> 00:03:27.835
you can only do so many projects before you run out of time.

00:03:27.835 --> 00:03:31.219
Off-policy on the other hand is learning from

00:03:31.219 --> 00:03:37.669
someone else's experience and as such it is more sample efficient because well,

00:03:37.669 --> 00:03:40.464
you can learn from many different sources.

00:03:40.465 --> 00:03:45.395
However, these way of learning is more prone to misunderstandings,

00:03:45.395 --> 00:03:50.380
I might not be able to explain things in a way that you understand well.

00:03:50.379 --> 00:03:52.129
The nano degree analogy in

00:03:52.129 --> 00:03:56.210
the off-policy case is learning from watching the lessons for example.

00:03:56.210 --> 00:03:58.835
You learn much faster this way but again,

00:03:58.835 --> 00:04:03.770
perhaps not as good and deep as you would from your own hands-on experience.

00:04:03.770 --> 00:04:08.060
Usually a good balance between these two ways of learning allows for

00:04:08.060 --> 00:04:10.925
the best-performing deep reinforcement learning agents

00:04:10.925 --> 00:04:14.905
and perhaps the best-performing humans too, who knew?

00:04:14.905 --> 00:04:18.649
Now, if you're interested in this topic and you want to learn

00:04:18.649 --> 00:04:22.370
about an agent that combines on and off-policy learning,

00:04:22.370 --> 00:04:25.834
I recommend you read the paper by Goodall title

00:04:25.834 --> 00:04:31.334
Q-Prop Sample Efficient Policy Gradient with an off-policy critic.

00:04:31.334 --> 00:04:34.109
But reading these off-policy learning.

00:04:34.110 --> 00:04:37.910
So if you really want to learn about this topic after reading

00:04:37.910 --> 00:04:42.825
the paper go ahead and implement a queue prop agent yourself for your project.

00:04:42.824 --> 00:04:45.500
That will be challenging and fun.

