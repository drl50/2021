WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.040
Hey guys, we got here.

00:00:02.040 --> 00:00:06.000
So, I wanted to show you now the DDPG.

00:00:06.000 --> 00:00:08.984
Now, remember DDPG's somewhat,

00:00:08.984 --> 00:00:12.764
it's questionable if is say an actor-critic or not.

00:00:12.765 --> 00:00:15.600
But nevertheless it's a very important algorithm

00:00:15.599 --> 00:00:20.210
and the guys that created the algorithm said this is actor-critics.

00:00:20.210 --> 00:00:22.700
So, we're going to just go with that. All right.

00:00:22.699 --> 00:00:27.509
So, DDPG it does has an implementation DDPG.

00:00:28.449 --> 00:00:31.309
It's much easier to read.

00:00:31.309 --> 00:00:34.019
So, I wanted to show that one real quick and then go

00:00:34.020 --> 00:00:38.760
to Shangtong Zhang implementation as well.

00:00:38.759 --> 00:00:42.344
So, here in the DDPG pendulum,

00:00:42.344 --> 00:00:45.509
you can see the model. Pretty straightforward.

00:00:45.509 --> 00:00:48.140
There's an actor here, there's a critic here,

00:00:48.140 --> 00:00:51.619
and then here you can see how the action is being passed in

00:00:51.619 --> 00:00:53.750
the forward pass and then it's getting

00:00:53.750 --> 00:00:57.579
concatenated to get the value of that state action pair.

00:00:57.579 --> 00:01:01.039
So, the actor-critic is basically doing a Q function

00:01:01.039 --> 00:01:04.310
which is a little bit different but that's all good. All right.

00:01:04.310 --> 00:01:08.435
So, you can see how the forward pass here is just very regular.

00:01:08.435 --> 00:01:16.310
Here's the initialization function and here's the forward pass.

00:01:16.310 --> 00:01:20.120
You can see here how we're using here a Tanh and

00:01:20.120 --> 00:01:24.960
that is because over the control we're doing at control here pendulum.

00:01:26.900 --> 00:01:30.060
In the critic side here,

00:01:30.060 --> 00:01:32.534
you see how the state size,

00:01:32.534 --> 00:01:36.215
the very beginning is the state size and then you have

00:01:36.215 --> 00:01:39.980
the full connected one units plus the action size

00:01:39.980 --> 00:01:44.045
because this is why it's going to have the concatenation over here.

00:01:44.045 --> 00:01:47.180
So, you do that for pass over there.

00:01:47.180 --> 00:01:48.775
I'm sorry, the forward pass is here.

00:01:48.775 --> 00:01:57.150
You'd do that, fc2 over here to this which is going to be used over here.

00:01:57.150 --> 00:02:01.325
Then it's going to be a relu and then thus

00:02:01.325 --> 00:02:05.939
this is going to be passed to this third layer,

00:02:05.939 --> 00:02:08.259
which is this guy over here.

00:02:08.780 --> 00:02:14.689
Which is one output which is going to be a single value.

00:02:14.689 --> 00:02:18.219
So, that's the model.

00:02:18.219 --> 00:02:21.104
Then we have the agent,

00:02:21.104 --> 00:02:23.859
I'm going to the Python notebook real quick

00:02:23.860 --> 00:02:27.800
because really doesn't have much other than running the code but I wanted to show that.

00:02:27.800 --> 00:02:29.450
So, this is just a running basically,

00:02:29.449 --> 00:02:32.599
resetting the agent and then making the edge enact and

00:02:32.599 --> 00:02:36.430
this the agent step and then the environments step and so on.

00:02:36.430 --> 00:02:40.790
So, this is basically a runner even though the function is called

00:02:40.789 --> 00:02:44.614
the DDPG really doesn't really have anything related to actually just DDPG.

00:02:44.615 --> 00:02:48.350
This is just running the DDPG algorithm which is being

00:02:48.349 --> 00:02:53.414
imported from the DDPG agent, the agent function.

00:02:53.414 --> 00:02:57.028
We're going to do the DDPG agent and we have

00:02:57.028 --> 00:03:02.609
the agent class in there with the actor and the critic, the optimizers.

00:03:02.610 --> 00:03:03.885
So, this is the intro.

00:03:03.884 --> 00:03:07.229
We're a couple interested in things that we talked about in the actor-critic lectures.

00:03:07.229 --> 00:03:13.669
So, the actor you have a local and a target and the critic has a local and a target.

00:03:13.669 --> 00:03:14.719
I call it normal.

00:03:14.719 --> 00:03:19.715
I mean it is just a regular network and then the target network.

00:03:19.715 --> 00:03:22.840
You do have a replay buffer here as you can see.

00:03:22.840 --> 00:03:25.610
There's some process noise that this is

00:03:25.610 --> 00:03:28.805
the way that is another thing that was introduced.

00:03:28.805 --> 00:03:33.504
They put noise into a network to make it explore.

00:03:33.504 --> 00:03:37.280
So, this is step, you add the memory and this is just more like

00:03:37.280 --> 00:03:41.629
the actual replay memory code.

00:03:41.629 --> 00:03:44.224
In the acting code,

00:03:44.224 --> 00:03:53.144
you transform the state from NumPy to Torch and then you do an actor local.

00:03:53.145 --> 00:03:57.210
So, you're grabbing that function and doing a forward pass on that state.

00:03:57.210 --> 00:03:59.224
Moving it to the CPU,

00:03:59.224 --> 00:04:05.539
getting the data and then getting into NumPy and then you set up for training and

00:04:05.539 --> 00:04:09.560
then you're going to be adding noise to the action in

00:04:09.560 --> 00:04:13.985
this way and then we're still going to be clipping between minus one and one.

00:04:13.985 --> 00:04:16.000
In the learn function,

00:04:16.000 --> 00:04:19.714
and I wanted to show you one of the things that we discussed

00:04:19.714 --> 00:04:24.409
in the lecture which is the TAU soft update right there.

00:04:24.410 --> 00:04:27.530
All right. So, the soft update function is as you can see here is

00:04:27.529 --> 00:04:30.799
basically grabbing all of the target model and

00:04:30.800 --> 00:04:33.379
the local model parameters and then it's putting them in

00:04:33.379 --> 00:04:37.654
there and then so it's going to do each other parameters.

00:04:37.654 --> 00:04:41.119
The matching parameters between the target and the local.

00:04:41.120 --> 00:04:43.264
It's going to copy

00:04:43.264 --> 00:04:51.839
a mixture into this guy, into the target.

00:04:51.839 --> 00:04:55.699
So, the target network is basically receiving

00:04:55.699 --> 00:05:00.185
updates that are a combination between the local.

00:05:00.185 --> 00:05:06.845
So, the most up-to-date network and target and itself basically.

00:05:06.845 --> 00:05:10.625
Then, so TAU over there can be a very small number,

00:05:10.625 --> 00:05:12.410
and then in this case,

00:05:12.410 --> 00:05:16.705
I think TAU has a value of 0.001.

00:05:16.704 --> 00:05:23.564
So, basically, that's basically grabbing like a very large chunk,

00:05:23.564 --> 00:05:29.160
like 99.9 percent of

00:05:29.160 --> 00:05:35.025
itself and then a very very small chunk of the other network.

00:05:35.024 --> 00:05:38.209
Here's the noise class.

00:05:38.209 --> 00:05:39.935
You can look at that as well.

00:05:39.935 --> 00:05:41.209
I mean other than that,

00:05:41.209 --> 00:05:45.659
this algorithm is very very much like DQN.

00:05:45.660 --> 00:05:47.745
You have a replay buffer,

00:05:47.745 --> 00:05:50.750
you basically do the same interaction and all that.

00:05:50.750 --> 00:05:54.470
The only thing is how do we deal with the continuous variable,

00:05:54.470 --> 00:06:00.735
I'm sorry with yeah with a continuous action space, continuous actions?

00:06:00.735 --> 00:06:05.555
The way that you do that is just basically you have another optimizer

00:06:05.555 --> 00:06:11.300
that tries to learn the optimal action for any given state.

00:06:11.300 --> 00:06:13.310
Then using that optimal action,

00:06:13.310 --> 00:06:18.149
you can pass it through the other network to get the actual value.

00:06:18.149 --> 00:06:20.549
So, it's pretty nice.

00:06:20.550 --> 00:06:24.079
Now, I'm just going to go and show you where the implementation of

00:06:24.079 --> 00:06:29.159
DDPG is a Shangtong's repository.

00:06:29.160 --> 00:06:30.439
You can look in here.

00:06:30.439 --> 00:06:32.269
You have deep_rl.

00:06:32.269 --> 00:06:36.349
Remember something that the examples file usually

00:06:36.350 --> 00:06:40.025
helps you see the different ways that you can use this.

00:06:40.024 --> 00:06:45.179
So, DDPG low dimensional state and then DDPG pixel.

00:06:45.529 --> 00:06:53.519
You can see if you want to use it for the pixel ones like Torch or you want to do Atari,

00:06:53.519 --> 00:06:55.444
then you'd probably want to look into this.

00:06:55.444 --> 00:06:56.790
If you're going to be using images,

00:06:56.790 --> 00:06:59.650
if you're going to be using low dimensional state,

00:07:01.550 --> 00:07:04.365
like we're doing here for the pendulum,

00:07:04.365 --> 00:07:07.620
then you can see how he has it here.

00:07:07.620 --> 00:07:10.155
So, preset for the pendulum.

00:07:10.154 --> 00:07:14.139
Then I guess in the end he selected a RoboSchoolHopper.

00:07:14.139 --> 00:07:17.615
But it's basically the same thing.

00:07:17.615 --> 00:07:21.574
You can use any of these environments and it will work fine.

00:07:21.574 --> 00:07:28.399
So, the DeterministicActorCriticNet, you can see here the body of the actor is that.

00:07:28.399 --> 00:07:30.589
The body of the critic is a little bit difference,

00:07:30.589 --> 00:07:33.299
two-layer fully convolutional with action, interesting.

00:07:33.300 --> 00:07:35.939
All right. We probably need to look into that and see what that is.

00:07:35.939 --> 00:07:39.290
It's probably exactly what we just discussed on the previous one.

00:07:39.290 --> 00:07:42.545
The random process is the noise that we talked about here.

00:07:42.545 --> 00:07:44.030
He has the full name,

00:07:44.029 --> 00:07:48.329
which I'm not going to even try to pronounce.

00:07:48.329 --> 00:07:53.180
But yeah, other than that you see a target network can make

00:07:53.180 --> 00:07:55.310
mix over here is 1e to the

00:07:55.310 --> 00:07:58.384
minus 3 which is basically the same that we had in the other one.

00:07:58.384 --> 00:08:00.769
So there are many similar things.

00:08:00.769 --> 00:08:02.750
There's going to be a different style repository.

00:08:02.750 --> 00:08:05.660
That's all. Let's take a look real quick at the deterministic.

00:08:05.660 --> 00:08:08.270
So, the bodies you go deterministic.

00:08:08.269 --> 00:08:10.185
So, it was this one,

00:08:10.185 --> 00:08:12.639
two layer with action.

00:08:12.639 --> 00:08:16.444
So, here you can see how we have a similar bit of code,

00:08:16.444 --> 00:08:18.050
torch.cat, X,

00:08:18.050 --> 00:08:22.040
action and here you can see how the hidden size plus the action that I mentioned.

00:08:22.040 --> 00:08:23.810
So, it's basically the same thing.

00:08:23.810 --> 00:08:30.930
Here in the heads, the head says that the DeterministicActionNet.

00:08:30.930 --> 00:08:34.335
So, we look for the DeterministicActionNet.

00:08:34.335 --> 00:08:37.280
We get here and here you can see that we're

00:08:37.279 --> 00:08:41.100
using an ActorCriticNet which is basically the same network,

00:08:41.100 --> 00:08:50.235
actually same base as the A2C that I talked about in a few videos I well suppose.

00:08:50.235 --> 00:08:53.450
So, yeah, you can see here the deterministic action and

00:08:53.450 --> 00:08:56.960
he's using that and then here you have the function for

00:08:56.960 --> 00:09:00.560
the actor operation optimization sorry and

00:09:00.559 --> 00:09:04.669
then the credit for the sub-optimization function as well,

00:09:04.669 --> 00:09:10.879
which is going to be passed in through this which again you go back to

00:09:10.879 --> 00:09:12.919
the examples and then you can see here how

00:09:12.919 --> 00:09:18.979
the optimizer is going to be somewhere in here.

00:09:20.419 --> 00:09:25.129
So, Adam, the learning rate in one is a little bit different.

00:09:25.129 --> 00:09:30.465
I think this is actually using similar hyperparameters than in the paper.

00:09:30.465 --> 00:09:34.220
I think in the paper they did use these two learning rates or

00:09:34.220 --> 00:09:38.509
they for sure use different one for the actor and one for the critic.

00:09:38.509 --> 00:09:40.649
So, maybe take a look at that.

00:09:40.649 --> 00:09:44.049
So, it's a completely different implementation when you want to see the agent,

00:09:44.049 --> 00:09:45.219
this is the A2C.

00:09:45.220 --> 00:09:48.160
I go back. I can go back to the DDPG agent.

00:09:48.159 --> 00:09:49.959
Here's the DDPG agent,

00:09:49.960 --> 00:09:51.190
you can actually look at it.

00:09:51.190 --> 00:09:53.260
There is a soft update here,

00:09:53.259 --> 00:09:54.975
there is an eval step,

00:09:54.975 --> 00:09:59.710
there is evaluation, some preparation for that.

00:10:00.559 --> 00:10:04.334
This is the regular learning steps.

00:10:04.335 --> 00:10:09.004
You can see here you take an action grab that normalizes state.

00:10:09.004 --> 00:10:12.779
You have to look sometimes the normalizer, doesn't really normalize.

00:10:12.779 --> 00:10:14.069
Now, that we want normalizer,

00:10:14.070 --> 00:10:16.100
I'm actually clip the reward as well.

00:10:16.100 --> 00:10:18.350
You have to look into that. Replay buffer.

00:10:18.350 --> 00:10:21.620
You want to feed this experience tuples state action,

00:10:21.620 --> 00:10:25.039
so you send it over there and then if the replay buffer is greater

00:10:25.039 --> 00:10:29.419
than some memory size then you're going to get into the learning steps.

00:10:29.419 --> 00:10:35.375
Then here you can see how you grab the experiences from the replay buffer.

00:10:35.375 --> 00:10:41.230
You unwrap that into the correct variables.

00:10:41.720 --> 00:10:46.700
So, you pass out and the target network through the target network and

00:10:46.700 --> 00:10:50.990
then here is the forward pass interestingly that is being done this way.

00:10:50.990 --> 00:10:53.524
But I think there's a reason for that.

00:10:53.524 --> 00:10:57.199
You may need to dig into it a little bit more.

00:10:57.200 --> 00:10:59.629
So, here's the discount, q_next,

00:10:59.629 --> 00:11:02.580
calculate the q_next and so on.

00:11:02.629 --> 00:11:09.320
The zero_grad and then the step of the optimizer for the critic in there,

00:11:09.320 --> 00:11:16.730
then the optimizer for the actor and then you update the network in a soft update matter.

00:11:16.730 --> 00:11:18.659
That is it.

00:11:18.659 --> 00:11:21.004
Well, I hope this is useful.

00:11:21.004 --> 00:11:22.835
This is a very nice implementation.

00:11:22.835 --> 00:11:28.775
I was actually delighted to see very nicely written code.

00:11:28.774 --> 00:11:30.754
So, I want to show this one as well.

00:11:30.754 --> 00:11:34.049
Hope this is useful for you guys and yeah.

00:11:34.049 --> 00:11:37.199
Have fun with your reinforcement learning journey.

