WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.500
DDPG is a different kind of actor-critic method.

00:00:04.500 --> 00:00:08.955
In fact, it could be seen as approximate DQN,

00:00:08.955 --> 00:00:11.939
instead of an actual actor critic.

00:00:11.939 --> 00:00:16.844
The reason for this is that the critic in DDPG,

00:00:16.844 --> 00:00:23.129
is used to approximate the maximizer over the Q values of the next state,

00:00:23.129 --> 00:00:25.724
and not as a learned baseline,

00:00:25.725 --> 00:00:27.780
as we have seen so far.

00:00:27.780 --> 00:00:29.220
Though, this is still

00:00:29.219 --> 00:00:34.394
a very important algorithm and it is good to discuss it in more detail.

00:00:34.395 --> 00:00:38.720
One of the limitations of the DQN agent is that it

00:00:38.719 --> 00:00:43.070
is not straightforward to use in continuous action spaces.

00:00:43.070 --> 00:00:49.534
Imagine a DQN network that takes inner state and outputs the action value function.

00:00:49.534 --> 00:00:51.509
For example, for two action,

00:00:51.509 --> 00:00:55.409
say, up and down, Q(s,

00:00:55.409 --> 00:01:00.169
"up") gives you the estimated expected value for selecting

00:01:00.170 --> 00:01:07.780
the up action in state s, say minus 2.18.

00:01:08.079 --> 00:01:14.359
Q(s "down"), gives you the estimated expected value for selecting

00:01:14.359 --> 00:01:20.230
the down action in state s, say 8.45.

00:01:20.230 --> 00:01:25.670
To find the max action value function for this state,

00:01:25.670 --> 00:01:29.314
you just calculate the max of these values. Pretty easy.

00:01:29.314 --> 00:01:32.480
It's very easy to do a max operation in

00:01:32.480 --> 00:01:36.859
this example because this is a discrete action space.

00:01:36.859 --> 00:01:40.939
Even if you had more actions say a left,

00:01:40.939 --> 00:01:45.284
a right, a jump and so on,

00:01:45.284 --> 00:01:48.724
you still have a discrete action space.

00:01:48.724 --> 00:01:52.609
Even if it was high dimensional with many,

00:01:52.609 --> 00:01:56.495
many more actions, it would still be doable.

00:01:56.495 --> 00:02:00.935
But why do you need an action with continuous range?

00:02:00.935 --> 00:02:06.519
How do you get the value of a continuous action with this architecture?

00:02:06.519 --> 00:02:10.469
Say you want the jump action to be continuous,

00:02:10.469 --> 00:02:15.484
a variable between one and 100 centimeters.

00:02:15.485 --> 00:02:20.975
How do you find the value of jump, say 50 centimeters.

00:02:20.974 --> 00:02:24.449
This is one of the problems DDPG solves.

00:02:24.449 --> 00:02:28.429
In DDPG, we use two deep neural networks.

00:02:28.430 --> 00:02:32.485
We can call one the actor and the other the critic.

00:02:32.485 --> 00:02:34.580
Nothing new to this point.

00:02:34.580 --> 00:02:41.630
Now, the actor here is used to approximate the optimal policy deterministically.

00:02:41.629 --> 00:02:48.319
That means we want to always output the best believed action for any given state.

00:02:48.319 --> 00:02:52.219
This is unlike a stochastic policies in which we want the policy to

00:02:52.219 --> 00:02:56.319
learn a probability distribution over the actions.

00:02:56.319 --> 00:03:03.634
In DDPG, we want the believed best action every single time we query the actor network.

00:03:03.634 --> 00:03:06.560
That is a deterministic policy.

00:03:06.560 --> 00:03:11.659
The actor is basically learning the argmax a Q(S,

00:03:11.659 --> 00:03:15.185
a), which is the best action.

00:03:15.185 --> 00:03:18.259
The critic learns to evaluate

00:03:18.259 --> 00:03:25.250
the optimal action value function by using the actors best believed action.

00:03:25.250 --> 00:03:28.068
Again, we use this actor,

00:03:28.068 --> 00:03:31.405
which is an approximate maximizer,

00:03:31.405 --> 00:03:38.283
to calculate a new target value for training the action value function,

00:03:38.283 --> 00:03:40.909
much in the way DQN does.

