<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Summary
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Introduction to Policy-Based Methods
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Policy-Based Methods.html">
       01. Policy-Based Methods
      </a>
     </li>
     <li class="">
      <a href="02. Policy Function Approximation.html">
       02. Policy Function Approximation
      </a>
     </li>
     <li class="">
      <a href="03. More on the Policy.html">
       03. More on the Policy
      </a>
     </li>
     <li class="">
      <a href="04. Hill Climbing.html">
       04. Hill Climbing
      </a>
     </li>
     <li class="">
      <a href="05. Hill Climbing Pseudocode.html">
       05. Hill Climbing Pseudocode
      </a>
     </li>
     <li class="">
      <a href="06. Beyond Hill Climbing.html">
       06. Beyond Hill Climbing
      </a>
     </li>
     <li class="">
      <a href="07. More Black-Box Optimization.html">
       07. More Black-Box Optimization
      </a>
     </li>
     <li class="">
      <a href="08. Coding Exercise.html">
       08. Coding Exercise
      </a>
     </li>
     <li class="">
      <a href="09. Workspace.html">
       09. Workspace
      </a>
     </li>
     <li class="">
      <a href="10. OpenAI Request for Research.html">
       10. OpenAI Request for Research
      </a>
     </li>
     <li class="">
      <a href="11. Why Policy-Based Methods.html">
       11. Why Policy-Based Methods?
      </a>
     </li>
     <li class="">
      <a href="12. Summary.html">
       12. Summary
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          12. Summary
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="summary">
          Summary
         </h1>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Objective function" class="img img-fluid" src="img/screen-shot-2018-06-26-at-11.53.35-am.png"/>
          <figcaption class="figure-caption">
           <p>
            Objective function
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-policy-based-methods">
          ### Policy-Based Methods
         </h2>
         <ul>
          <li>
           With
           <strong>
            value-based methods
           </strong>
           , the agent uses its experience with the environment to maintain an estimate of the optimal action-value function.  The optimal policy is then obtained from the optimal action-value function estimate.
          </li>
          <li>
           <strong>
            Policy-based methods
           </strong>
           directly learn the optimal policy, without having to maintain a separate value function estimate.
          </li>
         </ul>
         <h2 id="-policy-function-approximation">
          ### Policy Function Approximation
         </h2>
         <ul>
          <li>
           In deep reinforcement learning, it is common to represent the policy with a neural network.
           <ul>
            <li>
             This network takes the environment state as
             <strong>
              <em>
               input
              </em>
             </strong>
             .
            </li>
            <li>
             If the environment has discrete actions, the
             <strong>
              <em>
               output
              </em>
             </strong>
             layer has a node for each possible action and contains the probability that the agent should select each possible action.
            </li>
           </ul>
          </li>
          <li>
           The weights in this neural network are initially set to random values.  Then, the agent updates the weights as it interacts with (
           <em>
            and learns more about
           </em>
           ) the environment.
          </li>
         </ul>
         <h2 id="-more-on-the-policy">
          ### More on the Policy
         </h2>
         <ul>
          <li>
           Policy-based methods can learn either stochastic or deterministic policies, and they can be used to solve environments with either finite or continuous action spaces.
          </li>
         </ul>
         <h2 id="-hill-climbing">
          ### Hill Climbing
         </h2>
         <ul>
          <li>
           <strong>
            Hill climbing
           </strong>
           is an iterative algorithm that can be used to find the weights
           <span class="mathquill ud-math">
            \theta
           </span>
           for an optimal policy.
          </li>
          <li>
           At each iteration,
           <ul>
            <li>
             We slightly perturb the values of the current best estimate for the weights
             <span class="mathquill ud-math">
              \theta_{best}
             </span>
             , to yield a new set of weights.
            </li>
            <li>
             These new weights are then used to collect an episode.  If the new weights
             <span class="mathquill ud-math">
              \theta_{new}
             </span>
             resulted in higher return than the old weights, then we set
             <span class="mathquill ud-math">
              \theta_{best} \leftarrow \theta_{new}
             </span>
             .
            </li>
           </ul>
          </li>
         </ul>
         <h2 id="-beyond-hill-climbing">
          ### Beyond Hill Climbing
         </h2>
         <ul>
          <li>
           <strong>
            Steepest ascent hill climbing
           </strong>
           is a variation of hill climbing that chooses a small number of neighboring policies at each iteration and chooses the best among them.
          </li>
          <li>
           <strong>
            Simulated annealing
           </strong>
           uses a pre-defined schedule to control how the policy space is explored, and gradually reduces the search radius as we get closer to the optimal solution.
          </li>
          <li>
           <strong>
            Adaptive noise scaling
           </strong>
           decreases the search radius with each iteration when a new best policy is found, and otherwise increases the search radius.
          </li>
         </ul>
         <h2 id="-more-black-box-optimization">
          ### More Black-Box Optimization
         </h2>
         <ul>
          <li>
           The
           <strong>
            cross-entropy method
           </strong>
           iteratively suggests a small number of neighboring policies, and uses a small percentage of the best performing policies to calculate a new estimate.
          </li>
          <li>
           The
           <strong>
            evolution strategies
           </strong>
           technique considers the return corresponding to each candidate policy.  The policy estimate at the next iteration is a weighted sum of all of the candidate policies, where policies that got higher return are given higher weight.
          </li>
         </ul>
         <h2 id="-why-policy-based-methods">
          ### Why Policy-Based Methods?
         </h2>
         <ul>
          <li>
           There are three reasons why we consider policy-based methods:
           <ol>
            <li>
             <strong>
              Simplicity
             </strong>
             : Policy-based methods directly get to the problem at hand (estimating the optimal policy), without having to store a bunch of additional data (i.e., the action values) that may not be useful.
            </li>
            <li>
             <strong>
              Stochastic policies
             </strong>
             : Unlike value-based methods, policy-based methods can learn true stochastic policies.
            </li>
            <li>
             <strong>
              Continuous action spaces
             </strong>
             : Policy-based methods are well-suited for continuous action spaces.
            </li>
           </ol>
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('12. Summary')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
