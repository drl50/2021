WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.964
After learning important sampling and the clip surrogate function,

00:00:03.964 --> 00:00:07.230
we can finally summarize the PPO algorithm.

00:00:07.230 --> 00:00:12.150
First, we collect some trajectories based on some policy pi theta,

00:00:12.150 --> 00:00:16.339
and initialize theta prime to be equal to theta.

00:00:16.339 --> 00:00:18.809
Next, we compute the gradient of

00:00:18.809 --> 00:00:23.000
the clip surrogate function using the collected trajectories.

00:00:23.000 --> 00:00:27.289
Then we update theta prime using gradient ascent.

00:00:27.289 --> 00:00:30.484
After that we can repeat step two and three,

00:00:30.484 --> 00:00:36.185
re-computing the clip surrogate function and updating the policy again and again,

00:00:36.185 --> 00:00:38.420
without generating new trajectories.

00:00:38.420 --> 00:00:43.429
Although typically step two to three are only repeated a few times.

00:00:43.429 --> 00:00:46.549
Then we go back to step one generating

00:00:46.549 --> 00:00:51.394
more trajectories and repeat all the previous steps, and that's it.

00:00:51.395 --> 00:00:55.100
Now that you've learned what Proximal Policy Optimization is,

00:00:55.100 --> 00:00:58.870
let's put it in action and implemented it.

