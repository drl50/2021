WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:03.390
In this video, we'll learn how to use

00:00:03.390 --> 00:00:07.030
important sampling in the context of policy gradient,

00:00:07.030 --> 00:00:10.789
which will lead us to the surrogate function.

00:00:11.240 --> 00:00:15.655
Say we're trying to update our policy, Pi Theta Prime.

00:00:15.654 --> 00:00:19.214
To do that, we need to estimate a gradient, g,

00:00:19.214 --> 00:00:23.795
but we only have trajectories generated by an older policy, Pi Theta.

00:00:23.795 --> 00:00:26.315
How do we compute the gradient then?

00:00:26.315 --> 00:00:29.740
Mathematically, we could utilize important sampling,

00:00:29.739 --> 00:00:33.594
and the answer is just what a normal policy gradient would be,

00:00:33.594 --> 00:00:35.975
times a re-weighting factor.

00:00:35.975 --> 00:00:39.024
We can rearrange these equations,

00:00:39.024 --> 00:00:45.140
and the re-weighting factor is just the product of all the policy across each step.

00:00:45.140 --> 00:00:49.344
Notice that I've picked out the parts at time step t here.

00:00:49.344 --> 00:00:52.100
We can rearrange the equation a little bit more,

00:00:52.100 --> 00:00:55.789
and notice that we can cancel the terms on the left,

00:00:55.789 --> 00:01:00.490
but still we are left with a product of policies at different times,

00:01:00.490 --> 00:01:02.480
denoted by the dot dot dot.

00:01:02.479 --> 00:01:05.765
Can we somehow simplify this expression further?

00:01:05.765 --> 00:01:09.855
Well, this is where the idea of proximal policy comes in.

00:01:09.855 --> 00:01:13.925
If the old and current policy is close enough to each other,

00:01:13.924 --> 00:01:17.500
all these factors will be pretty close to one.

00:01:17.500 --> 00:01:21.355
Then perhaps we can ignore them.

00:01:21.355 --> 00:01:25.954
So, now the equation simplifies even further.

00:01:25.954 --> 00:01:29.454
It looks very similar to the old policy gradient.

00:01:29.454 --> 00:01:34.179
In fact, if the current policy is the same as the old policy,

00:01:34.180 --> 00:01:39.090
we would have exactly the same vanilla policy gradient,

00:01:39.090 --> 00:01:41.549
but remember, this expression is

00:01:41.549 --> 00:01:45.784
different because we are comparing two different policies.

00:01:45.784 --> 00:01:49.159
Now that we have the approximate form of the gradient,

00:01:49.159 --> 00:01:56.129
we can think of it as the gradient of a new object called the surrogate function.

00:01:56.129 --> 00:01:58.084
So, using this new gradient,

00:01:58.084 --> 00:02:01.059
we can perform gradient ascent to update our policy,

00:02:01.060 --> 00:02:05.579
which we can think of as directly maximizing the surrogate function.

00:02:05.579 --> 00:02:08.780
But there's still one important issue we have not addressed yet.

00:02:08.780 --> 00:02:13.140
If we keep re-using old trajectories and updating our policy,

00:02:13.139 --> 00:02:17.289
at some point the new policy might become different enough from the old one,

00:02:17.289 --> 00:02:20.919
so all the approximations we made could become invalid.

00:02:20.919 --> 00:02:23.964
We need to find a way to make sure this doesn't happen.

00:02:23.965 --> 00:02:26.599
Let's see how in part two.

