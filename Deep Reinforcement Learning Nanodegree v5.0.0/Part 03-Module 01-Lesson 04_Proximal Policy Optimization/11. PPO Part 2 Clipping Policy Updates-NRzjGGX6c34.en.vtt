WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.850
In this video, we will learn how to clip

00:00:02.850 --> 00:00:08.460
the surrogate function to ensure that the new policy remains close to the old one.

00:00:08.460 --> 00:00:13.679
So, really, what is the problem of updating our policy and ignoring the fact that

00:00:13.679 --> 00:00:15.375
the approximations may not be valid

00:00:15.375 --> 00:00:19.370
anymore or the problem basically lead to a really bad policy.

00:00:19.370 --> 00:00:21.200
Let's see that in action.

00:00:21.199 --> 00:00:25.714
Say we have some policy parameterized by Theta prime shown on the left,

00:00:25.714 --> 00:00:28.245
with the reward function shown on the right.

00:00:28.245 --> 00:00:29.970
The current policy is labeled by

00:00:29.969 --> 00:00:34.450
the red dot and the goal is to update the policy to the optimal one,

00:00:34.450 --> 00:00:36.405
they're both by the green star.

00:00:36.405 --> 00:00:41.390
To update the policy, you can compute a surrogate function shown by the red curve.

00:00:41.390 --> 00:00:44.960
So, this function approximates that we worked pretty well

00:00:44.960 --> 00:00:49.389
around the current policy but far away from the current policy diverges.

00:00:49.389 --> 00:00:53.590
So, if we continually update the policy by performing gradient ascent,

00:00:53.590 --> 00:00:57.005
we might get something like this and then this.

00:00:57.005 --> 00:01:00.545
The big problem here is that at some point we hit a cliff.

00:01:00.545 --> 00:01:05.900
Where the policy changes by a large amount from the perspective of the circuit function,

00:01:05.900 --> 00:01:07.915
the average reward is really great,

00:01:07.915 --> 00:01:12.050
but in reality the actual reward is actually really bad.

00:01:12.049 --> 00:01:16.519
The worst part is the policy is now stuck in a deep and a flat bottom.

00:01:16.519 --> 00:01:20.159
So that future updates won't be able to bring the policy back up,

00:01:20.159 --> 00:01:22.814
and now we're stuck with a really bad policy.

00:01:22.814 --> 00:01:24.664
How do we fix this?

00:01:24.665 --> 00:01:27.650
Wouldn't it be great if we could somehow stop

00:01:27.650 --> 00:01:31.385
the gradient ascent so that are policy doesn't fall off the cliff?

00:01:31.385 --> 00:01:33.440
Well, here's an idea.

00:01:33.439 --> 00:01:37.739
What if we just flatten the surrogate function like this?

00:01:38.060 --> 00:01:41.484
Now, what would policy update look like then?

00:01:41.484 --> 00:01:44.155
So, starting with the current policy,

00:01:44.155 --> 00:01:45.965
now labelled by the blue,

00:01:45.965 --> 00:01:47.850
we can apply gradient ascent.

00:01:47.849 --> 00:01:51.949
The updates remain the same until we hit the plateau.

00:01:51.950 --> 00:01:54.409
Now because the reward function is flat,

00:01:54.409 --> 00:01:59.155
the gradient is zero and the policy update will stop now.

00:01:59.155 --> 00:02:02.885
So, keep in mind that right now we're only showing

00:02:02.885 --> 00:02:06.125
a two dimensional figure with one theta prime direction,

00:02:06.125 --> 00:02:09.650
in most cases, there are thousands of parameters in the policy.

00:02:09.650 --> 00:02:12.800
So, there could be thousands of high dimensional cliffs in

00:02:12.800 --> 00:02:15.860
many different directions and applying this clipping

00:02:15.860 --> 00:02:19.340
mathematically will automatically take care of all these cliffs.

00:02:19.340 --> 00:02:22.175
How do we formalize this clipping procedure then?

00:02:22.175 --> 00:02:28.180
Well, we can write the clipped surrogate function this way like this.

00:02:28.180 --> 00:02:32.000
Now, the formula looks a little bit complicated but the idea is pretty simple.

00:02:32.000 --> 00:02:36.080
So, let's dissect the formula by looking at one specific term

00:02:36.080 --> 00:02:40.805
in the sum and set the future reward to 1 to make things easier.

00:02:40.805 --> 00:02:43.629
First, we start with the original surrogate function.

00:02:43.629 --> 00:02:49.840
So, in the red which is basically just the ratio Pi Theta prime over Pi Theta.

00:02:49.840 --> 00:02:54.935
The black dot shows location with the current policy as the same as the old one.

00:02:54.935 --> 00:03:01.219
Now, we want to make sure that two policies are similar or that the ratio is close to 1.

00:03:01.219 --> 00:03:03.275
So, we can choose a small Epsilon,

00:03:03.275 --> 00:03:05.280
usually 0.1 or 0.2,

00:03:05.280 --> 00:03:08.360
and apply the clip function to force

00:03:08.360 --> 00:03:13.920
this ratio to be within a small Epsilon window of 1 like this.

00:03:13.919 --> 00:03:19.424
But we actually only want to clip the top part and not the bottom part of the function.

00:03:19.425 --> 00:03:24.745
To do that, we can compare the clipped function to the original one and take the minimum.

00:03:24.745 --> 00:03:30.295
Like this in blue. Now, with this minimum function,

00:03:30.294 --> 00:03:31.750
this also ensures that

00:03:31.750 --> 00:03:35.689
the clipped surrogate function is always less than the original one.

00:03:35.689 --> 00:03:39.775
This way when we try to maximize the clip surrogate function,

00:03:39.775 --> 00:03:43.349
we're also indirectly maximizing the circuit function.

00:03:43.349 --> 00:03:45.025
So in a sense,

00:03:45.025 --> 00:03:49.115
we have a more conservative optimization procedure.

00:03:49.115 --> 00:03:53.344
Now, comparing all the curves together looks something like this.

00:03:53.344 --> 00:03:59.039
So, that's it. The essence of the PPO algorithm is simply computing

00:03:59.039 --> 00:04:02.349
the clipped surrogate function and performing updates multiple

00:04:02.349 --> 00:04:06.879
times using gradient ascent on the clipped surrogate function.

