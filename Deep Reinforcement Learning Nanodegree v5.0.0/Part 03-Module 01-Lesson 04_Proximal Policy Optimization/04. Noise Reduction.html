<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Noise Reduction
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Proximal Policy Optimization
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Instructor Introduction.html">
       01. Instructor Introduction
      </a>
     </li>
     <li class="">
      <a href="02. Lesson Preview.html">
       02. Lesson Preview
      </a>
     </li>
     <li class="">
      <a href="03. Beyond REINFORCE.html">
       03. Beyond REINFORCE
      </a>
     </li>
     <li class="">
      <a href="04. Noise Reduction.html">
       04. Noise Reduction
      </a>
     </li>
     <li class="">
      <a href="05. Credit Assignment.html">
       05. Credit Assignment
      </a>
     </li>
     <li class="">
      <a href="06. Policy Gradient Quiz.html">
       06. Policy Gradient Quiz
      </a>
     </li>
     <li class="">
      <a href="07. pong with REINFORCE (code walkthrough).html">
       07. pong with REINFORCE (code walkthrough)
      </a>
     </li>
     <li class="">
      <a href="08. pong with REINFORCE (workspace).html">
       08. pong with REINFORCE (workspace)
      </a>
     </li>
     <li class="">
      <a href="09. Importance Sampling.html">
       09. Importance Sampling
      </a>
     </li>
     <li class="">
      <a href="10. PPO part 1- The Surrogate Function.html">
       10. PPO part 1- The Surrogate Function
      </a>
     </li>
     <li class="">
      <a href="11. PPO part 2- Clipping Policy Updates.html">
       11. PPO part 2- Clipping Policy Updates
      </a>
     </li>
     <li class="">
      <a href="12. PPO summary.html">
       12. PPO summary
      </a>
     </li>
     <li class="">
      <a href="13. pong with PPO (code walkthrough).html">
       13. pong with PPO (code walkthrough)
      </a>
     </li>
     <li class="">
      <a href="14. pong with PPO (workspace).html">
       14. pong with PPO (workspace)
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          04. Noise Reduction
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
         <p>
          Noise Reduction
         </p>
        </h3>
        <video controls="">
         <source src="04. Noise Reduction-GCGqT2knFJ0.mp4" type="video/mp4"/>
         <track default="true" kind="subtitles" label="en" src="04. Noise Reduction-GCGqT2knFJ0.en.vtt" srclang="en"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="noise-reduction">
          Noise Reduction
         </h1>
         <p>
          The way we optimize the policy is by maximizing the average rewards
          <span class="mathquill ud-math">
           U(\theta)
          </span>
          . To do that we use stochastic gradient ascent. Mathematically, the gradient is given by an average over all the possible trajectories,
         </p>
         <div class="mathquill">
          \nabla_\theta U(\theta)
= 
\overbrace{\sum_\tau P(\tau; \theta)}^{
\begin{matrix}
\scriptsize\textrm{average over}\\
\scriptsize\textrm{all trajectories}
\end{matrix}
}
\underbrace{\left(
R_\tau \sum_t \nabla_\theta \log \pi_\theta(a_t^{(\tau)}|s_t^{(\tau)})
\right)}_{
\textrm{only one is sampled}
}
         </div>
         <p>
          There could easily be well over millions of trajectories for simple problems, and infinite for continuous problems.
         </p>
         <p>
          For practical purposes, we simply take one trajectory to compute the gradient, and update our policy. So a lot of times, the result of a sampled trajectory comes down to chance, and doesn't contain that much information about our policy. How does learning happen then? The hope is that after training for a long time, the tiny signal accumulates.
         </p>
         <p>
          The easiest option to reduce the noise in the gradient is to simply sample more trajectories! Using distributed computing, we can collect multiple trajectories in parallel, so that it wonâ€™t take too much time. Then we can estimate the policy gradient by averaging across all the different trajectories
         </p>
         <div class="mathquill">
          \left.
\begin{matrix}
s^{(1)}_t, a^{(1)}_t, r^{(1)}_t\\[6pt]
s^{(2)}_t, a^{(2)}_t, r^{(2)}_t\\[6pt]
s^{(3)}_t, a^{(3)}_t, r^{(3)}_t\\[6pt]
\vdots  
\end{matrix}
\;\;
\right\}\!\!\!\!
\rightarrow
g = \frac{1}{N}\sum_{i=1}^N 
R_i \sum_t\nabla_\theta \log \pi_\theta(a^{(i)}_t | s^{(i)}_t)
         </div>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="rewards-normalization">
          Rewards Normalization
         </h1>
         <p>
          There is another bonus for running multiple trajectories: we can collect all the total rewards and get a sense of how they are distributed.
         </p>
         <p>
          In many cases, the distribution of rewards shifts as learning happens. Reward = 1 might be really good in the beginning, but really bad after 1000 training episode.
         </p>
         <p>
          Learning can be improved if we normalize the rewards, where
          <span class="mathquill ud-math">
           \mu
          </span>
          is the mean, and
          <span class="mathquill ud-math">
           \sigma
          </span>
          the standard deviation.
         </p>
         <div class="mathquill">
          R_i \leftarrow
\frac{R_i -\mu}{\sigma}
\qquad
\mu = \frac{1}{N}\sum_i^N R_i
\qquad
\sigma = \sqrt{\frac{1}{N}\sum_i (R_i - \mu)^2}
         </div>
         <p>
          (when all the
          <span class="mathquill ud-math">
           R_i
          </span>
          are the same,
          <span class="mathquill ud-math">
           \sigma =0
          </span>
          , we can set all the normalized rewards to 0 to avoid numerical problems)
         </p>
         <p>
          This batch-normalization technique is also used in many other problems in AI (e.g. image classification), where normalizing the input can improve learning.
         </p>
         <p>
          Intuitively, normalizing the rewards roughly corresponds to picking half the actions to encourage/discourage, while also making sure the steps for gradient ascents are not too large/small.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="05. Credit Assignment.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('04. Noise Reduction')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
