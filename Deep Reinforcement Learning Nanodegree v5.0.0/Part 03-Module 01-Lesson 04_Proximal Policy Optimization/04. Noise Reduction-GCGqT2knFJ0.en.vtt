WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.209
Here, we will explain why the policy gradient is noisy,

00:00:05.209 --> 00:00:08.769
and discuss ways to reduce this noise.

00:00:09.439 --> 00:00:12.084
The way we optimized the policy,

00:00:12.085 --> 00:00:15.839
is by maximizing the average reward, U Theta.

00:00:15.839 --> 00:00:18.475
To do that, we use gradient ascent.

00:00:18.475 --> 00:00:23.745
Mathematically, the gradient is given by an average of the terms in the parentheses,

00:00:23.745 --> 00:00:28.060
over all the possible trajectories labeled by Tau.

00:00:28.059 --> 00:00:32.310
Now, the number of trajectories could easily be over millions,

00:00:32.310 --> 00:00:34.885
even for simple discrete problems,

00:00:34.884 --> 00:00:37.854
an infinite for continuous problems.

00:00:37.854 --> 00:00:40.079
So, for practical purposes,

00:00:40.079 --> 00:00:42.679
we simply take one trajectory,

00:00:42.679 --> 00:00:44.729
and compute a gradient,

00:00:44.729 --> 00:00:47.194
and then update our policy.

00:00:47.195 --> 00:00:51.564
A lot of times, the results of a sample trajectory,

00:00:51.564 --> 00:00:53.644
simply comes down to chance,

00:00:53.645 --> 00:00:57.609
and doesn't [inaudible] contain damage information about our policy.

00:00:57.609 --> 00:00:59.795
How does learning happen then?

00:00:59.795 --> 00:01:03.710
Well, the hope is that after training for really long time,

00:01:03.710 --> 00:01:06.040
these tiny signal accumulates.

00:01:06.040 --> 00:01:08.705
Still, it would be great if we could

00:01:08.704 --> 00:01:12.174
reduce these random noise in the sample trajectories.

00:01:12.174 --> 00:01:17.429
The easiest option is to simply sample more trajectories.

00:01:18.079 --> 00:01:23.409
Using distributed computing, we can even collect these trajectories in parallel,

00:01:23.409 --> 00:01:25.584
so that it won't take too much time.

00:01:25.584 --> 00:01:28.219
Then we can estimate the policy gradient,

00:01:28.219 --> 00:01:31.760
by simply averaging across all the different trajectories,

00:01:31.760 --> 00:01:33.855
by the formula G here.

00:01:33.855 --> 00:01:37.290
There's another bonus for running multiple trajectories,

00:01:37.290 --> 00:01:40.125
is that we can collect all the total rewards,

00:01:40.125 --> 00:01:42.829
and get a sense of how they are distributed.

00:01:42.829 --> 00:01:48.224
In many cases, the distribution of rewards shifts, as learning happens.

00:01:48.224 --> 00:01:54.559
An episode with total reward equals one might be really good early on in the training,

00:01:54.560 --> 00:01:57.819
but really bad after a thousand training episodes.

00:01:57.819 --> 00:02:03.534
Learning can be improved if we normalize the rewards like this,

00:02:03.534 --> 00:02:06.019
where Mu is the mean,

00:02:06.019 --> 00:02:08.560
and Sigma the standard deviation.

00:02:08.560 --> 00:02:14.125
Dispatch normalization technique is also used in many other problems in AI,

00:02:14.125 --> 00:02:16.064
such as image recognition,

00:02:16.064 --> 00:02:19.949
where normalizing the input, can improve learning.

