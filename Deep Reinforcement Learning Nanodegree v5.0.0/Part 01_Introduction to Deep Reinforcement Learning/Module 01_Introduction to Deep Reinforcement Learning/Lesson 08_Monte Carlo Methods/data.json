{
  "data": {
    "lesson": {
      "id": 671519,
      "key": "b1d9586f-1b1a-48f4-be1e-4c08a0912082",
      "title": "Monte Carlo Methods",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Write your own implementation of Monte Carlo control to teach an agent to play Blackjack!",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/b1d9586f-1b1a-48f4-be1e-4c08a0912082/671519/1544455995219/Monte+Carlo+Methods+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/b1d9586f-1b1a-48f4-be1e-4c08a0912082/671519/1544455991565/Monte+Carlo+Methods+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 617119,
          "key": "5352e3e6-021b-4775-8655-ae4223704c2d",
          "title": "Review",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5352e3e6-021b-4775-8655-ae4223704c2d",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617215,
              "key": "61b6eb96-8f80-40ee-b3ad-e180f25d53d8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Review",
              "instructor_notes": ""
            },
            {
              "id": 671503,
              "key": "7c33b1c9-7996-49a1-9419-64420d638ce8",
              "title": "L601 Intro RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "3H5x0lstvmo",
                "china_cdn_id": "3H5x0lstvmo.mp4"
              }
            },
            {
              "id": 617120,
              "key": "1de35c2a-4f1a-4e9c-8c52-2081e260df90",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Review Your Notes\n---\nIn the lesson **The RL Framework: The Problem**, you learned how to take a real-world problem and specify it in the language of reinforcement learning.  In order to rigorously define a reinforcement learning task, we generally use a **Markov Decision Process (MDP)** to model the environment.  The MDP specifies the rules that the environment uses to respond to the agent's actions, including how much reward to give to the agent in response to its behavior.  The agent's goal is to learn how to play by the rules of the environment, in order to maximize reward.",
              "instructor_notes": ""
            },
            {
              "id": 617209,
              "key": "87964483-55e5-4ad9-b839-08ccb0e9d651",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/September/59c29f47_screen-shot-2017-09-20-at-12.02.06-pm/screen-shot-2017-09-20-at-12.02.06-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/87964483-55e5-4ad9-b839-08ccb0e9d651",
              "caption": "Agent-Environment Interaction",
              "alt": "",
              "width": 369,
              "height": 137,
              "instructor_notes": null
            },
            {
              "id": 617210,
              "key": "4b6cdb9c-f412-49de-8905-55118dbde0dc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Next, in the lesson **The RL Framework: The Solution**, you learned how to specify a solution to the reinforcement learning problem.  In particular, the **optimal policy** <span class=\"mathquill\">\\pi_*</span> specifies - for each environment state - how the agent should select an action towards its goal of maximizing reward.  You learned that the agent could structure its search for an optimal policy by first estimating the **optimal action-value function** <span class=\"mathquill\">q_*</span>; then, once <span class=\"mathquill\">q_*</span> is known, <span class=\"mathquill\">\\pi_*</span> is quickly obtained. ",
              "instructor_notes": ""
            },
            {
              "id": 617211,
              "key": "45b1fc87-760b-4531-874d-83189d6a8aa6",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/September/59c930a9_screen-shot-2017-09-25-at-11.35.38-am/screen-shot-2017-09-25-at-11.35.38-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/45b1fc87-760b-4531-874d-83189d6a8aa6",
              "caption": "Value Function",
              "alt": "Value Function",
              "width": 434,
              "height": 235,
              "instructor_notes": null
            },
            {
              "id": 617401,
              "key": "fdabe460-8b4d-4a60-8d07-32c3789e44be",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Before continuing with this lesson, please take the time to review your notes, to ensure that the terminology from the previous two lessons is familiar to you.  In particular, you should peruse the summary page at the end of the lesson **The RL Framework: The Problem**, and the page at the end of **The RL Framework: The Solution** to ensure that the listed concepts are familiar.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617229,
          "key": "e6bedcd4-d515-4db8-b54a-6bbb4c2d1add",
          "title": "Gridworld Example",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e6bedcd4-d515-4db8-b54a-6bbb4c2d1add",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617446,
              "key": "1bf396e1-d663-45cf-8617-dec6ecd20866",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Gridworld Example",
              "instructor_notes": ""
            },
            {
              "id": 671505,
              "key": "26af6fb4-4da1-4610-80f3-67b381f4eae8",
              "title": "L602 Gridworld Example RENDER V2-2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Lwibg_IfmrA",
                "china_cdn_id": "Lwibg_IfmrA.mp4"
              }
            },
            {
              "id": 617450,
              "key": "c36a2922-1f80-4660-aff2-efde253519a0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---\nTo check your understanding of the environment, please answer the questions below. \n",
              "instructor_notes": ""
            },
            {
              "id": 617444,
              "key": "a291c200-4097-4ac0-965b-b8d040cc8858",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "a291c200-4097-4ac0-965b-b8d040cc8858",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What is the size of the set <span class=\"mathquill\">\\mathcal{S}^+</span> of all states (including terminal states)?",
                "answers": [
                  {
                    "id": "a1524891796876",
                    "text": "2",
                    "is_correct": false
                  },
                  {
                    "id": "a1524891805778",
                    "text": "3",
                    "is_correct": false
                  },
                  {
                    "id": "a1524891807803",
                    "text": "4",
                    "is_correct": true
                  },
                  {
                    "id": "a1524891809273",
                    "text": "5",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 617445,
              "key": "c2fa00ec-855e-4445-a24f-0ba172447e14",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "c2fa00ec-855e-4445-a24f-0ba172447e14",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What is the size of the set <span class=\"mathquill\">\\mathcal{A}</span> of all actions?",
                "answers": [
                  {
                    "id": "a1524891833719",
                    "text": "2",
                    "is_correct": false
                  },
                  {
                    "id": "a1524891843816",
                    "text": "3",
                    "is_correct": false
                  },
                  {
                    "id": "a1524891845249",
                    "text": "4",
                    "is_correct": true
                  },
                  {
                    "id": "a1524891846367",
                    "text": "5",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 617448,
              "key": "e60b35cd-1997-426d-83ca-6e4c1132db5d",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "e60b35cd-1997-426d-83ca-6e4c1132db5d",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "In the **Gridworld Example** video, it was mentioned that the discount rate <span class=\"mathquill\">\\gamma = 1</span>.  With this in mind, which of the following must be true?",
                "answers": [
                  {
                    "id": "a1524892331487",
                    "text": "The agent only cares about the most immediate reward.",
                    "is_correct": false
                  },
                  {
                    "id": "a1524892441099",
                    "text": "The reward is not discounted.",
                    "is_correct": true
                  },
                  {
                    "id": "a1525191358864",
                    "text": "The agent cares more about future reward than present reward.",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 617449,
              "key": "078b334e-177f-4612-9d35-c5b4d2d21438",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "078b334e-177f-4612-9d35-c5b4d2d21438",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Suppose that at some time step <span class=\"mathquill\">t</span>, the agent is in **state 2** and selects **action \"up\"**.  Which of the following is a possible **reward** and **next state** that the agent could receive at time step <span class=\"mathquill\">t+1</span>?  (_Select all that apply._)",
                "answers": [
                  {
                    "id": "a1524892545319",
                    "text": "reward: -1 | next state: 2",
                    "is_correct": true
                  },
                  {
                    "id": "a1525191566013",
                    "text": "reward: 10 | next state: 4",
                    "is_correct": false
                  },
                  {
                    "id": "a1525191578894",
                    "text": "reward: -1 | next state: 1",
                    "is_correct": true
                  },
                  {
                    "id": "a1525191590479",
                    "text": "reward: -1 | next state: 3",
                    "is_correct": true
                  },
                  {
                    "id": "a1525191596409",
                    "text": "reward: 10 | next state: 2",
                    "is_correct": false
                  },
                  {
                    "id": "a1525191617690",
                    "text": "reward: -1 | next state: 4",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 617550,
              "key": "661ef923-b62e-49e6-be8c-ed32f7e7b9a6",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "661ef923-b62e-49e6-be8c-ed32f7e7b9a6",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following choices describes the optimal policy <span class=\"mathquill\">\\pi_*</span>?",
                "answers": [
                  {
                    "id": "a1525036170537",
                    "text": "For each state, the agent should randomly select an action, where each action is selected with probability 1/4.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525191800731",
                    "text": "The agent should select \"up\" in state 1, \"right\" in state 2, and \"down\" in state 3.",
                    "is_correct": true
                  },
                  {
                    "id": "a1525191833497",
                    "text": "The agent should select \"down\" in state 1, \"up\" in state 2, and \"right\" in state 3.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525191845229",
                    "text": "The agent should select \"right\" in state 1, \"left\" in state 2, and \"up\" in state 3.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 617230,
          "key": "a5decd60-2eef-485e-9df7-3c19eee48dca",
          "title": "Monte Carlo Methods",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a5decd60-2eef-485e-9df7-3c19eee48dca",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624037,
              "key": "445bbd69-2f20-4193-8cd4-05773d319b36",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Monte Carlo Methods",
              "instructor_notes": ""
            },
            {
              "id": 671508,
              "key": "b8febf41-d50c-4293-a14f-a7593ef7ecee",
              "title": "L603 Monte Carlo Methods RENDER V3-2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "titaMCRl224",
                "china_cdn_id": "titaMCRl224.mp4"
              }
            },
            {
              "id": 624040,
              "key": "8577b46e-0c75-45db-ab5a-692053ad94cf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---\nTo check your understanding of the video, please answer the question below. \n",
              "instructor_notes": ""
            },
            {
              "id": 624034,
              "key": "2f1885cd-6cfc-463e-b543-1b68352f9213",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "2f1885cd-6cfc-463e-b543-1b68352f9213",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following describes the Monte Carlo approach discussed in the video?",
                "answers": [
                  {
                    "id": "a1525454071017",
                    "text": "After each time step, the agent will select a different action.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525454081789",
                    "text": "For the first episode, the agent selects the first action at every time step.  For the second episode, it selects a different action, and so on.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525454087423",
                    "text": "When the agent has a policy in mind, it follows the policy to collect a lot of episodes.  Then, for each state, to figure out which action is best, the agent can look for which action tended to result in the most cumulative reward.",
                    "is_correct": true
                  },
                  {
                    "id": "a1525454120791",
                    "text": "When the agent has a policy in mind, it follows the policy to collect a single episode.  The agent uses the episode to tell if the policy is good or bad by looking at the cumulative reward that was received by the agent.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 617231,
          "key": "2b520972-f9a8-4e55-b4b3-782040cc7bd5",
          "title": "MC Prediction - Part 1",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2b520972-f9a8-4e55-b4b3-782040cc7bd5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624038,
              "key": "540b7927-41cb-40a8-8693-7829ef36b70a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# MC Prediction",
              "instructor_notes": ""
            },
            {
              "id": 671509,
              "key": "e2971d1e-314a-4fb3-9c9c-ae1cf969c897",
              "title": "L604 MC Prediction Part 1RENDER V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "6ts9gdIS6vg",
                "china_cdn_id": "6ts9gdIS6vg.mp4"
              }
            },
            {
              "id": 624041,
              "key": "c59f0d73-f0cd-490d-9c88-b8f7831f447f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Important Note\n---\nIn this video, we demonstrated a toy example where the agent collected two episodes, consolidated the information in a table, and then used the table to come up with a better policy.  However, as discussed in the previous video, in real-world settings (_and even for the toy example depicted here!_), the agent will want to collect many more episodes, so that it can better trust the information stored in the table.  In this video, we use two episodes only to simplify the example.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617441,
          "key": "fdbb1bc2-c459-480c-b572-ec49599b96cb",
          "title": "MC Prediction - Part 2",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "fdbb1bc2-c459-480c-b572-ec49599b96cb",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624042,
              "key": "c9b507a4-af04-48b0-b909-fd63e48fcf0d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# MC Prediction",
              "instructor_notes": ""
            },
            {
              "id": 671510,
              "key": "e63d1414-a672-4325-8e4f-e78d8d3b4cd0",
              "title": "L605 MC Prediction Part 2 RENDER V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "jR49ZyKuJ98",
                "china_cdn_id": "jR49ZyKuJ98.mp4"
              }
            },
            {
              "id": 624047,
              "key": "5d5e95f9-0b8e-4033-a044-671a8abe13c7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---\nTo check your understanding of the video, please answer the question below. \n",
              "instructor_notes": ""
            },
            {
              "id": 624045,
              "key": "204f42a2-c299-498c-b44b-b7ea9c571d95",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "204f42a2-c299-498c-b44b-b7ea9c571d95",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following is true? (_Select all that apply._)",
                "answers": [
                  {
                    "id": "a1525455901877",
                    "text": "If the agent follows a policy for many episodes, we can use the results to directly estimate the action-value function corresponding to the same policy.",
                    "is_correct": true
                  },
                  {
                    "id": "a1525455993786",
                    "text": "If the agent knows the equiprobable random policy, we can use it to directly estimate the optimal policy.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525456122278",
                    "text": "The Q-table is used to estimate the action-value function.",
                    "is_correct": true
                  },
                  {
                    "id": "a1525456182171",
                    "text": "The action-value function is used to estimate the Q-table.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 617544,
          "key": "814122da-19f0-45ec-b702-8f65fdb9f5d5",
          "title": "MC Prediction - Part 3",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "814122da-19f0-45ec-b702-8f65fdb9f5d5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624048,
              "key": "64f82bec-7dbb-4ccd-ad3f-c5df4485eb58",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# MC Prediction",
              "instructor_notes": ""
            },
            {
              "id": 619636,
              "key": "e66ad6ce-8389-4308-aa80-fc7e8248802e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/April/5ae7362d_screen-shot-2018-04-30-at-10.27.56-am/screen-shot-2018-04-30-at-10.27.56-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e66ad6ce-8389-4308-aa80-fc7e8248802e",
              "caption": "",
              "alt": "Coin flip ",
              "width": 145,
              "height": 139,
              "instructor_notes": null
            },
            {
              "id": 617548,
              "key": "5c7ccfb1-1f90-42c7-b889-6e5856422529",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So far in this lesson, we have discussed how the agent can take a bad policy, like the equiprobable random policy, use it to collect some episodes, and then consolidate the results to arrive at a better policy.\n\nIn the video in the previous concept, you saw that estimating the action-value function with a Q-table is an important intermediate step.  We also refer to this as the **prediction problem**.\n\n> **Prediction Problem**: Given a policy, how might the agent estimate the value function for that policy?\n\nWe've been specifically interested in the action-value function, but the **prediction problem** also refers to approaches that can be used to estimate the state-value function.  We refer to Monte Carlo (MC) approaches to the prediction problem as **MC prediction methods**.\n\n## Pseudocode\n---\nAs you have learned in the videos, in the algorithm for MC prediction, we begin by collecting many episodes with the policy.  Then, we note that each entry in the Q-table corresponds to a particular state and action.  To populate an entry, we use the return that followed when the agent was in that state, and chose the action.  In the event that the agent has selected the same action many times from the same state, we need only average the returns.\n\nBefore we dig into the pseudocode, we note that there are two different versions of MC prediction, depending on how you decide to treat the special case where - _in a single episode_ - the same action is selected from the same state many times.  For more information, watch the video below.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 671511,
              "key": "31869f02-cffb-4c65-8417-780179133785",
              "title": "L606 MC Prediction Part 3 RENDERv1 V4",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "9LP6uXdmWxQ",
                "china_cdn_id": "9LP6uXdmWxQ.mp4"
              }
            },
            {
              "id": 628262,
              "key": "544d244c-2483-422b-b790-0ec509985f08",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "As discussed in the video, we define every occurrence of a state in an episode as a **visit** to that state-action pair.  And, in the event that a state-action pair is visited more than once in an episode, we have two options.  \n\n#### Option 1: Every-visit MC Prediction\nAverage the returns following all visits to each state-action pair, in all episodes.  \n\n#### Option 2: First-visit MC Prediction\nFor each episode, we only consider the first visit to the state-action pair.  The pseudocode for this option can be found below.",
              "instructor_notes": ""
            },
            {
              "id": 624137,
              "key": "8a9d67d2-e393-4987-bce5-e1fe1ef90e4f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aecb9fe_screen-shot-2018-05-04-at-2.51.59-pm/screen-shot-2018-05-04-at-2.51.59-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8a9d67d2-e393-4987-bce5-e1fe1ef90e4f",
              "caption": "",
              "alt": "",
              "width": 500,
              "height": 1172,
              "instructor_notes": null
            },
            {
              "id": 624055,
              "key": "dec96984-dc6b-41c5-812a-7e06ca2f6cca",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Don't let this pseudocode scare you!  The main idea is quite simple.  There are three relevant tables:\n- <span class=\"mathquill\">Q</span> - Q-table, with a row for each state and a column for each action.  The entry corresponding to state <span class=\"mathquill\">s</span> and action <span class=\"mathquill\">a</span> is denoted <span class=\"mathquill\">Q(s,a)</span>.\n- <span class=\"mathquill\">N</span> - table that keeps track of the number of first visits we have made to each state-action pair.\n- <span class=\"mathquill\">returns\\_sum</span> - table that keeps track of the sum of the rewards obtained after first visits to each state-action pair.\n\nIn the algorithm, the number of episodes the agent collects is equal to <span class=\"mathquill\">num\\_episodes</span>.  After each episode, <span class=\"mathquill\">N</span> and <span class=\"mathquill\">returns\\_sum</span> are updated to store the information contained in the episode.  Then, after all of the episodes have been collected and the values in <span class=\"mathquill\">N</span> and <span class=\"mathquill\">returns\\_sum</span> have been finalized, we quickly obtain the final estimate for <span class=\"mathquill\">Q</span>.\n\nSoon, you'll have the chance to implement this algorithm yourself!  \n\nYou will apply your code to OpenAI Gym's BlackJack environment.  Note that in the game of BlackJack, first-visit and every-visit MC return identical results!",
              "instructor_notes": ""
            },
            {
              "id": 619632,
              "key": "e2e55a0a-a9b0-4bb2-90c1-50dcc685edb5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## First-visit or Every-visit?\n---\nBoth the first-visit and every-visit method are **guaranteed to converge** to the true action-value function, as the number of visits to each state-action pair approaches infinity.  (_So, in other words, as long as the agent gets enough experience with each state-action pair, the value function estimate will be pretty close to the true value._)  In the case of first-visit MC, convergence follows from the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), and the details are covered in section 5.1 of the [textbook](http://go.udacity.com/rl-textbook).\n\nIf you are interested in learning more about the difference between first-visit and every-visit MC methods, you are encouraged to read Section 3 of [this paper](http://www-anw.cs.umass.edu/legacy/pubs/1995_96/singh_s_ML96.pdf\n).  The results are summarized in Section 3.6.  The authors show:\n- Every-visit MC is [biased](https://en.wikipedia.org/wiki/Bias_of_an_estimator), whereas first-visit MC is unbiased (see Theorems 6 and 7).\n- Initially, every-visit MC has lower [mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error), but as more episodes are collected, first-visit MC attains better MSE (see Corollary 9a and 10a, and Figure 4).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617223,
          "key": "3998d18c-7d6c-44d1-b8ff-7c37a5c3c6e0",
          "title": "OpenAI Gym: BlackJackEnv",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "3998d18c-7d6c-44d1-b8ff-7c37a5c3c6e0",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617224,
              "key": "3f3493a1-f107-489b-a502-d4d9868c210d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# OpenAI Gym: BlackJackEnv",
              "instructor_notes": ""
            },
            {
              "id": 617225,
              "key": "3ce785c7-fe0d-4a26-a5a2-2de8d2c4a5a5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In order to master the algorithms discussed in this lesson, you will write code to teach an agent to play Blackjack.",
              "instructor_notes": ""
            },
            {
              "id": 617226,
              "key": "b8004c05-c0d8-4845-8070-a121e0c2f99f",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59d245d3_2-card-21/2-card-21.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/b8004c05-c0d8-4845-8070-a121e0c2f99f",
              "caption": "Playing Cards ([Source](https://www.blackjackinfo.com/img/2-card-21.png))",
              "alt": "Playing Cards ([Source](https://www.blackjackinfo.com/img/2-card-21.png))",
              "width": 472,
              "height": 225,
              "instructor_notes": null
            },
            {
              "id": 617227,
              "key": "c54f47aa-92bd-4396-b1e6-4e366c5e72d4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Please read about the game of Blackjack in Example 5.1 of the [textbook](http://go.udacity.com/rl-textbook).\n\nWhen you have finished, please review the corresponding [GitHub file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py), by reading the commented block in the BlackjackEnv class.  (_While you do **not** need to understand how all of the code works, please read the commented block that explains the dynamics of the environment._)  For clarity, we have also pasted the description of the environment below:\n\n```text\n    \"\"\"Simple blackjack environment\n\n    Blackjack is a card game where the goal is to obtain cards that sum to as\n    near as possible to 21 without going over.  They're playing against a fixed\n    dealer.\n    Face cards (Jack, Queen, King) have point value 10.\n    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n    This game is placed with an infinite deck (or with replacement).\n    The game starts with each (player and dealer) having one face up and one\n    face down card.\n\n    The player can request additional cards (hit=1) until they decide to stop\n    (stick=0) or exceed 21 (bust).\n\n    After the player sticks, the dealer reveals their facedown card, and draws\n    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n\n    If neither player nor dealer busts, the outcome (win, lose, draw) is\n    decided by whose sum is closer to 21.  The reward for winning is +1,\n    drawing is 0, and losing is -1.\n\n    The observation of a 3-tuple of: the players current sum,\n    the dealer's one showing card (1-10 where 1 is ace),\n    and whether or not the player holds a usable ace (0 or 1).\n\n    This environment corresponds to the version of the blackjack problem\n    described in Example 5.1 in Reinforcement Learning: An Introduction\n    by Sutton and Barto (1998).\n    http://incompleteideas.net/sutton/book/the-book.html\n    \"\"\"\n```",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617228,
          "key": "231e238e-422e-470f-a3c8-f4fdfb49d4fc",
          "title": "Workspace - Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "231e238e-422e-470f-a3c8-f4fdfb49d4fc",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617266,
              "key": "a5e0a370-cf2d-4050-969f-3d3ed4ee9abf",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Workspace - Introduction",
              "instructor_notes": ""
            },
            {
              "id": 617265,
              "key": "dcd679d4-6998-482f-bc65-7988e8b23506",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "You will write all of your implementations within the classroom, using an interface identical to the one shown below.  Your Workspace contains the following files (among others):\n- **Monte_Carlo.ipynb** - the Jupyter notebook where you will write all of your implementations (_this is the **only** file that you will modify!_)\n- **Monte_Carlo_Solution.ipynb** - the corresponding instructor solutions \n- **plot_utils.py** - contains a plotting function for visualizing state-value functions and policies\n\nThe **Monte_Carlo.ipynb** notebook can be found below.  To peruse the other files, you need only click on \"jupyter\" in the top left corner to return to the Notebook dashboard.",
              "instructor_notes": ""
            },
            {
              "id": 620212,
              "key": "ace5434c-aa81-4d20-aefd-37384edfe5e3",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5ae93acb_screen-shot-2018-05-01-at-11.12.36-pm/screen-shot-2018-05-01-at-11.12.36-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/ace5434c-aa81-4d20-aefd-37384edfe5e3",
              "caption": "To view the solution notebook, click on \"jupyter\" in the top left corner.",
              "alt": "To view the solution notebook, click on \"jupyter\" in the top left corner.",
              "width": 516,
              "height": 81,
              "instructor_notes": null
            },
            {
              "id": 620052,
              "key": "93a8a952-6c8f-47a6-9bb6-c9128bc1e934",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## The Workspace\n--- \nPlease do not write or execute any code just yet - for now, you're encouraged to simply explore the file structure.  In particular, make sure you know where to find the solution notebook. We'll get started with coding within the Workspace soon!",
              "instructor_notes": ""
            },
            {
              "id": 671975,
              "key": "3b99d1af-d849-4945-b52f-2a8696eacdae",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewB1eSKL9pGX",
              "pool_id": "jupyter",
              "view_id": "jupyter-r1GCDqafm",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Monte_Carlo.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 617232,
          "key": "9005472a-a810-473b-ba66-c51badfa92ca",
          "title": "Coding Exercise",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9005472a-a810-473b-ba66-c51badfa92ca",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617285,
              "key": "05e46107-b481-4877-b614-a4ae10351e84",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/September/59c73b1b_jupyter/jupyter.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/05e46107-b481-4877-b614-a4ae10351e84",
              "caption": "",
              "alt": "",
              "width": 100,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 617248,
              "key": "f1eb57bb-e1cb-41f4-8247-bab08fa73b9a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Coding Exercise",
              "instructor_notes": ""
            },
            {
              "id": 617244,
              "key": "5ed5dc48-dbde-4d42-812e-408ba24b775f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Please use the next concept to complete the following sections of `Monte_Carlo.ipynb`:\n- Part 0: Explore BlackjackEnv\n- Part 1: MC Prediction\n\nTo reference the pseudocode while working on the notebook, you are encouraged to look at [this sheet](https://github.com/udacity/deep-reinforcement-learning/blob/master/cheatsheet/cheatsheet.pdf).\n\n## Important Note\n---\nPlease do **_not_** complete the entire notebook in the next concept - you should only complete **Part 0** and **Part 1**.  The final part of the notebook will be addressed later in the lesson. \n\n## Download the Exercise\n---\nIf you would prefer to work on your own machine, you can download the exercise from the [DRLND GitHub repository](https://github.com/udacity/deep-reinforcement-learning).\n\n## Check Your Implementation\n---\nOnce you have completed the exercise, you can check your solution by looking at the corresponding sections in `Monte_Carlo_Solution.ipynb`.  Watch the video below to see a solution walkthrough!  \n\nNote that the Jupyter interface will look slightly different, since at one point we experimented with [JupyterLab](http://jupyterlab.readthedocs.io/en/stable/).  However, all of the Python code is the same as you see in the videos!",
              "instructor_notes": ""
            },
            {
              "id": 666064,
              "key": "3d811788-a8ad-4272-bbbe-bb81d7b0b2ab",
              "title": "MC Prediction - Solution Walkthrough",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Pwiqk7Pncgc",
                "china_cdn_id": "Pwiqk7Pncgc.mp4"
              }
            }
          ]
        },
        {
          "id": 671971,
          "key": "c18ebeaf-68cb-4636-b527-7eb0d7fc7efe",
          "title": "Workspace",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c18ebeaf-68cb-4636-b527-7eb0d7fc7efe",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 671972,
              "key": "39d83c75-d78e-4075-8674-8a3a4924b553",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewB1eSKL9pGX",
              "pool_id": "jupyter",
              "view_id": "jupyter-SyBYL9TM7",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Monte_Carlo.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 617234,
          "key": "4fac5c4f-943c-4a31-a72f-258cb6c84a03",
          "title": "Greedy Policies",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "4fac5c4f-943c-4a31-a72f-258cb6c84a03",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 624091,
              "key": "cd07c614-e6cf-4929-a9aa-05afb51debc3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Greedy Policies",
              "instructor_notes": ""
            },
            {
              "id": 671512,
              "key": "f3424afa-e4f3-463c-b8c0-59b2b8cafb01",
              "title": "L611 Greedy Policies RENDER V4",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "DH6c-aODMLU",
                "china_cdn_id": "DH6c-aODMLU.mp4"
              }
            },
            {
              "id": 624095,
              "key": "677781ff-0f98-40b4-873f-f20315020247",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---\nAn example of a Q-table is provided below.",
              "instructor_notes": ""
            },
            {
              "id": 624094,
              "key": "744fd344-64d6-4636-a772-221d2633eabb",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aecac13_latex-image-1-copy/latex-image-1-copy.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/744fd344-64d6-4636-a772-221d2633eabb",
              "caption": "",
              "alt": "",
              "width": 300,
              "height": 283,
              "instructor_notes": null
            },
            {
              "id": 624102,
              "key": "56f52664-0980-4263-8b19-eb68dbb87085",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "56f52664-0980-4263-8b19-eb68dbb87085",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following describes the policy that is greedy with respect to the Q-table?",
                "answers": [
                  {
                    "id": "a1525460151884",
                    "text": "In state 1, select action 1.  In state 2, select action 1.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525460182043",
                    "text": "In state 1, select action 1.  In state 2, select action 2.",
                    "is_correct": false
                  },
                  {
                    "id": "a1525460192795",
                    "text": "In state 1, select action 2.  In state 2, select action 1.",
                    "is_correct": true
                  },
                  {
                    "id": "a1525460229060",
                    "text": "In state 1, select action 2.  In state 2, select action 2.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 628271,
          "key": "022c7821-5814-4ab1-bfd5-193025abc1bb",
          "title": "Epsilon-Greedy Policies",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "022c7821-5814-4ab1-bfd5-193025abc1bb",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 671520,
              "key": "f2068901-38ee-446e-b1f9-89a5a98b6fcb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Epsilon-Greedy Policies",
              "instructor_notes": ""
            },
            {
              "id": 671514,
              "key": "e2120c3f-7148-4103-85bd-923f2fdc2aa2",
              "title": "L612 Epsilon Greedy Policies RENDER V4",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "PxJMtlR06MY",
                "china_cdn_id": "PxJMtlR06MY.mp4"
              }
            },
            {
              "id": 628275,
              "key": "ddbe1b1c-5a25-4c94-b29d-0a5ebd506bed",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Note\n---\nIn the video above, you learned about <span class=\"mathquill\">\\epsilon</span>-greedy policies.  \n\nYou can think of the agent who follows an <span class=\"mathquill\">\\epsilon</span>-greedy policy as always having a (potentially unfair) coin at its disposal, with probability <span class=\"mathquill\">\\epsilon</span> of landing heads.  After observing a state, the agent flips the coin.\n- If the coin lands tails (so, with probability <span class=\"mathquill\">1-\\epsilon</span>), the agent selects the greedy action.\n- If the coin lands heads (so, with probability <span class=\"mathquill\">\\epsilon</span>), the agent selects an action _uniformly_ at random from the set of available (non-greedy **AND** greedy) actions.\n\nIn order to construct a policy <span class=\"mathquill\">\\pi</span> that is <span class=\"mathquill\">\\epsilon</span>-greedy with respect to the current action-value function estimate <span class=\"mathquill\">Q</span>, we will set\n\n<div class=\"mathquill\">\n\\pi(a|s) \\longleftarrow \n\\begin{cases}\n\\displaystyle 1-\\epsilon +\\epsilon/|\\mathcal{A}(s)|& \\textrm{if }a\\textrm{ maximizes }Q(s,a)\\\\\n\\displaystyle \\epsilon/|\\mathcal{A}(s)| & \\textrm{else}\n\\end{cases}\n</div>",
              "instructor_notes": ""
            },
            {
              "id": 671522,
              "key": "2d1fadda-13bf-436e-9487-a20141527a45",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "for each <span class=\"mathquill\">s\\in\\mathcal{S}</span> and <span class=\"mathquill\">a\\in\\mathcal{A}(s)</span>. \n\nMathematically, <span class=\"mathquill\">\\mathcal{A}(s)</span> is the set of all possible actions at state <span class=\"mathquill\">s</span> (which may be 'up', 'down','right', 'left' for example), and <span class=\"mathquill\">|\\mathcal{A}(s)|</span> the number of possible actions (including the optimal one!). The reason why we include an extra term <span class=\"mathquill\">\\epsilon/|\\mathcal{A}(s)|</span> for the optimal action is because the sum of all the probabilities needs to be 1. If we sum over the probabilities of performing non-optimal actions, we will get <span class=\"mathquill\">(|\\mathcal{A}(s)|-1)\\times \\epsilon/|\\mathcal{A}(s)| </span>, and adding this to <span class=\"mathquill\">1-\\epsilon + \\epsilon/|\\mathcal{A}(s)|</span> gives one.\n\n\nNote that <span class=\"mathquill\">\\epsilon</span> must always be a value between 0 and 1, inclusive (that is, <span class=\"mathquill\">\\epsilon \\in [0,1]</span>).\n\nIn this quiz, you will answer a few questions to test your intuition.",
              "instructor_notes": ""
            },
            {
              "id": 671524,
              "key": "47ca847a-efc1-45a1-90a9-254f1b6a5051",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 671525,
              "key": "ed04da17-3d31-455b-845e-d9e70644cf74",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "ed04da17-3d31-455b-845e-d9e70644cf74",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the values for epsilon yields an epsilon-greedy policy that is guaranteed to **always** select the greedy action?  Select all that apply.",
                "answers": [
                  {
                    "id": "a1530817681420",
                    "text": "(1) epsilon = 0",
                    "is_correct": true
                  },
                  {
                    "id": "a1530817688309",
                    "text": "(2) epsilon = 0.3",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817692958",
                    "text": "(3) epsilon = 0.5",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817697000",
                    "text": "(4) epsilon = 1",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817706040",
                    "text": "(5) This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement.",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 671527,
              "key": "8bbe8c44-4f7c-4f2f-a989-8b70b5402344",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "8bbe8c44-4f7c-4f2f-a989-8b70b5402344",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the values for epsilon yields an epsilon-greedy policy that is guaranteed to **always** select a non-greedy action?  Select all that apply.",
                "answers": [
                  {
                    "id": "a1530817723847",
                    "text": "(1) epsilon = 0",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817742057",
                    "text": "(2) epsilon = 0.3",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817742776",
                    "text": "(3) epsilon = 0.5",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817743447",
                    "text": "(4) epsilon = 1",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817744128",
                    "text": "(5) This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement.",
                    "is_correct": true
                  }
                ]
              }
            },
            {
              "id": 671528,
              "key": "74c14946-feac-407d-80e8-8ec558f392ce",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "74c14946-feac-407d-80e8-8ec558f392ce",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the values for epsilon yields an epsilon-greedy policy that is equivalent to the equiprobable random policy (where, from each state, each action is equally likely to be selected)?",
                "answers": [
                  {
                    "id": "a1530817773300",
                    "text": "(1) epsilon = 0",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817783192",
                    "text": "(2) epsilon = 0.3",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817783797",
                    "text": "(3) epsilon = 0.5",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817785200",
                    "text": "(4) epsilon = 1",
                    "is_correct": true
                  },
                  {
                    "id": "a1530817785829",
                    "text": "(5) This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement.",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 671529,
              "key": "ec05f5c9-95e5-4876-b354-3ada989ab09e",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "ec05f5c9-95e5-4876-b354-3ada989ab09e",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the values for epsilon yields an epsilon-greedy policy where the agent has the _possibility_ of selecting a greedy action, but _might_ select a non-greedy action instead?  In other words, how might you guarantee that the agent selects each of the available (greedy and non-greedy) actions with nonzero probability?",
                "answers": [
                  {
                    "id": "a1530817825046",
                    "text": "(1) epsilon = 0",
                    "is_correct": false
                  },
                  {
                    "id": "a1530817834645",
                    "text": "(2) epsilon = 0.3",
                    "is_correct": true
                  },
                  {
                    "id": "a1530817835312",
                    "text": "(3) epsilon = 0.5",
                    "is_correct": true
                  },
                  {
                    "id": "a1530817836055",
                    "text": "(4) epsilon = 1",
                    "is_correct": true
                  },
                  {
                    "id": "a1530817836733",
                    "text": "(5) This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 617452,
          "key": "11af8fd2-c626-4d72-813d-45193192b3ab",
          "title": "MC Control",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "11af8fd2-c626-4d72-813d-45193192b3ab",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 620037,
              "key": "a4050138-546f-4f96-b1f0-c1fa947f3c67",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# MC Control",
              "instructor_notes": ""
            },
            {
              "id": 624260,
              "key": "c0967205-215b-4e5b-a4f9-766ad9e35a53",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So far, you have learned how the agent can take a policy <span class=\"mathquill\">\\pi</span>, use it to interact with the environment for many episodes, and then use the results to estimate the action-value function <span class=\"mathquill\">q_\\pi</span> with a Q-table.\n\nThen, once the Q-table closely approximates the action-value function <span class=\"mathquill\">q_\\pi</span>, the agent can construct the policy <span class=\"mathquill\">\\pi'</span> that is <span class=\"mathquill\">\\epsilon</span>-greedy with respect to the Q-table, which will yield a policy that is better than the original policy <span class=\"mathquill\">\\pi</span>.\n\nFurthermore, if the agent alternates between these two steps, with:\n- **Step 1**: using the policy <span class=\"mathquill\">\\pi</span> to construct the Q-table, and\n- **Step 2**: improving the policy by changing it to be <span class=\"mathquill\">\\epsilon</span>-greedy with respect to the Q-table (<span class=\"mathquill\">\\pi' \\leftarrow \\epsilon\\text{-greedy}(Q)</span>, <span class=\"mathquill\">\\pi \\leftarrow  \\pi'</span>),\n\nwe will eventually obtain the optimal policy <span class=\"mathquill\">\\pi_*</span>.  \n\nSince this algorithm is a solution for the **control problem** (defined below), we call it a **Monte Carlo control method**.\n\n> **Control Problem**: Estimate the optimal policy.\n\nIt is common to refer to **Step 1** as **policy evaluation**, since it is used to determine the action-**value** function of the policy.  Likewise, since **Step 2** is used to **improve** the policy, we also refer to it as a **policy improvement** step.  ",
              "instructor_notes": ""
            },
            {
              "id": 624261,
              "key": "e128f937-db93-41dd-8c0d-1235ad115249",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aedf602_screen-shot-2018-05-05-at-1.20.10-pm/screen-shot-2018-05-05-at-1.20.10-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/e128f937-db93-41dd-8c0d-1235ad115249",
              "caption": "MC Control",
              "alt": "MC Control",
              "width": 500,
              "height": 1004,
              "instructor_notes": null
            },
            {
              "id": 624262,
              "key": "5f3f16a6-49e4-4daa-aed3-0952e3663339",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So, using this new terminology, we can summarize what we've learned to say that our **Monte Carlo control method** alternates between **policy evaluation** and **policy improvement** steps to recover the optimal policy <span class=\"mathquill\">\\pi_*</span>.\n\n## The Road Ahead\n---\nYou now have a working algorithm for Monte Carlo control!  So, what's to come?\n- In the next concept (**Exploration vs. Exploitation**), you will learn more about how to set the value of <span class=\"mathquill\">\\epsilon</span> when constructing <span class=\"mathquill\">\\epsilon</span>-greedy policies in the policy improvement step.\n- Then, you will learn about two improvements that you can make to the policy evaluation step in your control algorithm.\n - In the **Incremental Mean** concept, you will learn how to update the policy after every episode (instead of waiting to update the policy until after the values of the Q-table have fully converged from many episodes).\n - In the **Constant-alpha** concept, you will learn how to train the agent to leverage its most recent experience more effectively.\n\nFinally, to conclude the lesson, you will write your own algorithm for Monte Carlo control to solve OpenAI Gym's Blackjack environment, to put your new knowledge to practice!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 418490,
          "key": "30151e2d-760b-455e-9321-c7a8bc69a4c1",
          "title": "Exploration vs. Exploitation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "30151e2d-760b-455e-9321-c7a8bc69a4c1",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 418492,
              "key": "1ea579fb-1699-4427-aff0-857b00f9b05b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Exploration vs. Exploitation",
              "instructor_notes": ""
            },
            {
              "id": 418682,
              "key": "f20a9195-bb91-48f6-9761-fde0af46c370",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59d55ce3_exploration-vs.-exploitation/exploration-vs.-exploitation.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f20a9195-bb91-48f6-9761-fde0af46c370",
              "caption": "Exploration-Exploitation Dilemma ([Source]( http://slides.com/ericmoura/deck-2/embed))",
              "alt": "Exploration-Exploitation Dilemma",
              "width": 600,
              "height": 400,
              "instructor_notes": null
            },
            {
              "id": 418575,
              "key": "54013167-8d4b-49da-92d7-5a062512470b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Solving Environments in OpenAI Gym\n---\nIn many cases, we would like our reinforcement learning (RL) agents to learn to maximize reward as quickly as possible.  This can be seen in many OpenAI Gym environments.  \n\nFor instance, the [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) environment is considered solved once the agent attains an average reward of 0.78 over 100 consecutive trials.",
              "instructor_notes": ""
            },
            {
              "id": 418675,
              "key": "51937238-d240-4720-a729-acf729105707",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59d559bf_screen-shot-2017-10-04-at-4.58.58-pm/screen-shot-2017-10-04-at-4.58.58-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/51937238-d240-4720-a729-acf729105707",
              "caption": "",
              "alt": "",
              "width": 498,
              "height": 373,
              "instructor_notes": null
            },
            {
              "id": 418676,
              "key": "17ecbba4-bee5-4da0-a61d-72bb7bdb0671",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Algorithmic solutions to the [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) environment are ranked according to the number of episodes needed to find the solution.",
              "instructor_notes": ""
            },
            {
              "id": 418677,
              "key": "8018fd20-a14c-45f1-b7ed-5586c0557455",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59d55a58_screen-shot-2017-10-04-at-5.01.26-pm/screen-shot-2017-10-04-at-5.01.26-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/8018fd20-a14c-45f1-b7ed-5586c0557455",
              "caption": "",
              "alt": "",
              "width": 686,
              "height": 224,
              "instructor_notes": null
            },
            {
              "id": 418678,
              "key": "987165b2-9f09-4c7b-b001-75f5179f03b2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Solutions to [Taxi-v1](https://gym.openai.com/envs/Taxi-v1/), [Cartpole-v1](https://gym.openai.com/envs/CartPole-v1/), and [MountainCar-v0](https://gym.openai.com/envs/MountainCar-v0/) (along with many others) are also ranked according to the number of episodes before the solution is found.  Towards this objective, it makes sense to design an algorithm that learns the optimal policy <span class=\"mathquill\">\\pi_*</span> as quickly as possible.  ",
              "instructor_notes": ""
            },
            {
              "id": 418692,
              "key": "f0df4bde-7482-407b-87ae-e6964305ac64",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Exploration-Exploitation Dilemma\n---\nRecall that the environment's dynamics are initially unknown to the agent.  Towards maximizing return, the agent must learn about the environment through interaction.\n\nAt every time step, when the agent selects an action, it bases its decision on past experience with the environment.  And, towards minimizing the number of episodes needed to solve environments in OpenAI Gym, our first instinct could be to devise a strategy where the agent always selects the action that it believes (_based on its past experience_) will maximize return.  With this in mind, the agent could follow the policy that is greedy with respect to the action-value function estimate.  We examined this approach in a previous video and saw that it can easily lead to convergence to a sub-optimal policy.\n\nTo see why this is the case, note that in early episodes, the agent's knowledge is quite limited (and potentially flawed).  So, it is highly likely that actions _estimated_ to be non-greedy by the agent are in fact better than the _estimated_ greedy action.\n\nWith this in mind, a successful RL agent cannot act greedily at every time step (_that is_, it cannot always **exploit** its knowledge); instead, in order to discover the optimal policy, it has to continue to refine the estimated return for all state-action pairs (_in other words_, it has to continue to **explore** the range of possibilities by visiting every state-action pair).  That said, the agent should always act _somewhat greedily_, towards its goal of maximizing return _as quickly as possible_.  This motivated the idea of an <span class=\"mathquill\">\\epsilon</span>-greedy policy.\n\nWe refer to the need to balance these two competing requirements as the **Exploration-Exploitation Dilemma**.  One potential solution to this dilemma is implemented by gradually modifying the value of <span class=\"mathquill\">\\epsilon</span> when constructing <span class=\"mathquill\">\\epsilon</span>-greedy policies.\n\n## Setting the Value of <span class=\"mathquill\">\\epsilon</span>, in Theory\n---\nIt makes sense for the agent to begin its interaction with the environment by favoring **exploration** over **exploitation**.  After all, when the agent knows relatively little about the environment's dynamics, it should distrust its limited knowledge and **explore**, or try out various strategies for maximizing return.  With this in mind, the best starting policy is the equiprobable random policy, as it is equally likely to explore all possible actions from each state.  You discovered in the previous quiz that setting <span class=\"mathquill\">\\epsilon = 1</span> yields an <span class=\"mathquill\">\\epsilon</span>-greedy policy that is equivalent to the equiprobable random policy.\n\nAt later time steps, it makes sense to favor **exploitation** over **exploration**, where the policy gradually becomes more greedy with respect to the action-value function estimate.  After all, the more the agent interacts with the environment, the more it can trust its estimated action-value function.  You discovered in the previous quiz that setting <span class=\"mathquill\">\\epsilon = 0</span> yields the greedy policy (or, the policy that most favors exploitation over exploration). \n\nThankfully, this strategy (of initially favoring exploration over exploitation, and then gradually preferring exploitation over exploration) can be demonstrated to be optimal. \n\n## Greedy in the Limit with Infinite Exploration (GLIE)\n---\nIn order to guarantee that MC control converges to the optimal policy <span class=\"mathquill\">\\pi_*</span>, we need to ensure that two conditions are met.  We refer to these conditions as __Greedy in the Limit with Infinite Exploration (GLIE)__.  In particular, if:\n- every state-action pair <span class=\"mathquill\">s, a</span> (for all <span class=\"mathquill\">s\\in\\mathcal{S}</span> and <span class=\"mathquill\">a\\in\\mathcal{A}(s)</span>) is visited infinitely many times, and \n- the policy converges to a policy that is greedy with respect to the action-value function estimate <span class=\"mathquill\">Q</span>,\n\nthen MC control is guaranteed to converge to the optimal policy (in the limit as the algorithm is run for infinitely many episodes).  These conditions ensure that:\n- the agent continues to explore for all time steps, and\n- the agent gradually exploits more (and explores less).\n\nOne way to satisfy these conditions  is to modify the value of <span class=\"mathquill\">\\epsilon</span> when specifying an <span class=\"mathquill\">\\epsilon</span>-greedy policy.  In particular, let <span class=\"mathquill\">\\epsilon_i</span> correspond to the <span class=\"mathquill\">i</span>-th time step.  Then, both of these conditions are met if:\n-  <span class=\"mathquill\">\\epsilon_i > 0</span> for all time steps <span class=\"mathquill\">i</span>, and \n- <span class=\"mathquill\">\\epsilon_i</span> decays to zero in the limit as the time step <span class=\"mathquill\">i</span> approaches infinity (that is, <span class=\"mathquill\">\\lim_{i\\to\\infty} \\epsilon_i = 0</span>).\n\nFor example, to ensure convergence to the optimal policy, we could set <span class=\"mathquill\">\\epsilon_i = \\frac{1}{i}</span>.  (You are encouraged to verify that <span class=\"mathquill\">\\epsilon_i > 0</span> for all <span class=\"mathquill\">i</span>, and <span class=\"mathquill\">\\lim_{i\\to\\infty} \\epsilon_i = 0</span>.)\n",
              "instructor_notes": ""
            },
            {
              "id": 620040,
              "key": "4bb27907-02d1-4569-a2fb-fc803f13726e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Setting the Value of <span class=\"mathquill\">\\epsilon</span>, in Practice\n---\n\nAs you read in the above section, in order to guarantee convergence, we must let <span class=\"mathquill\">\\epsilon_i</span> decay in accordance with the GLIE conditions.  But sometimes \"guaranteed convergence\" *isn't good enough* in practice, since this really doesn't tell you how long you have to wait!  It is possible that you could need trillions of episodes to recover the optimal policy, for instance, and the \"guaranteed convergence\" would still be accurate! \n\n> Even though convergence is **not** guaranteed by the mathematics, you can often get better results by either:\n-  using fixed <span class=\"mathquill\">\\epsilon</span>, or\n- letting <span class=\"mathquill\">\\epsilon_i</span> decay to a small positive number, like 0.1.  \n\nThis is because one has to be very careful with setting the decay rate for <span class=\"mathquill\">\\epsilon</span>; letting it get too small too fast can be disastrous.  If you get late in training and <span class=\"mathquill\">\\epsilon</span> is really small, you pretty much want the agent to have already converged to the optimal policy, as it will take way too long otherwise for it to test out new actions!\n\nAs a famous example in practice, you can read more about how the value of <span class=\"mathquill\">\\epsilon</span> was set in the famous DQN algorithm by reading the Methods section of [the research paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf): \n> _The behavior policy during training was epsilon-greedy with epsilon annealed linearly from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter._\n\nWhen you implement your own algorithm for MC control later in this lesson, you are strongly encouraged to experiment with setting the value of <span class=\"mathquill\">\\epsilon</span> to build your intuition.  ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617238,
          "key": "da786971-e0bc-4e74-b85e-a47b74b2aaf8",
          "title": "Incremental Mean",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "da786971-e0bc-4e74-b85e-a47b74b2aaf8",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 619645,
              "key": "36a50de6-37fd-4bf5-a28c-5b996914c93a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Incremental Mean",
              "instructor_notes": ""
            },
            {
              "id": 619649,
              "key": "3f111eb5-4709-49c2-8b3e-40627394593c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In our current algorithm for Monte Carlo control, we collect a large number of episodes to build the Q-table (as an estimate for the action-value function corresponding to the agent's current policy).  Then, after the values in the Q-table have converged, we use the table to come up with an improved policy.\n\nMaybe it would be more efficient to update the Q-table **_after every episode_**.  Then, the updated Q-table could be used to improve the policy.  That new policy could then be used to generate the next episode, and so on.  ",
              "instructor_notes": ""
            },
            {
              "id": 626952,
              "key": "9cee5d35-6be6-492a-bd76-2aa8b33c55a8",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5af4d181_screen-shot-2018-05-10-at-6.10.16-pm/screen-shot-2018-05-10-at-6.10.16-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/9cee5d35-6be6-492a-bd76-2aa8b33c55a8",
              "caption": "MC Control with Incremental Mean",
              "alt": "MC Control with Incremental Mean",
              "width": 500,
              "height": 1048,
              "instructor_notes": null
            },
            {
              "id": 626951,
              "key": "86c28483-62ba-4a2a-a910-507d323e90c7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So, _how might we modify our code to accomplish this_?  Watch the video below to see!\n",
              "instructor_notes": ""
            },
            {
              "id": 671515,
              "key": "db5470b6-09e4-42b5-8fb9-40385d56e626",
              "title": "L615 Incremental Mean RENDER V4",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "h-8MB7V1LiE",
                "china_cdn_id": "h-8MB7V1LiE.mp4"
              }
            },
            {
              "id": 624258,
              "key": "32975fe1-d5db-4052-9ead-9b133e32fc3e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this case, even though we're updating the policy before the values in the Q-table accurately approximate the action-value function, this lower-quality estimate nevertheless still has enough information to help us propose successively better policies. If you're curious to learn more, you can read section 5.6 of [the textbook](https://s3-us-west-1.amazonaws.com/udacity-dlnfd/suttonbookdraft2018jan1.pdf).",
              "instructor_notes": ""
            },
            {
              "id": 619644,
              "key": "37784de8-bec6-4b31-975d-4e0d7dc3380f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Pseudocode\n---\nThe pseudocode can be found below.\n",
              "instructor_notes": ""
            },
            {
              "id": 624165,
              "key": "c9a0cff0-ba1a-4be7-8ac4-0e98550eb7ec",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aecc149_screen-shot-2018-05-04-at-3.14.47-pm/screen-shot-2018-05-04-at-3.14.47-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/c9a0cff0-ba1a-4be7-8ac4-0e98550eb7ec",
              "caption": "",
              "alt": "",
              "width": 500,
              "height": 1228,
              "instructor_notes": null
            },
            {
              "id": 624170,
              "key": "161e344e-9485-4a42-96ac-a42224f4da5b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "There are two relevant tables:\n- <span class=\"mathquill\">Q</span> - Q-table, with a row for each state and a column for each action.  The entry corresponding to state <span class=\"mathquill\">s</span> and action <span class=\"mathquill\">a</span> is denoted <span class=\"mathquill\">Q(s,a)</span>.\n- <span class=\"mathquill\">N</span> - table that keeps track of the number of first visits we have made to each state-action pair.\n\nThe number of episodes the agent collects is equal to <span class=\"mathquill\">num\\_episodes</span>.  \n\nThe algorithm proceeds by looping over the following steps:\n- **Step 1**: The policy <span class=\"mathquill\">\\pi</span> is improved to be <span class=\"mathquill\">\\epsilon</span>-greedy with respect to <span class=\"mathquill\">Q</span>, and the agent uses <span class=\"mathquill\">\\pi</span> to collect an episode.\n- **Step 2**: <span class=\"mathquill\">N</span> is updated to count the total number of first visits to each state action pair.  \n- **Step 3**: The estimates in <span class=\"mathquill\">Q</span> are updated to take into account the most recent information.\n\nIn this way, the agent is able to improve the policy after every episode!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617239,
          "key": "639bf9e6-e961-4860-83bc-641a68b7b69d",
          "title": "Constant-alpha",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "639bf9e6-e961-4860-83bc-641a68b7b69d",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 619647,
              "key": "986c2644-be2d-4ab5-bf05-38f25013627f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Constant-alpha",
              "instructor_notes": ""
            },
            {
              "id": 619648,
              "key": "0e71515c-678d-4e41-987f-a589c72d8f5c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the video below, you will learn about another improvement that you can make to your Monte Carlo control algorithm.",
              "instructor_notes": ""
            },
            {
              "id": 671518,
              "key": "448c8c84-5ce5-4aa4-acdb-36e05b76a70b",
              "title": "MC Control: Constant-alpha",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "QFV1nI9Zpoo",
                "china_cdn_id": "QFV1nI9Zpoo.mp4"
              }
            },
            {
              "id": 619650,
              "key": "eeaca5a9-638d-4cf0-b4e9-b321aa62fb2a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Pseudocode\n---\nThe pseudocode for constant-<span class=\"mathquill\">\\alpha</span> GLIE MC Control can be found below.\n\n",
              "instructor_notes": ""
            },
            {
              "id": 624138,
              "key": "67be9f1f-7484-440b-881d-6f1a2c7c9be8",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/May/5aecba4c_screen-shot-2018-05-04-at-2.49.48-pm/screen-shot-2018-05-04-at-2.49.48-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/67be9f1f-7484-440b-881d-6f1a2c7c9be8",
              "caption": "",
              "alt": "",
              "width": 550,
              "height": 1202,
              "instructor_notes": null
            },
            {
              "id": 624140,
              "key": "9cf50efe-8433-4285-9933-869a251001cb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Setting the Value of <span class=\"mathquill\">\\alpha</span>\n---\nRecall the update equation that we use to amend the values in the Q-table:\n<div class=\"mathquill\">Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (G_t - Q(S_t, A_t))</div>\n\nTo examine how to set the the value of <span class=\"mathquill\">\\alpha</span> in more detail, we will slightly rewrite the equation as follows:\n\n<div class=\"mathquill\">Q(S_t,A_t) \\leftarrow (1-\\alpha)Q(S_t,A_t) + \\alpha G_t</div>\n\nWatch the video below to hear more about how to set the value of <span class=\"mathquill\">\\alpha</span>.",
              "instructor_notes": ""
            },
            {
              "id": 671517,
              "key": "5b7a2760-9615-475e-b793-d8aefc7711a0",
              "title": "L617 Constant Alpha Edits RENDER V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "LetHoOtNdJc",
                "china_cdn_id": "LetHoOtNdJc.mp4"
              }
            },
            {
              "id": 631264,
              "key": "fca1c201-6b1d-4a60-bf5d-2e719dce7a58",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here are some guiding principles that will help you to set the value of <span class=\"mathquill\">\\alpha</span> when implementing constant-<span class=\"mathquill\">\\alpha</span> MC control:\n\n- You should always set the value for <span class=\"mathquill\">\\alpha</span> to a number greater than zero and less than (or equal to) one.  \n - If <span class=\"mathquill\">\\alpha=0</span>, then the action-value function estimate is never updated by the agent.\n - If <span class=\"mathquill\">\\alpha = 1</span>, then the final value estimate for each state-action pair is always equal to the last return that was experienced by the agent (after visiting the pair).\n\n- Smaller values for <span class=\"mathquill\">\\alpha</span> encourage the agent to consider a longer history of returns when calculating the action-value function estimate.  Increasing the value of <span class=\"mathquill\">\\alpha</span> ensures that the agent focuses more on the most recently sampled returns.\n\n> **Important Note**: When implementing constant-<span class=\"mathquill\">\\alpha</span> MC control, you must be careful to not set the value of <span class=\"mathquill\">\\alpha</span> too close to 1.  This is because very large values can keep the algorithm from converging to the optimal policy <span class=\"mathquill\">\\pi_*</span>.  However, you must also be careful to not set the value of <span class=\"mathquill\">\\alpha</span> too low, as this can result in an agent who learns too slowly.  The best value of <span class=\"mathquill\">\\alpha</span> for your implementation will greatly depend on your environment and is best gauged through trial-and-error.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 617240,
          "key": "a8becf1a-9618-47a1-91e2-534395c333ed",
          "title": "Coding Exercise",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a8becf1a-9618-47a1-91e2-534395c333ed",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617286,
              "key": "351ae9e4-9839-455b-b541-21df54c33313",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/September/59c73b1b_jupyter/jupyter.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/351ae9e4-9839-455b-b541-21df54c33313",
              "caption": "",
              "alt": "",
              "width": 100,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 617289,
              "key": "e660b5ef-668b-4abf-b6d2-8ccfb5776da2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Coding Exercise",
              "instructor_notes": ""
            },
            {
              "id": 617284,
              "key": "fa6b8eaa-91b9-4368-bb29-e314d55507fa",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Please use the next concept to complete the following section of `Monte_Carlo.ipynb`:\n- Part 2: MC Control\n\nTo reference the pseudocode while working on the notebook, you are encouraged to look at [this sheet](https://github.com/udacity/deep-reinforcement-learning/blob/master/cheatsheet/cheatsheet.pdf).\n\n## Download the Exercise\n---\nIf you would prefer to work on your own machine, you can download the exercise from the [DRLND GitHub repository](https://github.com/udacity/deep-reinforcement-learning).\n\n## Check Your Implementation\n---\nOnce you have completed the exercise, you can check your solution by looking at the corresponding sections in `Monte_Carlo_Solution.ipynb`.  Watch the video below to see a solution walkthrough!",
              "instructor_notes": ""
            },
            {
              "id": 671600,
              "key": "a9ac0cf2-a1d0-4445-8291-c3f284fcd19f",
              "title": "M1 L6 S2 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "6E_3NJcoxmU",
                "china_cdn_id": "6E_3NJcoxmU.mp4"
              }
            }
          ]
        },
        {
          "id": 671973,
          "key": "1e5e4a4c-b67f-4bd8-8e22-4fb8dc300cc5",
          "title": "Workspace",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1e5e4a4c-b67f-4bd8-8e22-4fb8dc300cc5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 671974,
              "key": "c2e6169a-a89f-43d5-92b3-ae67f02414b3",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewB1eSKL9pGX",
              "pool_id": "jupyter",
              "view_id": "jupyter-rkL9wcTMQ",
              "gpu_capable": false,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/Monte_Carlo.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 617242,
          "key": "374c162d-29e0-413d-8578-d4199b5568c9",
          "title": "Summary",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "374c162d-29e0-413d-8578-d4199b5568c9",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 617291,
              "key": "c3435eb6-fc7d-4e48-8ced-8de1e0d116a7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Summary",
              "instructor_notes": ""
            },
            {
              "id": 617443,
              "key": "0a34fe3f-ed40-4e55-8e70-1b8bf2cb3290",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/October/59d69c65_screen-shot-2017-10-05-at-3.55.40-pm/screen-shot-2017-10-05-at-3.55.40-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/0a34fe3f-ed40-4e55-8e70-1b8bf2cb3290",
              "caption": "Optimal Policy and State-Value Function in Blackjack (Sutton and Barto, 2017)",
              "alt": "",
              "width": 488,
              "height": 355,
              "instructor_notes": null
            },
            {
              "id": 617294,
              "key": "cca8a2d2-1045-43d2-aac9-b1084838ffa4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Monte Carlo Methods\n---\n- Monte Carlo methods - even though the underlying problem involves a great degree of randomness, we can infer useful information that we can trust just by collecting a lot of samples.\n- The **equiprobable random policy** is the stochastic policy where - from each state - the agent randomly selects from the set of available actions, and each action is selected with equal probability. \n\n### MC Prediction\n---\n- Algorithms that solve the **prediction problem** determine the value function <span class=\"mathquill\">v_\\pi</span> (or <span class=\"mathquill\">q_\\pi</span>) corresponding to a policy <span class=\"mathquill\">\\pi</span>.\n- When working with finite MDPs, we can estimate the action-value function <span class=\"mathquill\">q_\\pi</span> corresponding to a policy <span class=\"mathquill\">\\pi</span> in a table known as a **Q-table**.  This table has one row for each state and one column for each action.  The entry in the <span class=\"mathquill\">s</span>-th row and <span class=\"mathquill\">a</span>-th column contains the agent's estimate for expected return that is likely to follow, if the agent starts in state <span class=\"mathquill\">s</span>, selects action <span class=\"mathquill\">a</span>, and then henceforth follows the policy <span class=\"mathquill\">\\pi</span>.\n- Each occurrence of the state-action pair <span class=\"mathquill\">s,a</span> (<span class=\"mathquill\">s\\in\\mathcal{S},a\\in\\mathcal{A}</span>) in an episode is called a **visit to <span class=\"mathquill\">s,a</span>**.\n- There are two types of MC prediction methods (for estimating <span class=\"mathquill\">q_\\pi</span>):\n - **First-visit MC** estimates <span class=\"mathquill\">q_\\pi(s,a)</span> as the average of the returns following _only first_ visits to <span class=\"mathquill\">s,a</span> (that is, it ignores returns that are associated to later visits).\n - **Every-visit MC** estimates <span class=\"mathquill\">q_\\pi(s,a)</span> as the average of the returns following _all_ visits to <span class=\"mathquill\">s,a</span>.\n\n### Greedy Policies\n---\n- A policy is **greedy** with respect to an action-value function estimate <span class=\"mathquill\">Q</span> if for every state <span class=\"mathquill\">s\\in\\mathcal{S}</span>, it is guaranteed to select an action <span class=\"mathquill\">a\\in\\mathcal{A}(s)</span> such that <span class=\"mathquill\">a = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)</span>.  (It is common to refer to the selected action as the **greedy action**.)\n- In the case of a finite MDP, the action-value function estimate is represented in a Q-table.  Then, to get the greedy action(s), for each row in the table, we need only select the action (or actions) corresponding to the column(s) that maximize the row.\n\n### Epsilon-Greedy Policies\n---\n- A policy is **<span class=\"mathquill\">\\epsilon</span>-greedy** with respect to an action-value function estimate <span class=\"mathquill\">Q</span> if for every state <span class=\"mathquill\">s\\in\\mathcal{S}</span>, \n - with probability <span class=\"mathquill\">1-\\epsilon</span>, the agent selects the greedy action, and\n - with probability <span class=\"mathquill\">\\epsilon</span>, the agent selects an action _uniformly_ at random from the set of available (non-greedy **AND** greedy) actions.\n\n### MC Control\n---\n- Algorithms designed to solve the **control problem** determine the optimal policy <span class=\"mathquill\">\\pi_*</span> from interaction with the environment.\n- The **Monte Carlo control method** uses alternating rounds of policy evaluation and improvement to recover the optimal policy.\n\n### Exploration vs. Exploitation\n---\n- All reinforcement learning agents face the **Exploration-Exploitation Dilemma**, where they must find a way to balance the drive to behave optimally based on their current knowledge (**exploitation**) and the need to acquire knowledge to attain better judgment (**exploration**).\n- In order for MC control to converge to the optimal policy, the **Greedy in the Limit with Infinite Exploration (GLIE)** conditions must be met:\n - every state-action pair <span class=\"mathquill\">s, a</span> (for all <span class=\"mathquill\">s\\in\\mathcal{S}</span> and <span class=\"mathquill\">a\\in\\mathcal{A}(s)</span>) is visited infinitely many times, and \n - the policy converges to a policy that is greedy with respect to the action-value function estimate <span class=\"mathquill\">Q</span>.\n\n### Incremental Mean\n---\n-  (In this concept, we amended the policy evaluation step to update the Q-table after every episode of interaction.)\n\n### Constant-alpha\n---\n- (In this concept, we derived the algorithm for **constant-<span class=\"mathquill\">\\alpha</span> MC control**, which uses a constant step-size parameter <span class=\"mathquill\">\\alpha</span>.)\n- The step-size parameter <span class=\"mathquill\">\\alpha</span> must satisfy <span class=\"mathquill\">0 < \\alpha \\leq 1</span>.  Higher values of <span class=\"mathquill\">\\alpha</span> will result in faster learning, but values of <span class=\"mathquill\">\\alpha</span> that are too high can prevent MC control from converging to <span class=\"mathquill\">\\pi_*</span>.\n\n",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}