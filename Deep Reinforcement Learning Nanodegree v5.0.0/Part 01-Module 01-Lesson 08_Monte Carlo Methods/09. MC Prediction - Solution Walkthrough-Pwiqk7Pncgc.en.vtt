WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.815
Hello. Once you finish writing your implementation,

00:00:03.815 --> 00:00:06.504
you can click on Jupyter on the top left corner.

00:00:06.504 --> 00:00:10.250
That'll bring you to a list of files on the left here.

00:00:10.250 --> 00:00:13.890
You can open Monte_Carlo_Solution.ipython notebook to

00:00:13.890 --> 00:00:17.679
see one way that we solved the exercise.

00:00:17.679 --> 00:00:21.480
So, what I did first was I restarted

00:00:21.480 --> 00:00:25.410
the kernel and cleared off output's just so it's clear exactly what the code is doing.

00:00:25.410 --> 00:00:30.274
We'll begin by importing the necessary packages and starting up the environment.

00:00:30.274 --> 00:00:33.195
Then, we print some information about the environment.

00:00:33.195 --> 00:00:37.049
So, there are 704 different states in

00:00:37.049 --> 00:00:41.869
the environment corresponding to 32 times 11 times two,

00:00:41.869 --> 00:00:47.500
and two possible actions corresponding to selecting to stick or to hit.

00:00:47.500 --> 00:00:54.230
Until we've written this four loop here that allows you to see some sample games where at

00:00:54.229 --> 00:00:57.059
each time step with each episode the agent

00:00:57.060 --> 00:01:01.540
selects either to stick or hit with 50% probability each.

00:01:01.539 --> 00:01:04.954
So, we see in these three sample games.

00:01:04.954 --> 00:01:10.170
In the first game the agent won and then it lost two games after that.

00:01:10.170 --> 00:01:13.635
You can run this for as many times as you want.

00:01:13.635 --> 00:01:17.310
So, in the next three games have lost all of those,

00:01:17.310 --> 00:01:20.280
and so on, where here it won the last game.

00:01:20.280 --> 00:01:23.480
So, our goal in this notebook is to figure out

00:01:23.480 --> 00:01:29.055
a different strategy that makes the agent more likely to win the game more often.

00:01:29.055 --> 00:01:31.335
Okay. So, on to part one.

00:01:31.334 --> 00:01:34.159
In this case, we are considering the policy where the player

00:01:34.159 --> 00:01:36.759
decides an action based only on her current score.

00:01:36.760 --> 00:01:40.460
So, if the sum of the cards we have is 18 or less,

00:01:40.459 --> 00:01:43.129
we figure that probably it's okay if we ask for

00:01:43.129 --> 00:01:46.394
a new card and we do that with 80 percent probability.

00:01:46.394 --> 00:01:49.179
If the sum of the cards is bigger than 18,

00:01:49.180 --> 00:01:51.740
it's probably too dangerous to accept a new cards.

00:01:51.739 --> 00:01:54.059
So, we're most likely to decide to stick.

00:01:54.060 --> 00:01:57.650
In this case, there's a lot of information in the state that we're ignoring,

00:01:57.650 --> 00:01:59.950
like the dealer's face-up card or,

00:01:59.950 --> 00:02:02.540
and whether or not we have a usable ace.

00:02:02.540 --> 00:02:04.980
But it's a good place to start for now.

00:02:05.000 --> 00:02:11.900
So, this generate episode from limits to caustic function simulates that policy,

00:02:11.900 --> 00:02:15.700
and it returns a corresponding episode from start to finish.

00:02:15.699 --> 00:02:19.924
And we can use this code cell to run that function three times.

00:02:19.925 --> 00:02:23.335
We see again, sometimes we win, sometimes we lose.

00:02:23.335 --> 00:02:29.300
The reward is received only at the end of the game and is -1 if we lose,

00:02:29.300 --> 00:02:31.350
and +1 if we win.

00:02:31.349 --> 00:02:36.310
So, here, this episode lasted longer than the next two episodes.

00:02:36.310 --> 00:02:40.349
We see that at the initial time step no reward was received.

00:02:40.349 --> 00:02:42.879
No reward was received again for the second action,

00:02:42.879 --> 00:02:49.639
but then when the game ended we got a reward of +1 because the agent won the game.

00:02:49.639 --> 00:02:54.864
So, then your goal in this notebook was to use this function that simulates

00:02:54.865 --> 00:02:57.534
a very specific policy and estimate

00:02:57.534 --> 00:03:02.495
its corresponding action value function using this mc_prediction_q function.

00:03:02.495 --> 00:03:06.069
So, when you take a look at this mc_prediction_q function,

00:03:06.069 --> 00:03:09.284
you'll notice that we keep track of three separate dictionaries,

00:03:09.284 --> 00:03:12.055
return_sum N and Q.

00:03:12.055 --> 00:03:15.469
To understand the role that these dictionaries play in the algorithm,

00:03:15.469 --> 00:03:17.205
we'll take a look at the pseudocode.

00:03:17.205 --> 00:03:20.830
So, Q will just be exactly what it sounds like.

00:03:20.830 --> 00:03:22.945
It's the Q-table which contains

00:03:22.944 --> 00:03:28.009
the estimates for the actual values corresponding to each state action pair.

00:03:28.009 --> 00:03:33.849
So, remember in Monte Carlo prediction we estimate the action value corresponding to

00:03:33.849 --> 00:03:37.750
each state action pair as just the average of the returns

00:03:37.750 --> 00:03:42.844
obtained or experienced after visiting that state action pair.

00:03:42.844 --> 00:03:46.629
So, one way to get the average is to just sum

00:03:46.629 --> 00:03:50.409
up all the numbers that we want to take the average of and divide by the total number

00:03:50.409 --> 00:03:57.694
of numbers until that total number is contained in this dictionary N. So,

00:03:57.694 --> 00:04:01.789
N of s, a is just the total number of times that we

00:04:01.789 --> 00:04:06.525
visited particular state action pair over all episodes,

00:04:06.525 --> 00:04:10.355
and this return_sum is just the sum of all of the returns

00:04:10.354 --> 00:04:16.004
obtained over all episodes after visiting a particular state action pair.

00:04:16.004 --> 00:04:19.855
So, returning to the mc_prediction_q function,

00:04:19.855 --> 00:04:25.525
we have our three dictionaries return_sum N and Q as we've discussed.Then,

00:04:25.524 --> 00:04:27.614
we just loop over episodes.

00:04:27.615 --> 00:04:31.129
So, we use that provided function to generate

00:04:31.129 --> 00:04:36.199
an episode that's going to be a list of state action reward tuples.

00:04:36.199 --> 00:04:39.439
Until then, we use this zip command to separate

00:04:39.439 --> 00:04:43.324
the states actions and rewards into separate quantities.

00:04:43.324 --> 00:04:45.814
Until I've written some code here

00:04:45.814 --> 00:04:48.779
to press that out a little bit with some print statements,

00:04:48.779 --> 00:04:52.099
here's an episode that lasted three time steps.

00:04:52.100 --> 00:04:54.175
So, here's the initial state,

00:04:54.175 --> 00:04:57.370
the initial action, and then the reward that resulted.

00:04:57.370 --> 00:05:00.040
At the next time step we also got a reward of zero.

00:05:00.040 --> 00:05:01.480
The game ended.

00:05:01.480 --> 00:05:04.809
The agent won, so the agent got a reward of one,

00:05:04.809 --> 00:05:07.880
and this zip command is useful for separating

00:05:07.879 --> 00:05:12.339
the state's actions and rewards from this original list of tuples.

00:05:12.339 --> 00:05:14.039
All right. So, what's left?

00:05:14.040 --> 00:05:16.080
Let's go back to the pseudocode to see.

00:05:16.079 --> 00:05:18.688
So, after generating an episode,

00:05:18.689 --> 00:05:20.855
we loop over time steps,

00:05:20.855 --> 00:05:24.694
look at the corresponding state action pair for each time step.

00:05:24.694 --> 00:05:29.055
If that's the first time that we visited that pair and the episode,

00:05:29.055 --> 00:05:34.300
then we increment N by one and add the return that was

00:05:34.300 --> 00:05:36.384
experienced by the agent from that point

00:05:36.384 --> 00:05:39.899
onward to the corresponding entry and return sum.

00:05:39.899 --> 00:05:41.625
Now,remember in this Blackjack example,

00:05:41.625 --> 00:05:45.595
that first visit and every visit mc_prediction are equivalent.

00:05:45.595 --> 00:05:48.905
We found it easier to implement every visit mc_ prediction.

00:05:48.904 --> 00:05:53.154
So, we'll do this update for every time step in each upset.

00:05:53.154 --> 00:05:59.599
So, then, once we have the corresponding updated values for return_sum and N,

00:05:59.600 --> 00:06:04.855
we can use them to also update our action value estimate or our Q table.

00:06:04.855 --> 00:06:06.205
So, returning to the code,

00:06:06.204 --> 00:06:07.984
that's exactly what we've done here,

00:06:07.985 --> 00:06:11.840
where we precompute these discount factors

00:06:11.839 --> 00:06:15.779
that we'll use to multiply it by the rewards before adding them up.

00:06:15.779 --> 00:06:17.875
So, then I've run that code cell.

00:06:17.875 --> 00:06:19.995
If you scroll down,

00:06:19.995 --> 00:06:25.980
we run that mc_prediction_q function for 500,000 episodes.

00:06:25.980 --> 00:06:31.879
That yields a corresponding Q table or an estimate of the action value function,

00:06:31.879 --> 00:06:36.219
and then we show you how to get the corresponding state value function.

00:06:36.220 --> 00:06:38.745
So, we've done that calculation for you,

00:06:38.745 --> 00:06:41.879
and we'll plot that corresponding state value function.

00:06:41.879 --> 00:06:45.235
Okay. So, once the code has finished running,

00:06:45.235 --> 00:06:48.370
you'll see that there are two plots corresponding to whether

00:06:48.370 --> 00:06:51.754
we have a usable ace or we don't have a usable ace.

00:06:51.754 --> 00:06:53.534
But in either case,

00:06:53.535 --> 00:06:56.410
we see that the highest state values correspond to when

00:06:56.410 --> 00:06:59.555
the player sum is something like 20 or 21,

00:06:59.555 --> 00:07:01.750
because in that case we're most likely to win the game.

00:07:01.750 --> 00:07:04.029
But in any event, I encourage you to take

00:07:04.029 --> 00:07:06.654
the time now to explore this solution in more detail.

00:07:06.654 --> 00:07:10.729
Read out any print statements that you need to tease out particular parts of the code.

00:07:10.730 --> 00:07:14.314
Then in the next part of the lesson and in this notebook,

00:07:14.314 --> 00:07:18.829
you'll learn how to use what you've done here to obtain the optimal policy.

