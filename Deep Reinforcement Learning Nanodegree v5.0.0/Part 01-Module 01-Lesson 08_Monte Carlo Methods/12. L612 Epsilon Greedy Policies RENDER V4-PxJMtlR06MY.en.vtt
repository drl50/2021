WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.139
So, in general, when the agent is interacting with the environment,

00:00:04.139 --> 00:00:06.689
and still trying to figure out what works and what

00:00:06.690 --> 00:00:09.660
doesn't in its quest to collect as much reward as possible,

00:00:09.660 --> 00:00:12.259
creating policies are quite dangerous.

00:00:12.259 --> 00:00:14.734
To see this, let's look at an example.

00:00:14.734 --> 00:00:16.519
Say you're an agent,

00:00:16.519 --> 00:00:18.439
and there are two doors in front of you,

00:00:18.440 --> 00:00:20.575
you need to decide which one has more value.

00:00:20.574 --> 00:00:24.820
At the beginning, you have no reason to favor any door over the other.

00:00:24.820 --> 00:00:29.019
So let's say you initialize your estimate for the value of each door to zero.

00:00:29.019 --> 00:00:31.384
In order to figure out which door to open,

00:00:31.385 --> 00:00:32.615
let's say you flip a coin,

00:00:32.615 --> 00:00:33.880
it comes up tails and so,

00:00:33.880 --> 00:00:35.280
you open Door B.

00:00:35.280 --> 00:00:38.524
When you do that, you receive a reward of zero.

00:00:38.524 --> 00:00:43.250
Let's say for simplicity that an episode finishes after a single door is opened.

00:00:43.250 --> 00:00:46.125
So in other words, after opening Door B,

00:00:46.125 --> 00:00:48.145
you received a return of zero.

00:00:48.145 --> 00:00:51.510
Okay. So, that doesn't change the estimate of the value function.

00:00:51.509 --> 00:00:53.979
So, it makes sense to just pick a door randomly again.

00:00:53.979 --> 00:00:55.359
So you flip a coin,

00:00:55.359 --> 00:00:57.905
it comes up heads this time and so you open Door A.

00:00:57.905 --> 00:01:00.570
When you do this, you get a reward of one.

00:01:00.570 --> 00:01:05.700
This of course updates the estimate for the value of Door A to one.

00:01:05.700 --> 00:01:09.615
So now, if we act greedily with respect to the value function,

00:01:09.614 --> 00:01:11.795
then we open Door A again.

00:01:11.795 --> 00:01:14.105
This time we get a reward of three,

00:01:14.105 --> 00:01:19.055
this updates the value of Door A to two and so at the next point in time,

00:01:19.055 --> 00:01:22.930
the greedy policy says to pick Door A again, and say,

00:01:22.930 --> 00:01:28.240
"Every time we do that we get some positive reward and it's always either one or three."

00:01:28.239 --> 00:01:31.069
So, for all time we're opening the same door.

00:01:31.069 --> 00:01:34.189
There's a big problem with this because we never really got a chance

00:01:34.189 --> 00:01:37.299
to truly explore what's behind the other door.

00:01:37.299 --> 00:01:42.370
For instance, consider the case that the mechanism behind Door A is what you'd expect.

00:01:42.370 --> 00:01:44.685
It yields a reward of one or three,

00:01:44.685 --> 00:01:46.805
where both are equally likely.

00:01:46.805 --> 00:01:49.215
But, the mechanism behind Door B,

00:01:49.215 --> 00:01:51.890
gives a reward of zero or 100.

00:01:51.890 --> 00:01:55.055
Then, that's information that you would have liked to discover,

00:01:55.055 --> 00:01:58.095
but following the greedy policy has prevented you.

00:01:58.094 --> 00:02:02.554
So the point is, that when we got to a situation earlier in our investigation,

00:02:02.555 --> 00:02:05.830
where Door A seemed more favorable than Door B,

00:02:05.829 --> 00:02:09.079
we really needed to spend more time making sure of that,

00:02:09.080 --> 00:02:11.300
because our early perceptions were incorrect.

00:02:11.300 --> 00:02:14.295
So instead of constructing that greedy policy,

00:02:14.294 --> 00:02:16.824
a better policy would be a stochastic one.

00:02:16.824 --> 00:02:19.810
The pick Door A with 95 percent probability

00:02:19.810 --> 00:02:22.830
and Door B with 5 percent probability let's say.

00:02:22.830 --> 00:02:27.405
Then, that's pretty close to the greedy policy so we're still acting pretty optimally,

00:02:27.405 --> 00:02:30.159
but there's the added value that if we continue to

00:02:30.159 --> 00:02:32.854
select Door B with some small probability,

00:02:32.854 --> 00:02:36.759
then at some point we're going to see that return of 100.

00:02:36.759 --> 00:02:41.764
This example motivates how we'll need to modify our current approach.

00:02:41.764 --> 00:02:45.778
Instead of always constructing a greedy policy,

00:02:45.778 --> 00:02:48.750
but always selects the greedy action,

00:02:48.750 --> 00:02:50.280
what we'll do instead,

00:02:50.280 --> 00:02:53.659
is construct a so-called Epsilon-greedy policy,

00:02:53.659 --> 00:02:56.479
that's most likely to pick the greedy action.

00:02:56.479 --> 00:02:59.689
But with some small but nonzero probability,

00:02:59.689 --> 00:03:02.250
picks one of the other actions instead.

00:03:02.250 --> 00:03:04.069
In this case, you will set

00:03:04.069 --> 00:03:08.439
some small positive number Epsilon which must be between zero and one.

00:03:08.439 --> 00:03:11.275
Then, with probability one minus Epsilon,

00:03:11.275 --> 00:03:13.370
the agent selects the greedy action,

00:03:13.370 --> 00:03:15.210
and with probability Epsilon,

00:03:15.210 --> 00:03:17.030
it selects any action randomly.

00:03:17.030 --> 00:03:22.085
So the larger it is the more likely you are to pick one of the non-greedy actions.

00:03:22.085 --> 00:03:25.099
Then, as long as Epsilon is set to a small number,

00:03:25.099 --> 00:03:29.960
we have a method for constructing a policy that's really close to the greedy policy,

00:03:29.960 --> 00:03:32.840
but the added benefit that it doesn't prevent the agent

00:03:32.840 --> 00:03:36.039
from continuing to explore the range of possibilities.

