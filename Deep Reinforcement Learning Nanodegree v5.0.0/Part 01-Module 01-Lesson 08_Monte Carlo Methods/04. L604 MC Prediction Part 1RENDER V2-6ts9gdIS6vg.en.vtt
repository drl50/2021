WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.915
So far we've been working with a simple Grid World example with four states.

00:00:04.915 --> 00:00:10.794
We assume the agent used the equiprobable random policy to interact with the environment.

00:00:10.794 --> 00:00:14.789
The agent collected two episodes and now the question is,

00:00:14.789 --> 00:00:17.099
how exactly should the agent consolidate

00:00:17.100 --> 00:00:21.280
this information towards its goal of obtaining the optimal policy?

00:00:21.280 --> 00:00:24.695
Well let's see. The real question we want to ask is,

00:00:24.695 --> 00:00:26.949
for each date, which action is best?

00:00:26.949 --> 00:00:31.579
To answer that maybe it makes sense to look at each date separately.

00:00:31.579 --> 00:00:34.079
To see this, let's look at state two.

00:00:34.079 --> 00:00:35.780
In the first episode,

00:00:35.780 --> 00:00:38.509
we decided to go left and then the sum of

00:00:38.509 --> 00:00:41.710
the rewards collected from that point onward was eight.

00:00:41.710 --> 00:00:45.924
Then, we decided to go right and got a return of nine.

00:00:45.924 --> 00:00:47.809
Then, in the next episode,

00:00:47.810 --> 00:00:51.945
we also decided to go up and ended up with a return of eight.

00:00:51.945 --> 00:00:57.814
When the agent selected to go right still in the same state it got a return of nine.

00:00:57.814 --> 00:01:03.399
So it looks like since action right gives us more return than up or left,

00:01:03.399 --> 00:01:05.390
it should be better to go right,

00:01:05.390 --> 00:01:09.775
and this is the motivating idea behind the algorithm we'll discuss in this lesson.

00:01:09.775 --> 00:01:12.620
So far, this information was easy to get just

00:01:12.620 --> 00:01:15.155
because the problem is so small, but generally,

00:01:15.155 --> 00:01:19.439
we'll want to work with much bigger tasks with tons of actions and states,

00:01:19.439 --> 00:01:21.614
and so we need to formalize this.

00:01:21.614 --> 00:01:25.009
So what we'll do is keep track of a table with

00:01:25.010 --> 00:01:29.575
one row for each non-terminal state and one column for each action.

00:01:29.575 --> 00:01:32.900
Then, for each episode what we can do is

00:01:32.900 --> 00:01:36.455
store the return we got from selecting each action in each state.

00:01:36.454 --> 00:01:39.924
So for state two when we selected action left,

00:01:39.924 --> 00:01:42.009
we got a return of eight,

00:01:42.010 --> 00:01:46.469
and when we selected action up we also got a return of eight.

00:01:46.469 --> 00:01:49.640
In the event that there are multiple episodes where we selected

00:01:49.640 --> 00:01:53.250
the same action from a state we'll just take the average of them.

00:01:53.250 --> 00:01:55.094
So in this case in state two,

00:01:55.094 --> 00:02:03.015
we got a return of nine and a return of nine and so the average of nine and nine is nine.

00:02:03.015 --> 00:02:05.704
So this row is currently incomplete,

00:02:05.704 --> 00:02:08.479
but as long as we collect more episodes we can

00:02:08.479 --> 00:02:12.324
guarantee that we'll eventually have a value here for action down,

00:02:12.324 --> 00:02:15.824
and we'll fill in the table for the remaining state action pairs.

00:02:15.824 --> 00:02:20.199
Again, if we collect more episodes we can populate the rest and.

00:02:20.199 --> 00:02:23.089
Once we've filled in the table it makes sense that when

00:02:23.090 --> 00:02:27.034
the agent is trying to figure out which actions are the best for each state,

00:02:27.034 --> 00:02:31.270
it can refer to this table and just take the actions that maximize each row.

00:02:31.270 --> 00:02:34.390
So for state one it looks like action up is

00:02:34.389 --> 00:02:38.149
best and in state two we should select action right,

00:02:38.150 --> 00:02:40.844
and in state three action up.

00:02:40.844 --> 00:02:44.555
So all of this yields this proposed policy which we'll denote

00:02:44.555 --> 00:02:48.615
by pi prime to distinguish it from our original policy pi.

00:02:48.615 --> 00:02:50.290
Now, as you can see,

00:02:50.289 --> 00:02:52.729
this is not the optimal policy,

00:02:52.729 --> 00:02:56.899
but what it does give us is a policy that is in some ways better than

00:02:56.900 --> 00:03:01.914
the random policy and the small step with taking the random policy,

00:03:01.914 --> 00:03:04.344
using it to interact with the environment,

00:03:04.344 --> 00:03:09.359
and constructing a better policy will turn out to be quite important.

