<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Summary
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Monte Carlo Methods
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Review.html">
       01. Review
      </a>
     </li>
     <li class="">
      <a href="02. Gridworld Example.html">
       02. Gridworld Example
      </a>
     </li>
     <li class="">
      <a href="03. Monte Carlo Methods.html">
       03. Monte Carlo Methods
      </a>
     </li>
     <li class="">
      <a href="04. MC Prediction - Part 1.html">
       04. MC Prediction - Part 1
      </a>
     </li>
     <li class="">
      <a href="05. MC Prediction - Part 2.html">
       05. MC Prediction - Part 2
      </a>
     </li>
     <li class="">
      <a href="06. MC Prediction - Part 3.html">
       06. MC Prediction - Part 3
      </a>
     </li>
     <li class="">
      <a href="07. OpenAI Gym BlackJackEnv.html">
       07. OpenAI Gym: BlackJackEnv
      </a>
     </li>
     <li class="">
      <a href="08. Workspace - Introduction.html">
       08. Workspace - Introduction
      </a>
     </li>
     <li class="">
      <a href="09. Coding Exercise.html">
       09. Coding Exercise
      </a>
     </li>
     <li class="">
      <a href="10. Workspace.html">
       10. Workspace
      </a>
     </li>
     <li class="">
      <a href="11. Greedy Policies.html">
       11. Greedy Policies
      </a>
     </li>
     <li class="">
      <a href="12. Epsilon-Greedy Policies.html">
       12. Epsilon-Greedy Policies
      </a>
     </li>
     <li class="">
      <a href="13. MC Control.html">
       13. MC Control
      </a>
     </li>
     <li class="">
      <a href="14. Exploration vs. Exploitation.html">
       14. Exploration vs. Exploitation
      </a>
     </li>
     <li class="">
      <a href="15. Incremental Mean.html">
       15. Incremental Mean
      </a>
     </li>
     <li class="">
      <a href="16. Constant-alpha.html">
       16. Constant-alpha
      </a>
     </li>
     <li class="">
      <a href="17. Coding Exercise.html">
       17. Coding Exercise
      </a>
     </li>
     <li class="">
      <a href="18. Workspace.html">
       18. Workspace
      </a>
     </li>
     <li class="">
      <a href="19. Summary.html">
       19. Summary
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          19. Summary
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="summary">
          Summary
         </h1>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Optimal Policy and State-Value Function in Blackjack (Sutton and Barto, 2017)" class="img img-fluid" src="img/screen-shot-2017-10-05-at-3.55.40-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            Optimal Policy and State-Value Function in Blackjack (Sutton and Barto, 2017)
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-monte-carlo-methods">
          ### Monte Carlo Methods
         </h2>
         <ul>
          <li>
           Monte Carlo methods - even though the underlying problem involves a great degree of randomness, we can infer useful information that we can trust just by collecting a lot of samples.
          </li>
          <li>
           The
           <strong>
            equiprobable random policy
           </strong>
           is the stochastic policy where - from each state - the agent randomly selects from the set of available actions, and each action is selected with equal probability.
          </li>
         </ul>
         <h2 id="-mc-prediction">
          ### MC Prediction
         </h2>
         <ul>
          <li>
           Algorithms that solve the
           <strong>
            prediction problem
           </strong>
           determine the value function
           <span class="mathquill ud-math">
            v_\pi
           </span>
           (or
           <span class="mathquill ud-math">
            q_\pi
           </span>
           ) corresponding to a policy
           <span class="mathquill ud-math">
            \pi
           </span>
           .
          </li>
          <li>
           When working with finite MDPs, we can estimate the action-value function
           <span class="mathquill ud-math">
            q_\pi
           </span>
           corresponding to a policy
           <span class="mathquill ud-math">
            \pi
           </span>
           in a table known as a
           <strong>
            Q-table
           </strong>
           .  This table has one row for each state and one column for each action.  The entry in the
           <span class="mathquill ud-math">
            s
           </span>
           -th row and
           <span class="mathquill ud-math">
            a
           </span>
           -th column contains the agent's estimate for expected return that is likely to follow, if the agent starts in state
           <span class="mathquill ud-math">
            s
           </span>
           , selects action
           <span class="mathquill ud-math">
            a
           </span>
           , and then henceforth follows the policy
           <span class="mathquill ud-math">
            \pi
           </span>
           .
          </li>
          <li>
           Each occurrence of the state-action pair
           <span class="mathquill ud-math">
            s,a
           </span>
           (
           <span class="mathquill ud-math">
            s\in\mathcal{S},a\in\mathcal{A}
           </span>
           ) in an episode is called a
           <strong>
            visit to
            <span class="mathquill ud-math">
             s,a
            </span>
           </strong>
           .
          </li>
          <li>
           There are two types of MC prediction methods (for estimating
           <span class="mathquill ud-math">
            q_\pi
           </span>
           ):
           <ul>
            <li>
             <strong>
              First-visit MC
             </strong>
             estimates
             <span class="mathquill ud-math">
              q_\pi(s,a)
             </span>
             as the average of the returns following
             <em>
              only first
             </em>
             visits to
             <span class="mathquill ud-math">
              s,a
             </span>
             (that is, it ignores returns that are associated to later visits).
            </li>
            <li>
             <strong>
              Every-visit MC
             </strong>
             estimates
             <span class="mathquill ud-math">
              q_\pi(s,a)
             </span>
             as the average of the returns following
             <em>
              all
             </em>
             visits to
             <span class="mathquill ud-math">
              s,a
             </span>
             .
            </li>
           </ul>
          </li>
         </ul>
         <h2 id="-greedy-policies">
          ### Greedy Policies
         </h2>
         <ul>
          <li>
           A policy is
           <strong>
            greedy
           </strong>
           with respect to an action-value function estimate
           <span class="mathquill ud-math">
            Q
           </span>
           if for every state
           <span class="mathquill ud-math">
            s\in\mathcal{S}
           </span>
           , it is guaranteed to select an action
           <span class="mathquill ud-math">
            a\in\mathcal{A}(s)
           </span>
           such that
           <span class="mathquill ud-math">
            a = \arg\max_{a\in\mathcal{A}(s)}Q(s,a)
           </span>
           .  (It is common to refer to the selected action as the
           <strong>
            greedy action
           </strong>
           .)
          </li>
          <li>
           In the case of a finite MDP, the action-value function estimate is represented in a Q-table.  Then, to get the greedy action(s), for each row in the table, we need only select the action (or actions) corresponding to the column(s) that maximize the row.
          </li>
         </ul>
         <h2 id="-epsilon-greedy-policies">
          ### Epsilon-Greedy Policies
         </h2>
         <ul>
          <li>
           A policy is
           <strong>
            <span class="mathquill ud-math">
             \epsilon
            </span>
            -greedy
           </strong>
           with respect to an action-value function estimate
           <span class="mathquill ud-math">
            Q
           </span>
           if for every state
           <span class="mathquill ud-math">
            s\in\mathcal{S}
           </span>
           ,
           <ul>
            <li>
             with probability
             <span class="mathquill ud-math">
              1-\epsilon
             </span>
             , the agent selects the greedy action, and
            </li>
            <li>
             with probability
             <span class="mathquill ud-math">
              \epsilon
             </span>
             , the agent selects an action
             <em>
              uniformly
             </em>
             at random from the set of available (non-greedy
             <strong>
              AND
             </strong>
             greedy) actions.
            </li>
           </ul>
          </li>
         </ul>
         <h2 id="-mc-control">
          ### MC Control
         </h2>
         <ul>
          <li>
           Algorithms designed to solve the
           <strong>
            control problem
           </strong>
           determine the optimal policy
           <span class="mathquill ud-math">
            \pi_*
           </span>
           from interaction with the environment.
          </li>
          <li>
           The
           <strong>
            Monte Carlo control method
           </strong>
           uses alternating rounds of policy evaluation and improvement to recover the optimal policy.
          </li>
         </ul>
         <h2 id="-exploration-vs-exploitation">
          ### Exploration vs. Exploitation
         </h2>
         <ul>
          <li>
           All reinforcement learning agents face the
           <strong>
            Exploration-Exploitation Dilemma
           </strong>
           , where they must find a way to balance the drive to behave optimally based on their current knowledge (
           <strong>
            exploitation
           </strong>
           ) and the need to acquire knowledge to attain better judgment (
           <strong>
            exploration
           </strong>
           ).
          </li>
          <li>
           In order for MC control to converge to the optimal policy, the
           <strong>
            Greedy in the Limit with Infinite Exploration (GLIE)
           </strong>
           conditions must be met:
           <ul>
            <li>
             every state-action pair
             <span class="mathquill ud-math">
              s, a
             </span>
             (for all
             <span class="mathquill ud-math">
              s\in\mathcal{S}
             </span>
             and
             <span class="mathquill ud-math">
              a\in\mathcal{A}(s)
             </span>
             ) is visited infinitely many times, and
            </li>
            <li>
             the policy converges to a policy that is greedy with respect to the action-value function estimate
             <span class="mathquill ud-math">
              Q
             </span>
             .
            </li>
           </ul>
          </li>
         </ul>
         <h2 id="-incremental-mean">
          ### Incremental Mean
         </h2>
         <ul>
          <li>
           (In this concept, we amended the policy evaluation step to update the Q-table after every episode of interaction.)
          </li>
         </ul>
         <h2 id="-constant-alpha">
          ### Constant-alpha
         </h2>
         <ul>
          <li>
           (In this concept, we derived the algorithm for
           <strong>
            constant-
            <span class="mathquill ud-math">
             \alpha
            </span>
            MC control
           </strong>
           , which uses a constant step-size parameter
           <span class="mathquill ud-math">
            \alpha
           </span>
           .)
          </li>
          <li>
           The step-size parameter
           <span class="mathquill ud-math">
            \alpha
           </span>
           must satisfy
           <span class="mathquill ud-math">
            0 &lt; \alpha \leq 1
           </span>
           .  Higher values of
           <span class="mathquill ud-math">
            \alpha
           </span>
           will result in faster learning, but values of
           <span class="mathquill ud-math">
            \alpha
           </span>
           that are too high can prevent MC control from converging to
           <span class="mathquill ud-math">
            \pi_*
           </span>
           .
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('19. Summary')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
