WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.504
Let's recall the problem at hand.

00:00:02.504 --> 00:00:04.650
We have an agent and environment.

00:00:04.650 --> 00:00:06.964
Time is broken into discrete time steps,

00:00:06.964 --> 00:00:08.160
and at every time step,

00:00:08.160 --> 00:00:11.089
the agent receives a reward and state from the environment,

00:00:11.089 --> 00:00:14.029
and chooses an action to perform in response.

00:00:14.029 --> 00:00:19.119
In this way, the interaction involved is a sequence of states actions and rewards.

00:00:19.120 --> 00:00:24.170
In this lesson, we will confine our attention to episodic tasks where the interaction

00:00:24.170 --> 00:00:29.400
stops at some time step capital T when the agent encounters a terminal state.

00:00:29.399 --> 00:00:32.714
We refer to the sequence as an episode.

00:00:32.715 --> 00:00:36.000
For any episode, the agent's goal is to find

00:00:36.000 --> 00:00:40.840
the optimal policy in order to maximize expected cumulative reward.

00:00:40.840 --> 00:00:46.375
As we've seen, the agent can only accomplish this by interacting with the environment.

00:00:46.375 --> 00:00:50.869
In this lesson, we'll dig deeply into a class of algorithms that help

00:00:50.869 --> 00:00:56.209
the agent to understand and leverage this interaction to obtain the optimal policy.

