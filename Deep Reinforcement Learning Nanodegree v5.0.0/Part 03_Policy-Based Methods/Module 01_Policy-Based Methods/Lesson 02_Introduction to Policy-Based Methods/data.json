{
  "data": {
    "lesson": {
      "id": 613179,
      "key": "81693145-da28-44cb-aba5-c52b42bc209d",
      "title": "Introduction to Policy-Based Methods",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Policy-based methods try to directly optimize for the optimal policy.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/81693145-da28-44cb-aba5-c52b42bc209d/613179/1544455765357/Introduction+to+Policy-Based+Methods+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/81693145-da28-44cb-aba5-c52b42bc209d/613179/1544455762392/Introduction+to+Policy-Based+Methods+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 613183,
          "key": "2f795032-e92a-4461-9f94-23e0c8f2fcc7",
          "title": "Policy-Based Methods",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2f795032-e92a-4461-9f94-23e0c8f2fcc7",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 667023,
              "key": "5a608dac-aeec-488c-b17a-6bd2f20a8778",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Policy-Based Methods",
              "instructor_notes": ""
            },
            {
              "id": 667218,
              "key": "b0a1fdaf-b431-425e-bbc6-dc44aff07de0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this lesson, you'll learn all about **policy-based methods**.  Watch the video below to learn what policy-based methods are, and how they differ from value-based methods!",
              "instructor_notes": ""
            },
            {
              "id": 690503,
              "key": "7a3a785f-00f4-4235-b77e-c35c777d2170",
              "title": "M3 L2 C01 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "mMnhi8yzwKk",
                "china_cdn_id": "mMnhi8yzwKk.mp4"
              }
            }
          ]
        },
        {
          "id": 672087,
          "key": "10688b97-5de7-4256-9e68-f820c787ec16",
          "title": "Policy Function Approximation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "10688b97-5de7-4256-9e68-f820c787ec16",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 672089,
              "key": "6f6140b0-81d6-4690-9fc5-d3934297f0a2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Policy Function Approximation",
              "instructor_notes": ""
            },
            {
              "id": 672091,
              "key": "cc8b744a-c1ac-4d79-81e8-dfc2116e2cfb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "How might we use a neural network to approximate a policy?  Watch the video below to find out!",
              "instructor_notes": ""
            },
            {
              "id": 672088,
              "key": "ed593469-242e-4148-9b80-e73d45fb599e",
              "title": "M3 L2 C02 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "v8tGjlc2aG4",
                "china_cdn_id": "v8tGjlc2aG4.mp4"
              }
            }
          ]
        },
        {
          "id": 665589,
          "key": "22823289-533f-4313-ac8d-65ce86d9d090",
          "title": "More on the Policy",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "22823289-533f-4313-ac8d-65ce86d9d090",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 667015,
              "key": "3e1f8373-acdb-4acc-a512-245d32b73a09",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# More on the Policy",
              "instructor_notes": ""
            },
            {
              "id": 671377,
              "key": "98ca8b3d-1ed5-46d2-9674-36d14fa1100c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the previous video, you learned how the agent could use a simple neural network architecture to approximate a **stochastic policy**.  The agent passes the current environment state as input to the network, which returns action probabilities.  Then, the agent samples from those probabilities to select an action.",
              "instructor_notes": ""
            },
            {
              "id": 671385,
              "key": "62a10dd4-29b2-48ec-9e92-f7f6695f4f44",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b38f93a_screen-shot-2018-07-01-at-10.54.05-am/screen-shot-2018-07-01-at-10.54.05-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/62a10dd4-29b2-48ec-9e92-f7f6695f4f44",
              "caption": "Neural network that encodes action probabilities ([Source](https://blog.openai.com/evolution-strategies/))",
              "alt": "Neural network that encodes action probabilities",
              "width": 600,
              "height": 550,
              "instructor_notes": null
            },
            {
              "id": 671384,
              "key": "7efea8db-0bb2-402d-9dfc-20426c9be4ee",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The same neural network architecture can be used to approximate a **deterministic policy**.  Instead of sampling from the action probabilities, the agent need only choose the greedy action.  \n\n## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 667016,
              "key": "9656caa7-a00a-4de1-9397-5787202bd5bd",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "9656caa7-a00a-4de1-9397-5787202bd5bd",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "In the video above, you learned that the neural network that approximates the policy takes the environment state as input.  The output layer returns the probability that the agent should select each possible action.  Which of the following is a valid activation function for the output layer?",
                "answers": [
                  {
                    "id": "a1530116683048",
                    "text": "linear (i.e., no activation function)",
                    "is_correct": false
                  },
                  {
                    "id": "a1530117818140",
                    "text": "softmax",
                    "is_correct": true
                  },
                  {
                    "id": "a1530117839905",
                    "text": "ReLu",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 671391,
              "key": "29dc087d-8ae2-483c-bd8c-b5506ac9bdc3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## What about continuous action spaces?\n---\nThe CartPole environment has a discrete action space.  So, how do we use a neural network to approximate a policy, if the environment has a continuous action space?\n\nAs you learned above, in the case of **_discrete_** action spaces, the neural network has one node for each possible action.\n\nFor **_continuous_** action spaces, the neural network has one node for each action entry (or index).  For example, consider the action space of the [bipedal walker](https://github.com/openai/gym/wiki/BipedalWalker-v2) environment, shown in the figure below.",
              "instructor_notes": ""
            },
            {
              "id": 671393,
              "key": "a429cce2-2df4-4697-8f0f-91ce746c8c8e",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b3901fa_screen-shot-2018-07-01-at-11.28.57-am/screen-shot-2018-07-01-at-11.28.57-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a429cce2-2df4-4697-8f0f-91ce746c8c8e",
              "caption": "Action space of `BipedalWalker-v2` ([Source](https://github.com/openai/gym/wiki/BipedalWalker-v2))",
              "alt": "Action space of `BipedalWalker-v2`",
              "width": 500,
              "height": 632,
              "instructor_notes": null
            },
            {
              "id": 671394,
              "key": "b216ffe5-20d7-4396-aab8-43a09a25044c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this case, any action is a vector of four numbers, so the output layer of the policy network will have four nodes.  \n\nSince every entry in the action must be a number between -1 and 1, we will add a [tanh activation function](https://pytorch.org/docs/stable/nn.html#torch.nn.Tanh) to the output layer.\n\nAs another example, consider the  [continuous mountain car](https://github.com/openai/gym/wiki/MountainCarContinuous-v0) benchmark.  The action space is shown in the figure below.  Note that for this environment, the action must be a value between -1 and 1.",
              "instructor_notes": ""
            },
            {
              "id": 671395,
              "key": "123b26b3-168a-4bf2-96ee-2296be445577",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b38ff31_screen-shot-2018-07-01-at-11.19.22-am/screen-shot-2018-07-01-at-11.19.22-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/123b26b3-168a-4bf2-96ee-2296be445577",
              "caption": "Action space of `MountainCarContinuous-v0` ([Source](https://github.com/openai/gym/wiki/MountainCarContinuous-v0))",
              "alt": "Action space of `MountainCarContinuous-v0`",
              "width": 500,
              "height": 370,
              "instructor_notes": null
            },
            {
              "id": 671396,
              "key": "79054166-9091-4bf5-906c-ed7969a2971f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 671397,
              "key": "4bcd8c45-a9f2-4474-9cdc-701b1e89f64d",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "4bcd8c45-a9f2-4474-9cdc-701b1e89f64d",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Consider the  `MountainCarContinuous-v0` environment.  Which of the following describes a valid output layer for the policy?  (_Select the option that yields valid actions that can be passed directly to the environment without any additional preprocessing._)",
                "answers": [
                  {
                    "id": "a1530804512794",
                    "text": "Layer size: 1, Activation function: softmax",
                    "is_correct": false
                  },
                  {
                    "id": "a1530804568539",
                    "text": "Layer size: 1, Activation function: tanh",
                    "is_correct": true
                  },
                  {
                    "id": "a1530804573827",
                    "text": "Layer size: 2, Activation function: softmax",
                    "is_correct": false
                  },
                  {
                    "id": "a1530804578678",
                    "text": "Layer size: 2, Activation function: ReLu",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 665590,
          "key": "e8c81b53-6572-4492-8bd2-87ff7e744a3f",
          "title": "Hill Climbing",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e8c81b53-6572-4492-8bd2-87ff7e744a3f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 667655,
              "key": "bfd3f584-78d8-4657-b612-3d7bcd9e6240",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Hill Climbing",
              "instructor_notes": ""
            },
            {
              "id": 668590,
              "key": "0a6a5041-37fa-4e26-8cbd-e10f40d62992",
              "title": "M3 L2 C04 V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "5E86a0OyVyI",
                "china_cdn_id": "5E86a0OyVyI.mp4"
              }
            },
            {
              "id": 667657,
              "key": "c507db43-1dd6-477e-9a49-52e22f514303",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Gradient Ascent\n---\n**Gradient ascent** is similar to gradient descent.  \n- Gradient descent steps in the **_direction opposite the gradient_**, since it wants to minimize a function.  \n- Gradient ascent is otherwise identical, except we step in the **_direction of the gradient_**, to reach the maximum.\n\nWhile we won't cover gradient-based methods in this lesson, you'll explore them later in the course!\n\n## Local Minima\n---\n\nIn the video above, you learned that **hill climbing** is a relatively simple algorithm that the agent can use to gradually improve the weights <span class=\"mathquill\">\\theta</span> in its policy network while interacting with the environment.\n\nNote, however, that it's **_not_** guaranteed to always yield the weights of the optimal policy.  This is because we can easily get stuck in a local maximum.  In this lesson, you'll learn about some policy-based methods that are less prone to this.\n\n## Additional Note\n---\nNote that [hill climbing](https://en.wikipedia.org/wiki/Hill_climbing) is not just for reinforcement learning!  It is a general optimization method that is used to find the maximum of a function.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 672416,
          "key": "c7658269-27ad-47fc-b8de-40eb94d94893",
          "title": "Hill Climbing Pseudocode",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c7658269-27ad-47fc-b8de-40eb94d94893",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 672419,
              "key": "6d7d2fb4-9eb7-4a41-bb69-df593ed6cf8b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Hill Climbing Pseudocode",
              "instructor_notes": ""
            },
            {
              "id": 690505,
              "key": "c3ad64ab-a3cc-4f40-ab5d-9f061b072193",
              "title": "M3 L2 C05 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "0XzzqIXyax0",
                "china_cdn_id": "0XzzqIXyax0.mp4"
              }
            },
            {
              "id": 672418,
              "key": "1258cc5b-b9c7-4a10-a772-a51a665b9c1e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## What's the difference between <span class=\"mathquill\">G</span> and <span class=\"mathquill\">J</span>?\n---\nYou might be wondering: what's the difference between the return that the agent collects in a single episode (<span class=\"mathquill\">G</span>, _from the pseudocode above_) and the expected return <span class=\"mathquill\">J</span>? \n\nWell ... in reinforcement learning, the goal of the agent is to find the value of the policy network weights <span class=\"mathquill\">\\theta</span> that maximizes **_expected_** return, which we have denoted by <span class=\"mathquill\">J</span>.  \n\nIn the hill climbing algorithm, the values of <span class=\"mathquill\">\\theta</span> are evaluated according to how much return <span class=\"mathquill\">G</span> they collected in a **_single episode_**.  To see that this might be a little bit strange, note that due to randomness in the environment (and the policy, if it is stochastic), it is highly likely that if we collect a second episode with the same values for <span class=\"mathquill\">\\theta</span>, we'll likely get a different value for the return <span class=\"mathquill\">G</span>.  Because of this, the (sampled) return <span class=\"mathquill\">G</span> is not a perfect estimate for the expected return <span class=\"mathquill\">J</span>, but it often turns out to be **_good enough_** in practice.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 665593,
          "key": "30cf84e6-483d-456a-a8ed-7e514e8b3304",
          "title": "Beyond Hill Climbing",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "30cf84e6-483d-456a-a8ed-7e514e8b3304",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 665629,
              "key": "e82dafb8-68cb-464e-afd1-8c3bbc3a8ce2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Beyond Hill Climbing",
              "instructor_notes": ""
            },
            {
              "id": 665630,
              "key": "0efdda37-a67b-4129-8890-d9e5609b831c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the previous video, you learned about the hill climbing algorithm. \n\nWe denoted the expected return by <span class=\"mathquill\">J</span>.  Likewise, we used <span class=\"mathquill\">\\theta</span> to refer to the weights in the policy network. Then, since <span class=\"mathquill\">\\theta</span> encodes the policy, which influences how much reward the agent will likely receive, we know that <span class=\"mathquill\">J</span> is a function of <span class=\"mathquill\">\\theta</span>. \n\nDespite the fact that we have no idea what that function <span class=\"mathquill\">J = J(\\theta)</span> looks like, the _hill climbing_ algorithm helps us determine the value of <span class=\"mathquill\">\\theta</span> that maximizes it.  Watch the video below to learn about some improvements you can make to the hill climbing algorithm!  \n\n_Note_: We refer to the general class of approaches that find <span class=\"mathquill\">\\arg\\max_{\\theta}J(\\theta)</span> through randomly perturbing the most recent best estimate as **stochastic policy search**.  Likewise, we can refer to <span class=\"mathquill\">J</span> as an **objective function**, which just refers to the fact that we'd like to _maximize_ it!\n\n## Video\n---",
              "instructor_notes": ""
            },
            {
              "id": 665606,
              "key": "7f4b914b-2b1e-4693-bb2d-33872383df33",
              "title": "M2L3 04 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "QicxmyE5vTo",
                "china_cdn_id": "QicxmyE5vTo.mp4"
              }
            }
          ]
        },
        {
          "id": 665595,
          "key": "b3d797e7-b54d-4c92-b6fa-2e27800b5e92",
          "title": "More Black-Box Optimization",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b3d797e7-b54d-4c92-b6fa-2e27800b5e92",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 667694,
              "key": "0de45b45-773c-403b-ad6b-9e2775bc1841",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# More Black-Box Optimization",
              "instructor_notes": ""
            },
            {
              "id": 668566,
              "key": "80aac0ec-f17f-4011-a819-063729c378fe",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "All of the algorithms that youâ€™ve learned about in this lesson can be classified as **black-box optimization** techniques.  \n\n**Black-box** refers to the fact that in order to find the value of <span class=\"mathquill\">\\theta</span> that maximizes the function <span class=\"mathquill\">J = J(\\theta)</span>, we need only be able to estimate the value of <span class=\"mathquill\">J</span> at any potential value of <span class=\"mathquill\">\\theta</span>.  \n\nThat is, both hill climbing and steepest ascent hill climbing don't know that we're solving a reinforcement learning problem, and they do not care that the function we're trying to maximize corresponds to the expected return.  \n\nThese algorithms only know that for each value of <span class=\"mathquill\">\\theta</span>, there's a corresponding **_number_**.  We know that this **_number_** corresponds to the return obtained by using the policy corresponding to <span class=\"mathquill\">\\theta</span> to collect an episode, but the algorithms are not aware of this.  To the algorithms, the way we evaluate <span class=\"mathquill\">\\theta</span> is considered a black box, and they don't worry about the details.  The algorithms only care about finding the value of <span class=\"mathquill\">\\theta</span> that will maximize the number that comes out of the black box.\n\nIn the video below, you'll learn about a couple more black-box optimization techniques, to include the **cross-entropy method** and **[evolution strategies](https://blog.openai.com/evolution-strategies/)**.\n\n## Video\n---",
              "instructor_notes": ""
            },
            {
              "id": 668592,
              "key": "d93890ec-840e-410d-88d9-8543dab6ae34",
              "title": "M3 L2 C07 V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "2poDljPvY58",
                "china_cdn_id": "2poDljPvY58.mp4"
              }
            }
          ]
        },
        {
          "id": 667667,
          "key": "2add6d70-819b-43df-92ea-9a84d8c60569",
          "title": "Coding Exercise",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "2add6d70-819b-43df-92ea-9a84d8c60569",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 667697,
              "key": "7657695d-0729-465b-8793-b5ca05c3352d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/September/59c73b1b_jupyter/jupyter.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/7657695d-0729-465b-8793-b5ca05c3352d",
              "caption": "",
              "alt": "",
              "width": 100,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 689132,
              "key": "958bf8f7-7deb-40e2-bb5d-32cb6e4468e3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Coding Exercise",
              "instructor_notes": ""
            },
            {
              "id": 667698,
              "key": "71af9b42-62f0-4188-8e87-25c9f46946b3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the following Workspace, you will explore implementations of two different policy-based methods.\n\n## Exercise #1\n---\n\nIn this exercise, you'll explore an implementation of a cross-entropy method.  To begin, open the Workspace in the next concept, and open `CEM.ipynb` (in the `cross-entropy` folder).\n\n## Exercise #2\n---\n\nIn this exercise, you'll explore an implementation of hill-climbing with adaptive noise scaling.  To begin, open the Workspace in the next concept, and open `Hill_Climbing.ipynb` (in the `hill-climbing` folder).\n\n## Note\n---\nIn the Workspace, you will have the option to **ENABLE GPU** to accelerate training.  After training, you can use the provided code in the Jupyter notebook to watch your agent's performance.  Note that if visualizing the trained agent in the Workspace, GPU should be **disabled** -- otherwise the notebook will return an error.  \n\nThus, you are encouraged to follow the following workflow: \n1. train the agent with GPU **enabled**, and save the trained model weights, \n2. **disable GPU**, load the trained weights from file, and watch the trained agent.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 665592,
          "key": "df5ffc34-9101-41e8-831f-92ed03d1e3cf",
          "title": "Workspace",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "df5ffc34-9101-41e8-831f-92ed03d1e3cf",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 668560,
              "key": "1216c787-522a-49fa-b1ef-d9020bdfaf9e",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "view1f8fbb3c",
              "pool_id": "jupytergpu",
              "view_id": "1f8fbb3c-acbc-435b-b9bf-a5bbd0f9af4d",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 668539,
          "key": "7e095445-ba73-4f73-a6a7-29d3cc7ede97",
          "title": "OpenAI Request for Research",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7e095445-ba73-4f73-a6a7-29d3cc7ede97",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 668541,
              "key": "f5d1d9a3-7e3d-49a6-907f-addbdef530ee",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# OpenAI Request for Research",
              "instructor_notes": ""
            },
            {
              "id": 668548,
              "key": "c6f3cdce-7aa1-4a32-8737-fddc4b48416d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So far in this lesson, you have learned about many black-box optimization techniques for finding the optimal policy.  Run each algorithm for many random seeds, to test stability.  \n\nTake the time now to implement some of them, and compare performance on \nOpenAI Gym's `CartPole-v0` environment.  \n\n> **Note**: This suggested exercise is completely optional.\n\nOnce you have completed your analysis, you're encouraged to write up your own blog post that responds to [OpenAI's Request for Research](https://openai.com/requests-for-research/#cartpole)!  (_This request references policy gradient methods.  You'll learn about policy gradient methods in the next lesson._)\n",
              "instructor_notes": ""
            },
            {
              "id": 668547,
              "key": "6f7186d0-1013-4bcd-bba8-5aa18ee7257c",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b3790c2_cartpole/cartpole.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6f7186d0-1013-4bcd-bba8-5aa18ee7257c",
              "caption": "OpenAI Gym's CartPole environment",
              "alt": "OpenAI Gym's CartPole environment",
              "width": 300,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 668540,
              "key": "2f88cf3b-a2a0-4ad3-9b24-b3df6b8a9ab1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Implement (vanilla) hill climbing and steepest ascent hill climbing, both with simulated annealing and adaptive noise scaling.\n\nIf you also want to compare the performance to evolution strategies, you can find a well-written implementation [here](https://github.com/alirezamika/evostra).  To see how to apply it to an OpenAI Gym task, check out [this repository](https://github.com/alirezamika/bipedal-es).\n\nTo see one way to structure your analysis, check out [this blog post](http://kvfrans.com/simple-algoritms-for-solving-cartpole/), along with the [accompanying code](https://github.com/kvfrans/openai-cartpole).\n\nFor instance, you will likely find that hill climbing is very unstable, where the number of episodes that it takes to solve `CartPole-v0` varies greatly with the random seed.  (_Check out the figure below!_)",
              "instructor_notes": ""
            },
            {
              "id": 668542,
              "key": "966cc2ef-977d-4608-b9ef-22dc33e26b85",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b357385_screen-shot-2018-06-28-at-6.46.54-pm/screen-shot-2018-06-28-at-6.46.54-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/966cc2ef-977d-4608-b9ef-22dc33e26b85",
              "caption": "Histogram of number of episodes needed to solve CartPole with hill climbing. ([Source](http://kvfrans.com/simple-algoritms-for-solving-cartpole/))",
              "alt": "Histogram of number of episodes needed to solve CartPole with hill climbing.",
              "width": 500,
              "height": 500,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 665603,
          "key": "24679bd1-7b0a-4f7d-8b44-294f054b92b5",
          "title": "Why Policy-Based Methods?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "24679bd1-7b0a-4f7d-8b44-294f054b92b5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 665625,
              "key": "23510ea1-ba02-4f6a-b147-6c849273f1a7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Why Policy-Based Methods?",
              "instructor_notes": ""
            },
            {
              "id": 665627,
              "key": "ab365d2d-594b-4b8d-ad7c-63b7d56fe163",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this lesson, you've learned about several policy-based methods.  But why do we need policy-based methods at all, when value-based methods work so well? \n\nWatch the video below to find out!",
              "instructor_notes": ""
            },
            {
              "id": 665608,
              "key": "cbb2b549-ce18-40f0-a85f-7f17817f6567",
              "title": "M2L3 02 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "ToS8vXGdODE",
                "china_cdn_id": "ToS8vXGdODE.mp4"
              }
            }
          ]
        },
        {
          "id": 665609,
          "key": "50fefd72-d22c-4645-92bf-c74ee4311764",
          "title": "Summary",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "50fefd72-d22c-4645-92bf-c74ee4311764",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 665617,
              "key": "8b1bf30f-b921-4b28-8868-c357a3797cf1",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Summary",
              "instructor_notes": ""
            },
            {
              "id": 665621,
              "key": "6b4f1ae3-07ee-49af-9ab9-da9ff06ee07a",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/June/5b3270ab_screen-shot-2018-06-26-at-11.53.35-am/screen-shot-2018-06-26-at-11.53.35-am.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/6b4f1ae3-07ee-49af-9ab9-da9ff06ee07a",
              "caption": "Objective function",
              "alt": "Objective function",
              "width": 300,
              "height": 300,
              "instructor_notes": null
            },
            {
              "id": 665620,
              "key": "b0d4fb6d-88e1-418c-8d9e-776f90921f70",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### Policy-Based Methods\n---\n- With **value-based methods**, the agent uses its experience with the environment to maintain an estimate of the optimal action-value function.  The optimal policy is then obtained from the optimal action-value function estimate.\n- **Policy-based methods** directly learn the optimal policy, without having to maintain a separate value function estimate.\n\n### Policy Function Approximation\n---\n- In deep reinforcement learning, it is common to represent the policy with a neural network.  \n - This network takes the environment state as **_input_**.  \n - If the environment has discrete actions, the **_output_** layer has a node for each possible action and contains the probability that the agent should select each possible action.\n- The weights in this neural network are initially set to random values.  Then, the agent updates the weights as it interacts with (_and learns more about_) the environment.\n\n### More on the Policy\n---\n- Policy-based methods can learn either stochastic or deterministic policies, and they can be used to solve environments with either finite or continuous action spaces.\n\n### Hill Climbing\n---\n- **Hill climbing** is an iterative algorithm that can be used to find the weights <span class=\"mathquill\">\\theta</span> for an optimal policy.\n- At each iteration, \n - We slightly perturb the values of the current best estimate for the weights <span class=\"mathquill\">\\theta_{best}</span>, to yield a new set of weights.  \n - These new weights are then used to collect an episode.  If the new weights <span class=\"mathquill\">\\theta_{new}</span> resulted in higher return than the old weights, then we set <span class=\"mathquill\">\\theta_{best} \\leftarrow \\theta_{new}</span>.\n\n### Beyond Hill Climbing\n---\n- **Steepest ascent hill climbing** is a variation of hill climbing that chooses a small number of neighboring policies at each iteration and chooses the best among them.\n- **Simulated annealing** uses a pre-defined schedule to control how the policy space is explored, and gradually reduces the search radius as we get closer to the optimal solution.\n- **Adaptive noise scaling** decreases the search radius with each iteration when a new best policy is found, and otherwise increases the search radius.\n\n### More Black-Box Optimization\n---\n- The **cross-entropy method** iteratively suggests a small number of neighboring policies, and uses a small percentage of the best performing policies to calculate a new estimate.\n- The **evolution strategies** technique considers the return corresponding to each candidate policy.  The policy estimate at the next iteration is a weighted sum of all of the candidate policies, where policies that got higher return are given higher weight.  \n\n### Why Policy-Based Methods?\n---\n- There are three reasons why we consider policy-based methods:\n    1. **Simplicity**: Policy-based methods directly get to the problem at hand (estimating the optimal policy), without having to store a bunch of additional data (i.e., the action values) that may not be useful.\n    2. **Stochastic policies**: Unlike value-based methods, policy-based methods can learn true stochastic policies.\n    3. **Continuous action spaces**: Policy-based methods are well-suited for continuous action spaces.",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}