{
  "data": {
    "lesson": {
      "id": 676212,
      "key": "39290c92-fee6-49b6-9962-3afcd3e80abf",
      "title": "Actor-Critic Methods",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Miguel Morales explains how to combine value-based and policy-based methods, bringing together the best of both worlds, to solve challenging reinforcement learning problems.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/39290c92-fee6-49b6-9962-3afcd3e80abf/676212/1550867121754/Actor-Critic+Methods+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/39290c92-fee6-49b6-9962-3afcd3e80abf/676212/1550867116581/Actor-Critic+Methods+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 676213,
          "key": "d5d1fdf4-beee-4c2e-adf0-b4d8b7a5fffa",
          "title": "Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "d5d1fdf4-beee-4c2e-adf0-b4d8b7a5fffa",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722409,
              "key": "dde8857d-5196-436b-be44-5d4b2de56d3a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Introduction\n",
              "instructor_notes": ""
            },
            {
              "id": 797422,
              "key": "12587c2c-2b27-453e-9a00-465a4eebb5d4",
              "title": "M3L501 Introduction HS 1 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_OHo1pEaJcQ",
                "china_cdn_id": "_OHo1pEaJcQ.mp4"
              }
            },
            {
              "id": 722569,
              "key": "14161a92-e1ad-470b-affd-8db4cdb9e436",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Important links\n\n* I partnered with Udacity and Manning to give all students of this Nanodegree the opportunity to obtain a copy of my book at a 40% discount. The more perspectives you get on deep reinforcement learning, the better for you. If you'd like to give it a try, go get chapter 01 for free [here](http://bit.ly/gdrl_u). Make sure to use the discount code: 'gdrludacity40' at checkout for a 40% off.\n* I'm very proud to work for Lockheed Martin. Working for a company that does work that matters with a huge impact on the lives of many people is very rewarding (no pun intended.) If you are interested in joining us, check out this [link](http://bit.ly/lmco_u). We are always looking for talented individuals with experience in AI (and deep reinforcement learning).\n* Last, but not least. I'm an Instructional Associate at Georgia Tech for the Reinforcement Learning and Decision Making graduate course available on the OMSCS program that Udacity helped launch. If you'd like more information regarding this program, take a look [here](http://bit.ly/omscs_u).",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 681420,
          "key": "41cc8d5b-ecbc-4352-8fc0-1f3552115a32",
          "title": "Motivation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "41cc8d5b-ecbc-4352-8fc0-1f3552115a32",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722416,
              "key": "b694c894-1d19-4cdc-964b-50e6caac8ab5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Motivation",
              "instructor_notes": ""
            },
            {
              "id": 797423,
              "key": "3bf6e972-04be-4fa7-afa3-d06ce8a56b2e",
              "title": "M3 L5 02 Motivation V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "dpFPlDtdxyQ",
                "china_cdn_id": "dpFPlDtdxyQ.mp4"
              }
            }
          ]
        },
        {
          "id": 681421,
          "key": "e0ce7dae-960d-4185-ac17-22a16cb92b12",
          "title": "Bias and Variance",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "e0ce7dae-960d-4185-ac17-22a16cb92b12",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722417,
              "key": "4c0ac803-d9c7-4271-83a3-6cf72a724d1f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Bias and Variance",
              "instructor_notes": ""
            },
            {
              "id": 797424,
              "key": "8524f684-ae8e-48f9-9683-099ec20ebe13",
              "title": "M3 L5 03 Bias And Variance V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "_vnkkwm46uU",
                "china_cdn_id": "_vnkkwm46uU.mp4"
              }
            }
          ]
        },
        {
          "id": 681422,
          "key": "0e12badc-ca1d-4f72-8cd6-a65b7f358ef9",
          "title": "Two Ways for Estimating Expected Returns",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "0e12badc-ca1d-4f72-8cd6-a65b7f358ef9",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722418,
              "key": "283a98ee-0628-4686-b007-9495b7d01266",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Two Ways For Estimating Expected Returns",
              "instructor_notes": ""
            },
            {
              "id": 797425,
              "key": "1b2f4a57-4579-4ddc-a40f-a4344893894a",
              "title": "M3 L5 04 Two Ways For Estimating Expected Returns V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "2W6yIBDvfsQ",
                "china_cdn_id": "2W6yIBDvfsQ.mp4"
              }
            }
          ]
        },
        {
          "id": 681423,
          "key": "f15a4527-e179-45f6-8d6d-6c2335cf5fd2",
          "title": "Baselines and Critics",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f15a4527-e179-45f6-8d6d-6c2335cf5fd2",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722419,
              "key": "244d284e-a5f6-47a8-a1a4-5856c6ca18a4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Baselines and Critics",
              "instructor_notes": ""
            },
            {
              "id": 797426,
              "key": "668bd272-7482-4e89-8c67-7e1728be9b12",
              "title": "M3 L5 05 Baselines And Critics V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "wqmqoiUuQHI",
                "china_cdn_id": "wqmqoiUuQHI.mp4"
              }
            },
            {
              "id": 722644,
              "key": "aa179342-25ad-401d-8fa6-7ab39e4c334e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "The argument you often hear as to why to call a neural network trained with Monte-Carlo estimates a \"Critic\" is because function approximators, such as a neural network, are biased as a byproduct that they are not perfect. That's a fair point, though, I prefer the distinction based on whether we pick a Monte-Carlo or a TD estimate to train our function approximator. Now, definitely we should not be calling Actor-Critic methods every method that uses 2 neural networks. You'll be surprised!\n\nThe important takeaway for you, though, is that there are inconsistencies out there. You often see methods named \"Actor-Critic\" when they are not. I just want to bring the issue to your attention.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 681424,
          "key": "1dc5164d-315e-4c7c-b334-b3fff0841fde",
          "title": "Policy-based, Value-Based, and Actor-Critic",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1dc5164d-315e-4c7c-b334-b3fff0841fde",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722420,
              "key": "73bc0460-ffb3-4cc8-89a3-075077314e2a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Policy-based, Value-based and Actor-Critic",
              "instructor_notes": ""
            },
            {
              "id": 797427,
              "key": "b1afb3e4-2a8f-448c-8746-1367be975f13",
              "title": "M3 L5 06 Policybased Valuebased And ActorCritic V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "iyin896PNEc",
                "china_cdn_id": "iyin896PNEc.mp4"
              }
            }
          ]
        },
        {
          "id": 681425,
          "key": "9a115552-d2df-45fc-b33c-c26cfcff81a1",
          "title": "A Basic Actor-Critic Agent",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9a115552-d2df-45fc-b33c-c26cfcff81a1",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722421,
              "key": "7edf2729-36df-4b71-97b4-ca6458728d80",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# A Basic Actor-Critic Agent",
              "instructor_notes": ""
            },
            {
              "id": 797428,
              "key": "390a715f-1275-4612-a777-3c47b26551b4",
              "title": "M3 L5 07 A Basic ActorCritic Agent V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "KdHQ24hBKho",
                "china_cdn_id": "KdHQ24hBKho.mp4"
              }
            },
            {
              "id": 722645,
              "key": "967256d7-f18b-4817-9dbc-c3b4112c2dc8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "One important thing to note here is that I use <span class=\"mathquill\">V(s;\\theta_v)</span> or <span class=\"mathquill\">A(s,a)</span>, but sometimes <span class=\"mathquill\">V_\\pi(s;\\theta_v)</span> or <span class=\"mathquill\">A_\\pi(s,a)</span> (see the <span class=\"mathquill\">\\pi</span> there? See the <span class=\"mathquill\">\\theta_v</span>? What's going on?) \n\nThere are 2 thing actually going on in there. \n\n1. A very common thing you'll see in reinforcement learning is the oversimplification of notation. However, both styles, whether you see <span class=\"mathquill\">A(s,a)</span>, or <span class=\"mathquill\">A_\\pi(s,a)</span> (value functions with or without a <span class=\"mathquill\">\\pi</span>,) it means you are evaluating a value function of policy <span class=\"mathquill\">\\pi</span>. In the case of <span class=\"mathquill\">A</span>, the advantage function. A different case would be when you see a superscript <span class=\"mathquill\">*</span>. For example, <span class=\"mathquill\">A^*(s,a)</span> means the optimal advantage function. Q-learning learns the optimal action-value function, <span class=\"mathquill\">Q^*(s,a)</span>, for example.\n2. The other thing is the use of <span class=\"mathquill\">\\theta_v</span> in some value functions and not in others. This only means that such value function is using a neural network. For example, <span class=\"mathquill\">V(s;\\theta_v)</span> is using a neural network as a function approximator, but <span class=\"mathquill\">A(s,a)</span> is not. We are calculating the advantage function <span class=\"mathquill\">A(s, a)</span> using the state-value function <span class=\"mathquill\">V(s;\\theta_v)</span>, but <span class=\"mathquill\">A(s, a)</span> is not using function approximation directly.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 681426,
          "key": "0f59f0ba-cadb-456d-84fa-b33f4c769b03",
          "title": "A3C: Asynchronous Advantage Actor-Critic, N-step",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "0f59f0ba-cadb-456d-84fa-b33f4c769b03",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722422,
              "key": "ac7d5d7a-3545-46f3-9ec0-d8e012d66a46",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# A3C: Asynchronous Advantage Actor-Critic, N-step Bootstrapping",
              "instructor_notes": ""
            },
            {
              "id": 797429,
              "key": "d7cc3a0d-687b-40b8-881a-0eb58215b58e",
              "title": "M3 L5 08 A3C Asynchronous Advantage ActorCritic V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "twNXFplIAP8",
                "china_cdn_id": "twNXFplIAP8.mp4"
              }
            }
          ]
        },
        {
          "id": 722423,
          "key": "d754da7a-d270-4a6e-802a-7a74ac21635f",
          "title": "A3C: Asynchronous Advantage Actor-Critic, Parallel Training",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "d754da7a-d270-4a6e-802a-7a74ac21635f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722424,
              "key": "3e126e41-eb2c-4da0-b45d-0cb42eceb908",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# A3C: Asynchronous Advantage Actor-Critic, Parallel Training",
              "instructor_notes": ""
            },
            {
              "id": 797430,
              "key": "84aff732-0dae-4368-b6c6-229e12f02833",
              "title": "M3 L5 09 A3C Asynchronous Advantage ActorCritic Parallel Training V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "kKRbAKhjACo",
                "china_cdn_id": "kKRbAKhjACo.mp4"
              }
            }
          ]
        },
        {
          "id": 722425,
          "key": "8158135b-8e08-45a2-bb46-cee64107f1c9",
          "title": "A3C: Asynchronous Advantage Actor-Critic, Off- vs On-policy",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8158135b-8e08-45a2-bb46-cee64107f1c9",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722426,
              "key": "b20bda33-25db-495b-98cf-6e938828e72b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# A3C: Asynchronous Advantage Actor-Critic, Off-policy vs. On-policy",
              "instructor_notes": ""
            },
            {
              "id": 797431,
              "key": "b1434b4e-5b46-403d-b401-a1c676134b7b",
              "title": "M3 L5 10 A3C Asynchronous Advantage ActorCritic Offpolicy Vs Onpolicy V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "AZiy5R0DESU",
                "china_cdn_id": "AZiy5R0DESU.mp4"
              }
            },
            {
              "id": 722649,
              "key": "f199b12f-9cd5-40a3-b39f-c84759cec8c2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Link to the Q-Prop paper: https://arxiv.org/abs/1611.02247",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 681427,
          "key": "491eb241-a37e-4dc6-baee-806846d01b04",
          "title": "A2C: Advantage Actor-Critic",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "491eb241-a37e-4dc6-baee-806846d01b04",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722427,
              "key": "3c2cff65-6fe1-422b-9725-4b56ce4d203e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# A2C: Advantage Actor-Critic",
              "instructor_notes": ""
            },
            {
              "id": 797432,
              "key": "76f364dc-ed90-4d1b-84ed-c15d91d3864c",
              "title": "M3 L5 11 A2C Advantage ActorCritic V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "fIWe3xA97DA",
                "china_cdn_id": "fIWe3xA97DA.mp4"
              }
            }
          ]
        },
        {
          "id": 722428,
          "key": "1c125738-0b24-42f3-b70e-0a4514adba29",
          "title": "A2C Code Walk-through",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "1c125738-0b24-42f3-b70e-0a4514adba29",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722429,
              "key": "c4b7ea64-1dcc-4687-b678-2f7eb4348cf7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# A2C Code Walk-through",
              "instructor_notes": ""
            },
            {
              "id": 797433,
              "key": "7d885693-992b-4ac4-ac9c-2a1fe4f647bf",
              "title": "A2c Export V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "LiUBJje2N0c",
                "china_cdn_id": "LiUBJje2N0c.mp4"
              }
            }
          ]
        },
        {
          "id": 681428,
          "key": "20da8bd3-557a-41ba-964f-af6786a4e5a8",
          "title": "GAE: Generalized Advantage Estimation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "20da8bd3-557a-41ba-964f-af6786a4e5a8",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722430,
              "key": "edc34a95-4f4f-41a8-9db4-41bb861af279",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# GAE: Generalized Advantage Estimation",
              "instructor_notes": ""
            },
            {
              "id": 797434,
              "key": "219cbd7e-18d8-4459-9094-8a4331dd75b4",
              "title": "M3 L5 13 GAE Generalized Advantage Estimation V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "oLFocWp0dt0",
                "china_cdn_id": "oLFocWp0dt0.mp4"
              }
            },
            {
              "id": 722650,
              "key": "0b57d173-f3ec-4438-a480-e84ee5d18a49",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Link to the GAE paper: https://arxiv.org/abs/1506.02438",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 681429,
          "key": "7376e9fe-7ba2-43a2-89ab-e4bbd3276329",
          "title": "DDPG: Deep Deterministic Policy Gradient, Continuous Actions",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7376e9fe-7ba2-43a2-89ab-e4bbd3276329",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722431,
              "key": "94cc742e-75cf-475f-ac6e-d0bdfb21b80c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# DDPG: Deep Deterministic Policy Gradient, Continuous Action-space",
              "instructor_notes": ""
            },
            {
              "id": 797435,
              "key": "f8c72d12-3651-47f8-af0c-215841a0348d",
              "title": "M3 L5 14 DDPG Deep Deterministic Policy Gradient Continuous Actionspace V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "0NVOPIyrr98",
                "china_cdn_id": "0NVOPIyrr98.mp4"
              }
            },
            {
              "id": 722651,
              "key": "80054ac5-a4fb-4484-bf2a-c6b6d28f9e65",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the [DDPG paper](https://arxiv.org/abs/1509.02971), they introduced this algorithm as an \"Actor-Critic\" method. Though, some researchers think DDPG is best classified as a DQN method for continuous action spaces (along with [NAF](https://arxiv.org/abs/1603.00748)). Regardless, DDPG is a very successful method and it's good for you to gain some intuition.\n ",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 722432,
          "key": "a10a51d6-85dc-4656-9b73-62acc1479c79",
          "title": "DDPG: Deep Deterministic Policy Gradient, Soft Updates",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a10a51d6-85dc-4656-9b73-62acc1479c79",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722435,
              "key": "8dff6790-2158-4083-81fd-c5f29b338538",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# DDPG: Deep Deterministic Policy Gradient, Soft Updates",
              "instructor_notes": ""
            },
            {
              "id": 797436,
              "key": "65b7c47d-1111-4775-8c56-f5af4c33764b",
              "title": "M3 L5 15 DDPG Deep Deterministic Policy Gradient Soft Updates V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "RT-HDnAVe9o",
                "china_cdn_id": "RT-HDnAVe9o.mp4"
              }
            }
          ]
        },
        {
          "id": 722433,
          "key": "7df80143-86cc-45a7-95bc-841d4cdb692a",
          "title": "DDPG Code Walk-through",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7df80143-86cc-45a7-95bc-841d4cdb692a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722434,
              "key": "fc685427-f91d-4a94-a398-32ead3abdd5d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# DDPG Code Walkthrough",
              "instructor_notes": ""
            },
            {
              "id": 797437,
              "key": "882fc2b8-bb05-4ea7-86ed-ad401913545c",
              "title": "DDPG Export V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "08V9r3NgFSE",
                "china_cdn_id": "08V9r3NgFSE.mp4"
              }
            }
          ]
        },
        {
          "id": 681430,
          "key": "b4cf7f11-d9f2-4327-b424-83c238fab899",
          "title": "Summary",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b4cf7f11-d9f2-4327-b424-83c238fab899",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 722436,
              "key": "db35a182-c65e-4994-8e47-13876e308be5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Summary",
              "instructor_notes": ""
            },
            {
              "id": 797438,
              "key": "34f21c9a-4eed-4e71-92cb-f66a4f4dab1f",
              "title": "M3L517 Summary HS 1 V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "rRuiMhijw_s",
                "china_cdn_id": "rRuiMhijw_s.mp4"
              }
            },
            {
              "id": 722643,
              "key": "5f264056-776c-4592-a6d1-d65a005aeb1d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Final Reminders\n\n*  Make sure you take advantage of the discount Udacity is able to bring you on [Grokking Deep Reinforcement Learning](http://bit.ly/gdrl_u): remember, 'gdrludacity40' gives you a 40% off.\n* Finally, life is better together. Make sure to connect [here](http://bit.ly/mimoralea_t), [here](http://bit.ly/mimoralea_l), or [some other way](http://bit.ly/mimoralea).\n\nGood luck to you going forward!\n",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  }
}