{
  "data": {
    "lesson": {
      "id": 624917,
      "key": "56729989-c3b3-4c26-a37f-0fc3ec1b5eae",
      "title": "Policy Gradient Methods",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Policy gradient methods search for the optimal policy through gradient ascent.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/56729989-c3b3-4c26-a37f-0fc3ec1b5eae/624917/1545253010705/Policy+Gradient+Methods+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/56729989-c3b3-4c26-a37f-0fc3ec1b5eae/624917/1545253008385/Policy+Gradient+Methods+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 681692,
          "key": "f7f111f5-02c8-4e89-b452-aa654c75d60c",
          "title": "What are Policy Gradient Methods?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "f7f111f5-02c8-4e89-b452-aa654c75d60c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 682344,
              "key": "7dd87475-b4a8-4ba8-bb03-76bdb8f7dd27",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# What are Policy Gradient Methods?",
              "instructor_notes": ""
            },
            {
              "id": 682581,
              "key": "d4a40986-6a4a-46ed-a744-f86cb6d5e76f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Policy gradient methods are a subclass of policy-based methods.  Watch the video below to learn more!",
              "instructor_notes": ""
            },
            {
              "id": 797407,
              "key": "e055b530-337c-472f-b549-1caa0e965ea0",
              "title": "M3L3 C01 V3",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "ZEhQRASU5O4",
                "china_cdn_id": "ZEhQRASU5O4.mp4"
              }
            },
            {
              "id": 682346,
              "key": "52447067-d643-466b-8815-f96e6859ec0a",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In the **Introduction to Policy-Based Methods** lesson, you learned about many policy-based methods that could approximate either a deterministic or stochastic policy.  \n\nIn this lesson, we'll confine our attention to stochastic policies.\n\n## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 682347,
              "key": "11229caf-0d82-47b0-92e9-ec83a6080190",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "11229caf-0d82-47b0-92e9-ec83a6080190",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following is a valid approach, if we'd like to use a neural network to approximate an agent's stochastic policy (_for a discrete action space_)? (Select all that apply.)",
                "answers": [
                  {
                    "id": "a1532618811284",
                    "text": "Use a softmax activation function in the output layer. This will ensure the network outputs probabilities. For each state input, sample an action from the output probability distribution.",
                    "is_correct": true
                  },
                  {
                    "id": "a1532618998737",
                    "text": "Use a ReLU activation function in the output layer. This will ensure the network outputs probabilities. For each state input, sample an action from the output probability distribution.",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 694398,
              "key": "93fa7658-3b51-438f-a165-80a1a2f10d41",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "93fa7658-3b51-438f-a165-80a1a2f10d41",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following is true about the difference between policy-based and policy gradient methods? (Select all that apply.)",
                "answers": [
                  {
                    "id": "a1534778163053",
                    "text": "Policy gradient methods are a subclass of policy-based methods.",
                    "is_correct": true
                  },
                  {
                    "id": "a1534778202295",
                    "text": "Not all policy gradient methods are policy-based methods.",
                    "is_correct": false
                  },
                  {
                    "id": "a1534778213226",
                    "text": "Not all policy-based methods are policy gradient methods.",
                    "is_correct": true
                  },
                  {
                    "id": "a1534778226328",
                    "text": "Both policy-based methods and policy gradient methods directly try to optimize for the optimal policy, without maintaining value function estimates.",
                    "is_correct": true
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 624921,
          "key": "c9f2b70d-633d-4217-ae95-882aeb6b587a",
          "title": "The Big Picture",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c9f2b70d-633d-4217-ae95-882aeb6b587a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 676185,
              "key": "0a4914d1-38a8-4340-8778-1695ec511d76",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# The Big Picture",
              "instructor_notes": ""
            },
            {
              "id": 682582,
              "key": "b37dfa41-5fda-4d45-b709-64ca23726fbb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Before digging into the details of policy gradient methods, we'll discuss how they work at a high level.",
              "instructor_notes": ""
            },
            {
              "id": 797408,
              "key": "9405fa71-2b1b-420f-bda7-ef467ebc1695",
              "title": "M3L3 C02 V6",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "zoOgRoaxGiU",
                "china_cdn_id": "zoOgRoaxGiU.mp4"
              }
            },
            {
              "id": 694400,
              "key": "237127f3-7481-4b45-8adb-7f3067308759",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Quiz\n---",
              "instructor_notes": ""
            },
            {
              "id": 694401,
              "key": "2f9e8791-3fa0-4aa3-b930-093ceead728d",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "2f9e8791-3fa0-4aa3-b930-093ceead728d",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of the following is true about how the policy gradient method will work? (Select all that apply.)",
                "answers": [
                  {
                    "id": "a1534778499025",
                    "text": "For each episode, if the agent won the game, we'll amend the policy network weights to make each (state, action) pair that appeared in the episode to make them more likely to repeat in future episodes.",
                    "is_correct": true
                  },
                  {
                    "id": "a1534778630975",
                    "text": "For each episode, we randomly nudge the policy weights a bit by adding Gaussian noise, and if the agent did better, we keep the new weights; otherwise, we revert to the old weights.",
                    "is_correct": false
                  },
                  {
                    "id": "a1534778747086",
                    "text": "For each episode, if the agent lost the game, we'll change the policy network weights to make it less likely to repeat the corresponding (state, action) pairs in future episodes.",
                    "is_correct": true
                  },
                  {
                    "id": "a1534778808276",
                    "text": "After each time step, if we arrive at a terminal state, we add noise to the policy network weights.",
                    "is_correct": false
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 676157,
          "key": "c0e96242-507b-4672-9986-1e8ccb263d59",
          "title": "Connections to Supervised Learning",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "c0e96242-507b-4672-9986-1e8ccb263d59",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 682343,
              "key": "4bbcd6c8-8d6c-4878-9607-32112cda874e",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Connections to Supervised Learning",
              "instructor_notes": ""
            },
            {
              "id": 682585,
              "key": "8e03ebde-712d-4a93-9a7d-eef59d2ad2e7",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Policy gradient methods are very similar to supervised learning.",
              "instructor_notes": ""
            },
            {
              "id": 797409,
              "key": "58602a02-3607-469f-b7e0-b77372278602",
              "title": "M3L3 C03 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "dJz_p4FKE-g",
                "china_cdn_id": "dJz_p4FKE-g.mp4"
              }
            },
            {
              "id": 682583,
              "key": "230765b2-0ee5-4a5d-94a6-da3400a7d229",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## (Optional) Learn More \n---\nTo further explore the connections between policy gradient methods and supervised learning, you're encouraged to check out Andrej Karpathy's [famous blog post](http://karpathy.github.io/2016/05/31/rl/).  ",
              "instructor_notes": ""
            },
            {
              "id": 682584,
              "key": "bfa959a5-13ad-409a-8db2-f99327f34699",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b5b71dd_screen-shot-2018-07-27-at-2.25.43-pm/screen-shot-2018-07-27-at-2.25.43-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/bfa959a5-13ad-409a-8db2-f99327f34699",
              "caption": "Learn more about the connections between supervised learning and reinforcement learning. ([Source](http://karpathy.github.io/2016/05/31/rl/))",
              "alt": "Learn more about the connections between supervised learning and reinforcement learning.",
              "width": 700,
              "height": 1182,
              "instructor_notes": null
            }
          ]
        },
        {
          "id": 676155,
          "key": "150a0d9a-d6b8-4d60-adaa-93f4f86952d5",
          "title": "Problem Setup",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "150a0d9a-d6b8-4d60-adaa-93f4f86952d5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 676186,
              "key": "f9a06075-2d21-4795-a8cd-836de1897339",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Problem Setup",
              "instructor_notes": ""
            },
            {
              "id": 682586,
              "key": "17728fd7-d7b9-4fad-b84e-ebd796e00548",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We're now ready to get started with rigorously defining how policy gradient methods will work.",
              "instructor_notes": ""
            },
            {
              "id": 797410,
              "key": "2a9c405a-ac8a-4abd-be41-e1f93f148e2b",
              "title": "M3L3 C04 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "St9ftvMQ_ks",
                "china_cdn_id": "St9ftvMQ_ks.mp4"
              }
            },
            {
              "id": 682339,
              "key": "8d3e2c53-5567-4512-9642-fe2eb2307747",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Important Note\n---\nBefore moving on, make sure it's clear to you that the equation discussed in the video (and shown below) calculates an [expectation](https://en.wikipedia.org/wiki/Expected_value).  \n\n<div class=\"mathquill\">U(\\theta) = \\sum_\\tau \\mathbb{P}(\\tau;\\theta)R(\\tau)</div>\n\nTo see how it corresponds to the **expected return**, note that we've expressed the **return** <span class=\"mathquill\">R(\\tau)</span> as a function of the trajectory <span class=\"mathquill\">\\tau</span>.  Then, we calculate the weighted average (*where the weights are given by <span class=\"mathquill\">\\mathbb{P}(\\tau;\\theta)</span>*) of all possible values that the return <span class=\"mathquill\">R(\\tau)</span> can take.  \n\n## Why Trajectories?\n---\nYou may be wondering: _why are we using trajectories instead of episodes?_  The answer is that maximizing expected return over trajectories (instead of episodes) lets us search for optimal policies for both episodic *and continuing* tasks!\n\nThat said, for many episodic tasks, it often makes sense to just use the full episode.  In particular, for the case of the video game example described in the lessons, reward is only delivered at the end of the episode.  In this case, in order to estimate the expected return, the trajectory should correspond to the full episode; otherwise, we don't have enough reward information to meaningfully estimate the expected return.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 689011,
          "key": "152cf1ed-4a1d-449a-aa1d-8d86a656be38",
          "title": "REINFORCE",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "152cf1ed-4a1d-449a-aa1d-8d86a656be38",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 689590,
              "key": "3fc3f1cc-2bd4-401c-b1a0-393c06338002",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# REINFORCE",
              "instructor_notes": ""
            },
            {
              "id": 689591,
              "key": "731ffb18-313e-4135-b482-a754f29d647b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "You've learned that our goal is to find the values of the weights <span class=\"mathquill\">\\theta</span> in the neural network that maximize the expected return <span class=\"mathquill\">U</span>\n\n<div class=\"mathquill\">U(\\theta) = \\sum_\\tau P(\\tau;\\theta)R(\\tau)</div>\n\nwhere <span class=\"mathquill\">\\tau</span> is an arbitrary trajectory.  One way to determine the value of <span class=\"mathquill\">\\theta</span> that maximizes this function is through **gradient ascent**.  This algorithm is closely related to **gradient descent**, where the differences are that:\n- gradient descent is designed to find the **minimum** of a function, whereas gradient ascent will find the **maximum**, and \n- gradient descent steps in the direction of the **negative gradient**, whereas gradient ascent steps in the direction of the **gradient**.\n\nOur update step for gradient ascent appears as follows:\n\n<div class=\"mathquill\">\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta U(\\theta)</div>\n\nwhere <span class=\"mathquill\">\\alpha</span> is the step size that is generally allowed to decay over time.  Once we know how to calculate or estimate this gradient, we can repeatedly apply this update step, in the hopes that <span class=\"mathquill\">\\theta</span> converges to the value that maximizes <span class=\"mathquill\">U(\\theta)</span>.\n\n## Video\n---",
              "instructor_notes": ""
            },
            {
              "id": 797411,
              "key": "4a461f15-fbf8-4f43-9744-3a87a8bf9ef8",
              "title": "M3L3 C05 V2",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "o6CI2q3IXEs",
                "china_cdn_id": "o6CI2q3IXEs.mp4"
              }
            },
            {
              "id": 694663,
              "key": "649db1ba-2d1d-4d64-945c-d3cf03f97fcc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Pseudocode\n---\nThe algorithm described in the video is known as **REINFORCE**.  The pseudocode is summarized below.\n\n1. Use the policy <span class=\"mathquill\">\\pi_\\theta</span> to collect <span class=\"mathquill\">m</span> trajectories <span class=\"mathquill\">\\{ \\tau^{(1)}, \\tau^{(2)}, \\ldots, \\tau^{(m)}\\}</span> with horizon <span class=\"mathquill\">H</span>.  We refer to the <span class=\"mathquill\">i</span>-th trajectory as <div class=\"mathquill\">\\tau^{(i)} = (s_0^{(i)}, a_0^{(i)}, \\ldots, s_H^{(i)}, a_H^{(i)}, s_{H+1}^{(i)})</div>.\n2. Use the trajectories to estimate the gradient <span class=\"mathquill\">\\nabla_\\theta U(\\theta)</span>: <div class=\"mathquill\">\\nabla_\\theta U(\\theta) \\approx \\hat{g} := \\frac{1}{m}\\sum_{i=1}^m \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) R(\\tau^{(i)})</div>\n3. Update the weights of the policy: <div class=\"mathquill\">\\theta \\leftarrow \\theta + \\alpha \\hat{g}</div>\n4. Loop over steps 1-3.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 676156,
          "key": "a554a82c-87e5-43fc-9edf-ff0099ee054a",
          "title": "(Optional) Derivation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a554a82c-87e5-43fc-9edf-ff0099ee054a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 682348,
              "key": "3cc67b35-760f-48ef-adf0-9dcf6bd322ff",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b59f39e_grad-descent/grad-descent.gif",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3cc67b35-760f-48ef-adf0-9dcf6bd322ff",
              "caption": "Behavior of different optimizers for stochastic gradient descent. ([Source](http://ruder.io/optimizing-gradient-descent/))",
              "alt": "Behavior of different optimizers for stochastic gradient descent. ",
              "width": 500,
              "height": 246,
              "instructor_notes": null
            },
            {
              "id": 676177,
              "key": "aabe0e91-79e1-4371-bfac-87a4f3ca8b42",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# (Optional) Derivation",
              "instructor_notes": ""
            },
            {
              "id": 689023,
              "key": "b0396d9d-a659-4bf4-a592-4deaf0e49d0d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "If you'd like to learn how to derive the equation that we use to approximate the gradient, please read the text below.  Specifically, you'll learn how to derive\n\n<div class=\"mathquill\">\\nabla_\\theta U(\\theta) \\approx \\hat{g}= \\frac{1}{m}\\sum_{i=1}^m \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) R(\\tau^{(i)})</div>\n\nThis derivation is **optional** and can be safely skipped.",
              "instructor_notes": ""
            },
            {
              "id": 676211,
              "key": "42222120-39b3-47a4-8092-ca7f36204ff0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Likelihood Ratio Policy Gradient\n---\nWe'll begin by exploring how to calculate the gradient <span class=\"mathquill\">\\nabla_\\theta U(\\theta)</span>.  The calculation proceeds as follows:\n\n<div class=\"mathquill\">\\begin{aligned}\\nabla_\\theta U(\\theta) &= \\nabla_\\theta \\sum_\\tau P(\\tau;\\theta)R(\\tau) & (1)\\\\\n&=  \\sum_\\tau \\nabla_\\theta P(\\tau;\\theta)R(\\tau) & (2)\\\\\n&= \\sum_\\tau \\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)} \\nabla_\\theta P(\\tau;\\theta)R(\\tau) & (3)\\\\\n&= \\sum_\\tau P(\\tau;\\theta) \\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta)}R(\\tau) & (4)\\\\\n&= \\sum_\\tau P(\\tau;\\theta) \\nabla_\\theta \\log P(\\tau;\\theta) R(\\tau) & (5)\n\\end{aligned}</div>\n\nFirst, we note line (1) follows directly from <span class=\"mathquill\">U(\\theta) = \\sum_\\tau P(\\tau;\\theta)R(\\tau)</span>, where we've only taken the gradient of both sides.\n\nThen, we can get line (2) by just noticing that we can rewrite the gradient of the sum as the sum of the gradients.\n\nIn line (3), we only multiply every term in the sum by <span class=\"mathquill\">\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)}</span>, which is perfectly allowed because this fraction is equal to one!\n\nNext, line (4) is just a simple rearrangement of the terms from the previous line.  That is, <span class=\"mathquill\">\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)} \\nabla_\\theta P(\\tau;\\theta) = P(\\tau;\\theta) \\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta)}</span>.\n\nFinally, line (5) follows from the chain rule, and the fact that the gradient of the log of a function is always equal to the gradient of the function, divided by the function.  (_In case it helps to see this with simpler notation, recall that <span class=\"mathquill\">\\nabla_x \\log f(x) = \\frac{\\nabla_x f(x)}{f(x)}</span>._)  Thus, <span class=\"mathquill\">\\nabla_\\theta \\log P(\\tau;\\theta) = \\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta)}</span>.\n\nThe final \"trick\" that yields line (5) (i.e., <span class=\"mathquill\">\\nabla_\\theta \\log P(\\tau;\\theta) = \\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta)}</span>) is referred to as the **likelihood ratio trick** or **REINFORCE trick**.  \n\nLikewise, it is common to refer to the gradient as the **likelihood ratio policy gradient**: \n<div class=\"mathquill\">\\nabla_\\theta U(\\theta) = \\sum_\\tau P(\\tau;\\theta) \\nabla_\\theta \\log P(\\tau;\\theta) R(\\tau)</div>\n\nOnce weâ€™ve written the gradient as an expected value in this way, it becomes much easier to estimate. ",
              "instructor_notes": ""
            },
            {
              "id": 696224,
              "key": "67d3a9b5-0f5f-4209-b1f7-71c3da6cf093",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Sample-Based Estimate\n---\nIn the video on the previous page, you learned that we can approximate the likelihood ratio policy gradient with a sample-based average, as shown below:\n\n<div class=\"mathquill\">\\nabla_\\theta U(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^m \\nabla_\\theta \\log \\mathbb{P}(\\tau^{(i)};\\theta)R(\\tau^{(i)})</div>\n\nwhere each <span class=\"mathquill\">\\tau^{(i)}</span> is a sampled trajectory. ",
              "instructor_notes": ""
            },
            {
              "id": 689592,
              "key": "6360ed6f-3c7a-45b7-ba10-2ee8858407bd",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Finishing the Calculation\n---\nBefore calculating the expression above, we will need to further simplify <span class=\"mathquill\">\\nabla_\\theta \\log \\mathbb{P}(\\tau^{(i)};\\theta)</span>.  The derivation proceeds as follows:\n\n<div class=\"mathquill\">\\begin{aligned} \\nabla_\\theta \\log \\mathbb{P}(\\tau^{(i)};\\theta) &= \\nabla_\\theta \\log \\Bigg[ \\prod_{t=0}^{H} \\mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )\\pi_\\theta(a_t^{(i)}|s_t^{(i)})  \\Bigg] & (1)\\\\\n&= \\nabla_\\theta \\Bigg[ \\sum_{t=0}^{H} \\log \\mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} ) + \\sum_{t=0}^{H}\\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)})  \\Bigg] & (2)\\\\\n&= \\nabla_\\theta\\sum_{t=0}^{H} \\log \\mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} ) + \\nabla_\\theta \\sum_{t=0}^{H}\\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) & (3)\\\\\n&= \\nabla_\\theta \\sum_{t=0}^{H}\\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) & (4)\\\\\n&= \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) & (5)\n\\end{aligned}</div>\n\nFirst, line (1) shows how to calculate the probability of an arbitrary trajectory <span class=\"mathquill\">\\tau^{(i)}</span>.  Namely, <span class=\"mathquill\">\\mathbb{P}(\\tau^{(i)};\\theta) = \\prod_{t=0}^{H} \\mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )\\pi_\\theta(a_t^{(i)}|s_t^{(i)}) </span>, where we have to take into account the action-selection probabilities from the policy and the state transition dynamics of the MDP.\n\nThen, line (2) follows from the fact that the log of a product is equal to the sum of the logs.\n\nThen, line (3) follows because the gradient of the sum can be written as the sum of gradients.\n\nNext, line (4) holds, because <span class=\"mathquill\">\\sum_{t=0}^{H} \\log \\mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )</span> has no dependence on <span class=\"mathquill\">\\theta</span>, so <span class=\"mathquill\">\\nabla_\\theta\\sum_{t=0}^{H} \\log \\mathbb{P}(s_{t+1}^{(i)}|s_{t}^{(i)}, a_t^{(i)} )=0</span>.\n\nFinally, line (5) holds, because we can rewrite the gradient of the sum as the sum of gradients.\n\n## That's it!\n---\nPlugging in the calculation above yields the equation for estimating the gradient:\n<div class=\"mathquill\">\\nabla_\\theta U(\\theta) \\approx \\hat{g} = \\frac{1}{m}\\sum_{i=1}^m \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) R(\\tau^{(i)})</div>\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 676219,
          "key": "aa47ae9a-6879-4a8c-bf94-e7be3968895c",
          "title": "Coding Exercise",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "aa47ae9a-6879-4a8c-bf94-e7be3968895c",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 689128,
              "key": "3f733b04-8a93-44e2-9362-ebc6122ee2d4",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2017/September/59c73b1b_jupyter/jupyter.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/3f733b04-8a93-44e2-9362-ebc6122ee2d4",
              "caption": "",
              "alt": "",
              "width": 100,
              "height": 100,
              "instructor_notes": null
            },
            {
              "id": 689036,
              "key": "0ee75593-217a-4e07-817f-6c2948fa6547",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Coding Exercise",
              "instructor_notes": ""
            },
            {
              "id": 689126,
              "key": "d0cef941-17ec-4ecc-ba59-463bb6be02cc",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this exercise, you will use an implementation of REINFORCE to solve OpenAI Gym's CartPole environment.  \n\n**Note**: In the implementation, each trajectory corresponds to a full episode, and we collect <span class=\"mathquill\">m=1</span> trajectories.  You're strongly encouraged to refer to the pseudocode for REINFORCE while perusing the implementation.\n\nLater in the Nanodegree program, you will learn about some modifications that you can use to improve this algorithm.  You're strongly encouraged to implement these modifications, to get better performance!\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 676220,
          "key": "56a08e09-739f-450a-9414-5501cb5ea949",
          "title": "Workspace",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "56a08e09-739f-450a-9414-5501cb5ea949",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 689127,
              "key": "e4d298a6-7dd2-4e19-af40-26cd73efd799",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewBkgdKHogHQ",
              "pool_id": "jupytergpu",
              "view_id": "jupyter-HyuFBigHm",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/REINFORCE.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 694568,
          "key": "7a68236d-0671-40eb-8fb0-ac32601c7f8b",
          "title": "What's Next?",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7a68236d-0671-40eb-8fb0-ac32601c7f8b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 696232,
              "key": "7696f8a0-3a60-4ee8-b218-6398ac802793",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# What's Next?",
              "instructor_notes": ""
            },
            {
              "id": 696375,
              "key": "2bf47e6e-c215-4db0-adec-eab13146fee6",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "In this lesson, you've learned all about the REINFORCE algorithm, which was illustrated with a toy environment with a **_discrete_** action space.  But it's also important to mention that REINFORCE can also be used to solve environments with continuous action spaces!\n\nFor an environment with a continuous action space, the corresponding policy network could have an output layer that parametrizes a [continuous probability distribution](https://en.wikipedia.org/wiki/Probability_distribution#Continuous_probability_distribution).  \n\nFor instance, assume the output layer returns the mean <span class=\"mathquill\">\\mu</span> and variance <span class=\"mathquill\">\\sigma^2</span> of a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution).  ",
              "instructor_notes": ""
            },
            {
              "id": 698588,
              "key": "947d7f03-f346-40c3-8859-0752bc5a1aef",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/August/5b81ba57_350px-normal-distribution-pdf/350px-normal-distribution-pdf.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/947d7f03-f346-40c3-8859-0752bc5a1aef",
              "caption": "Probability density function corresponding to normal distribution (Source: Wikipedia)",
              "alt": "Probability density function corresponding to normal distribution (Source: Wikipedia)",
              "width": 350,
              "height": 224,
              "instructor_notes": null
            },
            {
              "id": 698589,
              "key": "0cdeffb9-9eff-4fc6-9f91-10ce19efa835",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Then in order to select an action, the agent needs only to pass the most recent state <span class=\"mathquill\">s_t</span> as input to the network, and then use the output mean <span class=\"mathquill\">\\mu</span> and variance <span class=\"mathquill\">\\sigma^2</span> to sample from the distribution <span class=\"mathquill\">a_t\\sim\\mathcal{N}(\\mu, \\sigma^2)</span>.\n\nThis should work in theory, but it's unlikely to perform well in practice!  To improve performance with continuous action spaces, we'll have to make some small modifications to the REINFORCE algorithm, and you'll learn more about these modifications in the upcoming lessons.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 676158,
          "key": "05bb4950-0839-476c-bdc1-442736026894",
          "title": "Summary",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "05bb4950-0839-476c-bdc1-442736026894",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 676159,
              "key": "75e7c81c-3c62-4591-b009-b86eeba63a4f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Summary",
              "instructor_notes": ""
            },
            {
              "id": 676226,
              "key": "d0ec8e60-094a-4b2a-90be-80a4cf0cea32",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/July/5b4e634b_screen-shot-2018-07-17-at-4.44.10-pm/screen-shot-2018-07-17-at-4.44.10-pm.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/d0ec8e60-094a-4b2a-90be-80a4cf0cea32",
              "caption": "REINFORCE increases the probability of \"good\" actions and decreases the probability of \"bad\" actions. ([Source](https://blog.openai.com/evolution-strategies/))",
              "alt": "REINFORCE increases the probability of \"good\" actions and decreases the probability of \"bad\" actions.",
              "width": 800,
              "height": 438,
              "instructor_notes": null
            },
            {
              "id": 676160,
              "key": "fd81e8a7-6c5a-4a63-818f-604a3034d72b",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "### What are Policy Gradient Methods?\n---\n- **Policy-based methods** are a class of algorithms that search directly for the optimal policy, without simultaneously maintaining value function estimates.\n- **Policy gradient methods** are a subclass of policy-based methods that estimate the weights of an optimal policy through gradient ascent.\n- In this lesson, we represent the policy with a neural network, where our goal is to find the weights <span class=\"mathquill\">\\theta</span> of the network that maximize expected return.\n\n### The Big Picture\n---\n- The policy gradient method will iteratively amend the policy network weights to:\n - make (state, action) pairs that resulted in positive return more likely, and\n - make (state, action) pairs that resulted in negative return less likely.\n\n### Problem Setup\n---\n- A **trajectory** <span class=\"mathquill\">\\tau</span> is a state-action sequence <span class=\"mathquill\">s_0, a_0, \\ldots, s_H, a_H, s_{H+1}</span>.\n- In this lesson, we will use the notation <span class=\"mathquill\">R(\\tau)</span> to refer to the return corresponding to trajectory <span class=\"mathquill\">\\tau</span>.\n- Our goal is to find the weights <span class=\"mathquill\">\\theta</span> of the policy network to maximize the **expected return** <span class=\"mathquill\">U(\\theta) := \\sum_\\tau \\mathbb{P}(\\tau;\\theta)R(\\tau)</span>.  \n\n### REINFORCE\n---\n- The pseudocode for REINFORCE is as follows:\n 1. Use the policy <span class=\"mathquill\">\\pi_\\theta</span> to collect <span class=\"mathquill\">m</span> trajectories <span class=\"mathquill\">\\{ \\tau^{(1)}, \\tau^{(2)}, \\ldots, \\tau^{(m)}\\}</span> with horizon <span class=\"mathquill\">H</span>.  We refer to the <span class=\"mathquill\">i</span>-th trajectory as <div class=\"mathquill\">\\tau^{(i)} = (s_0^{(i)}, a_0^{(i)}, \\ldots, s_H^{(i)}, a_H^{(i)}, s_{H+1}^{(i)})</div>.\n 2. Use the trajectories to estimate the gradient <span class=\"mathquill\">\\nabla_\\theta U(\\theta)</span>: <div class=\"mathquill\">\\nabla_\\theta U(\\theta) \\approx \\hat{g} := \\frac{1}{m}\\sum_{i=1}^m \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}|s_t^{(i)}) R(\\tau^{(i)})</div>\n 3. Update the weights of the policy: <div class=\"mathquill\">\\theta \\leftarrow \\theta + \\alpha \\hat{g}</div>\n 4. Loop over steps 1-3.\n\n### Derivation\n---\n- We derived the **likelihood ratio policy gradient**: <span class=\"mathquill\">\\nabla_\\theta U(\\theta) = \\sum_\\tau \\mathbb{P}(\\tau;\\theta)\\nabla_\\theta \\log \\mathbb{P}(\\tau;\\theta)R(\\tau) </span>.  \n- We can approximate the gradient above with a sample-weighted average: <div class=\"mathquill\">\\nabla_\\theta U(\\theta) \\approx \\frac{1}{m}\\sum_{i=1}^m \\nabla_\\theta \\log \\mathbb{P}(\\tau^{(i)};\\theta)R(\\tau^{(i)}) </div>.\n- We calculated the following: <div class=\"mathquill\">\\nabla_\\theta \\log \\mathbb{P}(\\tau^{(i)};\\theta) = \\sum_{t=0}^{H} \\nabla_\\theta \\log \\pi_\\theta (a_t^{(i)}|s_t^{(i)}) </div>.\n\n### What's Next?\n---\n- REINFORCE can solve Markov Decision Processes (MDPs) with either discrete or continuous action spaces.",
              "instructor_notes": ""
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}