{
  "data": {
    "lesson": {
      "id": 668595,
      "key": "e6ae0022-3c68-479a-8111-e264de5e34ab",
      "title": "Proximal Policy Optimization",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Learn what Proximal Policy Optimization (PPO) is and how it can improve policy gradients. Also learn how to implement the algorithm by training a computer to play the Atari Pong game.\n",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/e6ae0022-3c68-479a-8111-e264de5e34ab/668595/1545253300178/Proximal+Policy+Optimization+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/e6ae0022-3c68-479a-8111-e264de5e34ab/668595/1545253297276/Proximal+Policy+Optimization+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": null,
      "lab": null,
      "concepts": [
        {
          "id": 713945,
          "key": "80fadb64-a1ae-4b3e-ba84-d1fa4de31216",
          "title": "Instructor Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "80fadb64-a1ae-4b3e-ba84-d1fa4de31216",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 713947,
              "key": "f7378656-5171-4481-9986-75df77248162",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Instructor Introduction",
              "instructor_notes": ""
            },
            {
              "id": 797412,
              "key": "479d6bc3-e964-4ac7-a42b-b389bc7025a5",
              "title": "Instructor Introduction",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "sokSgNtGj9Y",
                "china_cdn_id": "sokSgNtGj9Y.mp4"
              }
            },
            {
              "id": 713949,
              "key": "b8a1ac93-00a3-4f7a-b175-4cb7ea5129b8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Hi, I'm Tim, nice to meet you all! I'm your new instructor for the deep reinforcement learning course. Before joining the Udacity team, I was a post-doctoral researcher in particle physics at UC Berkeley. \n\nIn fundamental physics, we borrow many tools from machine learning and AI to help us model complex interactions, and derive insights from the massive data generated by some of the biggest experiments on earth. \n\nFrom the smallest to the biggest, machine learning has helped us understand the properties of fundamental particles and the structure of the Universe.\n\nLooking forward to our lessons, I'm very excited to share with you some of the cutting edge developments in reinforcement learning. I hope you will have as much fun as I had creating these lesson materials! So, let's get started!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 713950,
          "key": "7c86c8b5-eaf8-4f4e-b204-ae62a99a935a",
          "title": "Lesson Preview",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "7c86c8b5-eaf8-4f4e-b204-ae62a99a935a",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 797413,
              "key": "b3dfc0db-1f6a-4955-9a5e-cb54bcc0f6ae",
              "title": "Training an agent to play atari-pong!",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "w27mvWFBnvQ",
                "china_cdn_id": "w27mvWFBnvQ.mp4"
              }
            },
            {
              "id": 713951,
              "key": "4644b90c-db93-4ce9-afe2-4c47cc785e82",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Lesson Preview\n\nState-of-the-art RL algorithms contain many important tweaks in addition to simple value-based or policy-based methods. One of these key improvements is called Proximal Policy Optimization (PPO) -- also closely related to Trust Region Policy Optimization (TRPO). It has allowed faster and more stable learning. From developing agile robots, to creating expert level gaming AI, PPO has proven useful in a wide domain of applications, and has become part of the standard toolkits in complicated learning environments.\n\nIn this lesson, we will first review the most basic policy gradient algorithm -- REINFORCE, and discuss issues associated with the algorithm. We will get an in-depth understanding of why these problems arise, and find ways to fix them. The solutions will lead us to PPO. Our lesson will focus on learning the intuitions behind why and how PPO improves learning, and implement it to teach a computer to play Atari-Pong, using only the pixels as input (see video below). Let's dive in!\n\n*The idea of PPO was published by the team at OpenAI, and you can read their paper through this [link](https://arxiv.org/abs/1707.06347)*\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 713953,
          "key": "344c4181-edec-4f98-bfd1-61ba0a861c3e",
          "title": "Beyond REINFORCE",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "344c4181-edec-4f98-bfd1-61ba0a861c3e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 713954,
              "key": "1bb0059e-f4d6-48d0-8b45-78f761a3c570",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Beyond REINFORCE\n\nHere, we briefly review key ingredients of the REINFORCE algorithm. \n\nREINFORCE works as follows: First, we initialize a random policy <span class=\"mathquill\">\\pi_\\theta(a;s)</span>, and using the policy we collect a trajectory -- or a list of (state, actions, rewards) at each time step:\n\n<div class=\"mathquill\">\ns_1, a_1, r_1, s_2, a_2, r_2, ...\n</div>\n\nSecond, we compute the total reward of the trajectory <span class=\"mathquill\">R=r_1+r_2+r_3+...</span>, and compute an estimate the gradient of the expected reward, <span class=\"mathquill\">g</span>:\n\n<div class=\"mathquill\">\ng = R \\sum_t \\nabla_\\theta \\log\\pi_\\theta(a_t|s_t)\n</div>\n\nThird, we update our policy using gradient ascent with learning rate <span class=\"mathquill\">\\alpha</span>:\n\n<div class=\"mathquill\">\n\\theta \\leftarrow \\theta + \\alpha g\n</div>\n\nThe process then repeats.",
              "instructor_notes": ""
            },
            {
              "id": 713955,
              "key": "00066866-a3de-4682-afb1-d2359baa3bce",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "What are the main problems of REINFORCE? There are three issues:\n\n1. The update process is very **inefficient**! We run the policy once, update once, and then throw away the trajectory.\n\n2. The gradient estimate <span class=\"mathquill\">g</span> is very **noisy**. By chance the collected trajectory may not be representative of the policy.\n\n3. There is no clear **credit assignment**. A trajectory may contain many good/bad actions and whether these actions are reinforced depends only on the final total output.\n\nIn the following concepts, we will go over ways to improve the REINFORCE algorithm and resolve all 3 issues. All of the improvements will be utilized and implemented in the PPO algorithm.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 713957,
          "key": "990d77ea-8ad7-48b4-aa68-a3b43fb876da",
          "title": "Noise Reduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "990d77ea-8ad7-48b4-aa68-a3b43fb876da",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 797414,
              "key": "6ec6f69a-8e81-4f72-a284-8bc9cac2e9d8",
              "title": "Noise Reduction",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "GCGqT2knFJ0",
                "china_cdn_id": "GCGqT2knFJ0.mp4"
              }
            },
            {
              "id": 713958,
              "key": "6d912d9c-34a4-4d11-b9bb-4ad85e9136f3",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Noise Reduction\n\nThe way we optimize the policy is by maximizing the average rewards <span class=\"mathquill\">U(\\theta)</span>. To do that we use stochastic gradient ascent. Mathematically, the gradient is given by an average over all the possible trajectories, \n\n<div class=\"mathquill\">\n\\nabla_\\theta U(\\theta)\n= \n\\overbrace{\\sum_\\tau P(\\tau; \\theta)}^{\n\\begin{matrix}\n\\scriptsize\\textrm{average over}\\\\\n\\scriptsize\\textrm{all trajectories}\n\\end{matrix}\n}\n\\underbrace{\\left(\nR_\\tau \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t^{(\\tau)}|s_t^{(\\tau)})\n\\right)}_{\n\\textrm{only one is sampled}\n}\n</div>\n\nThere could easily be well over millions of trajectories for simple problems, and infinite for continuous problems.\n\nFor practical purposes, we simply take one trajectory to compute the gradient, and update our policy. So a lot of times, the result of a sampled trajectory comes down to chance, and doesn't contain that much information about our policy. How does learning happen then? The hope is that after training for a long time, the tiny signal accumulates.\n\n\n\nThe easiest option to reduce the noise in the gradient is to simply sample more trajectories! Using distributed computing, we can collect multiple trajectories in parallel, so that it won’t take too much time. Then we can estimate the policy gradient by averaging across all the different trajectories\n\n<div class=\"mathquill\">\n\\left.\n\\begin{matrix}\ns^{(1)}_t, a^{(1)}_t, r^{(1)}_t\\\\[6pt]\ns^{(2)}_t, a^{(2)}_t, r^{(2)}_t\\\\[6pt]\ns^{(3)}_t, a^{(3)}_t, r^{(3)}_t\\\\[6pt]\n\\vdots  \n\\end{matrix}\n\\;\\;\n\\right\\}\\!\\!\\!\\!\n\\rightarrow\ng = \\frac{1}{N}\\sum_{i=1}^N \nR_i \\sum_t\\nabla_\\theta \\log \\pi_\\theta(a^{(i)}_t | s^{(i)}_t)\n</div>\n\n\n",
              "instructor_notes": ""
            },
            {
              "id": 713960,
              "key": "c6dc8dea-d161-4de5-8616-a96b1c92f194",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Rewards Normalization\n\nThere is another bonus for running multiple trajectories: we can collect all the total rewards and get a sense of how they are distributed.\n\nIn many cases, the distribution of rewards shifts as learning happens. Reward = 1 might be really good in the beginning, but really bad after 1000 training episode. \n\nLearning can be improved if we normalize the rewards, where <span class=\"mathquill\">\\mu</span> is the mean, and <span class=\"mathquill\">\\sigma</span> the standard deviation.\n\n\n<div class=\"mathquill\">\nR_i \\leftarrow\n\\frac{R_i -\\mu}{\\sigma}\n\\qquad\n\\mu = \\frac{1}{N}\\sum_i^N R_i\n\\qquad\n\\sigma = \\sqrt{\\frac{1}{N}\\sum_i (R_i - \\mu)^2}\n</div>\n\n(when all the <span class=\"mathquill\">R_i</span> are the same, <span class=\"mathquill\">\\sigma =0</span>, we can set all the normalized rewards to 0 to avoid numerical problems)\n\nThis batch-normalization technique is also used in many other problems in AI (e.g. image classification), where normalizing the input can improve learning.\n\nIntuitively, normalizing the rewards roughly corresponds to picking half the actions to encourage/discourage, while also making sure the steps for gradient ascents are not too large/small.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 713963,
          "key": "8d616ffb-989a-48ba-aed7-8494b527eaff",
          "title": "Credit Assignment",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8d616ffb-989a-48ba-aed7-8494b527eaff",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 714899,
              "key": "1b26af43-ffe4-4800-a938-f5da40316faa",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Credit Assignment",
              "instructor_notes": ""
            },
            {
              "id": 797415,
              "key": "d99773d2-77fe-42bd-8888-6efd292823ee",
              "title": "Credit Assignment",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "tfZw1moB25Y",
                "china_cdn_id": "tfZw1moB25Y.mp4"
              }
            },
            {
              "id": 713964,
              "key": "facfb809-5b10-4fdf-9bc9-10e89386eb53",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Going back to the gradient estimate, we can take a closer look at the total reward <span class=\"mathquill\">R</span>, which is just a sum of reward at each step <span class=\"mathquill\">R=r_1+r_2+...+r_{t-1}+r_t+...</span> \n\n<div class=\"mathquill\">\ng=\\sum_t (...+r_{t-1}+r_{t}+...)\\nabla_{\\theta}\\log \\pi_\\theta(a_t|s_t)\n</div>\n\nLet’s think about what happens at time-step <span class=\"mathquill\">t</span>. Even before an action is decided, the agent has already received all the rewards up until step <span class=\"mathquill\">t-1</span>. So we can think of that part of the total reward as the reward from the past. The rest is denoted as the future reward. \n\n<div class=\"mathquill\">\n(\\overbrace{...+r_{t-1}}^{\\cancel{R^{\\rm past}_t}}+\n\\overbrace{r_{t}+...}^{R^{\\rm future}_t})\n</div>\n\nBecause we have a Markov process, the action at time-step <span class=\"mathquill\">t</span> can only affect the future reward, so the past reward shouldn’t be contributing to the policy gradient. \nSo to properly assign credit to the action <span class=\"mathquill\">a_t</span>, we should ignore the past reward. So a better policy gradient would simply have the future reward as the coefficient .\n\n<div class=\"mathquill\">\ng=\\sum_t R_t^{\\rm future}\\nabla_{\\theta}\\log \\pi_\\theta(a_t|s_t)\n</div>",
              "instructor_notes": ""
            },
            {
              "id": 715107,
              "key": "3b44f6dc-dc13-4c48-b62f-b533466c1a24",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Notes on Gradient Modification\n\nYou might wonder, why is it okay to just change our gradient? Wouldn't that change our original goal of maximizing the expected reward?\n\nIt turns out that mathematically, ignoring past rewards might change the gradient for each specific trajectory, but it doesn't change the **averaged** gradient. So even though the gradient is different during training, on average we are still maximizing the average reward. In fact, the resultant gradient is less noisy, so training using future reward should speed things up!",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 713971,
          "key": "ab1b60b5-605f-49cb-9077-61c6a5e12cf1",
          "title": "Policy Gradient Quiz",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "ab1b60b5-605f-49cb-9077-61c6a5e12cf1",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 714709,
              "key": "681a858b-3d92-41c9-b7de-ef68b7571fa0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Policy Gradient Quiz\n\nSuppose we are training an agent to play a computer game. There are only two possible action: \n\n0 = Do nothing, \n1 = Move \n\nThere are three time-steps in each game, and our policy is completely determined by one parameter <span class=\"mathquill\">\\theta</span>, such that the probability of \"moving\" is <span class=\"mathquill\">\\theta</span>, and the probability of doing nothing is <span class=\"mathquill\">1-\\theta</span>\n\nInitially <span class=\"mathquill\">\\theta=0.5</span>. Three games are played, the results are:\n\nGame 1:\nactions: (1,0,1)\nrewards: (1,0,1)\n\nGame 2:\nactions: (1,0,0)\nrewards: (0,0,1)\n\nGame 3:\nactions: (0,1,0)\nrewards: (1,0,1)\n",
              "instructor_notes": ""
            },
            {
              "id": 714710,
              "key": "8870cb79-05ac-4fda-8111-7881cc8ef049",
              "title": "Computing policy gradient",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "8870cb79-05ac-4fda-8111-7881cc8ef049",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What are the future rewards for the first game? \n\nRecall the results for game 1 are: actions: (1,0,1)\nrewards: (1,0,1)\n",
                "answers": [
                  {
                    "id": "a1536948177377",
                    "text": "(1,0,1)",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948199614",
                    "text": "(1,0,2)",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948200278",
                    "text": "(2,0,1)",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948200830",
                    "text": "(2,1,1)",
                    "is_correct": true
                  },
                  {
                    "id": "a1536948201430",
                    "text": "(1,1,2)",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 714711,
              "key": "745b5543-365b-41be-95b6-51a00d98911c",
              "title": "",
              "semantic_type": "RadioQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "745b5543-365b-41be-95b6-51a00d98911c",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "What is the policy gradient computed from the second game, using future rewards?\n\nactions: (1,0,0)\nrewards: (0,0,1)\n\n",
                "answers": [
                  {
                    "id": "a1536948295902",
                    "text": "-2",
                    "is_correct": true
                  },
                  {
                    "id": "a1536948306948",
                    "text": "-1",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948307572",
                    "text": "0",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948308132",
                    "text": "1",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948308691",
                    "text": "2",
                    "is_correct": false
                  }
                ]
              }
            },
            {
              "id": 714712,
              "key": "28145bb8-f3e1-44fc-a8d7-fdf9d35fea9b",
              "title": "",
              "semantic_type": "CheckboxQuizAtom",
              "is_public": true,
              "instructor_notes": null,
              "user_state": {
                "node_key": "28145bb8-f3e1-44fc-a8d7-fdf9d35fea9b",
                "completed_at": null,
                "last_viewed_at": null,
                "unstructured": null
              },
              "question": {
                "prompt": "Which of these statements are true regarding the 3rd game? Recall the results for the 3rd game are:\n\nactions: (0,1,0)\nrewards: (1,0,1)\n",
                "answers": [
                  {
                    "id": "a1536948352309",
                    "text": "We can add a baseline -1 point to the rewards, the computed gradient wouldn't change",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948376233",
                    "text": "The contribution to the gradient from the second and third steps cancel each other",
                    "is_correct": true
                  },
                  {
                    "id": "a1536948376802",
                    "text": "The computed policy gradient from this game is 0",
                    "is_correct": false
                  },
                  {
                    "id": "a1536948377370",
                    "text": "The computed policy gradient from this game is negative",
                    "is_correct": true
                  },
                  {
                    "id": "a1536948377971",
                    "text": "Using the total reward vs future reward give the same policy gradient in this game",
                    "is_correct": true
                  }
                ]
              }
            }
          ]
        },
        {
          "id": 714880,
          "key": "b25ecd29-368f-4a18-b1cb-76e77ff1e299",
          "title": "pong with REINFORCE (code walkthrough)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b25ecd29-368f-4a18-b1cb-76e77ff1e299",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 714894,
              "key": "50eb3f02-c8ec-4b9a-967a-fa23cf2b2492",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Pong with REINFORCE (code walkthrough)\n\n",
              "instructor_notes": ""
            },
            {
              "id": 797416,
              "key": "cfa963a0-b880-4b9a-a656-91e5f06ddd25",
              "title": "pong with REINFORCE walkthrough",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "eKIjPrQWIgg",
                "china_cdn_id": "eKIjPrQWIgg.mp4"
              }
            },
            {
              "id": 723072,
              "key": "3d133d74-18c4-442b-a3f2-df647e9ad8b8",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Additional Notes\n* **The performance for the REINFORCE version may be poor. You can try training with a smaller t<sub>max</sub>=100 and more number of episodes=2000 to see concrete results.**\n* Try normalizing your future rewards over all the parallel agents, it can speed up training\n* Simpler networks might perform better than more complicated ones! The original input contains 80x80x2=12800 numbers, you might want to ensure that this number steadily decreases at each layer of the neural net.\n* Training performance may be significantly *worse* on local machines. I had worse performance training on my own windows desktop with a 4-core CPU and a GPU. This may be due to the slightly different ways the emulator is rendered. So please run the code on the workspace first before moving locally\n* It may be beneficial to train multiple epochs, say first using a small t<sub>max</sub>=200 with 500 episodes, and then train again with t<sub>max</sub> = 400 with 500 episodes, and then finally with a even larger t<sub>max</sub>.\n* Remember to save your policy after training! \n* for a challenge, try the 'Pong-v4' environment, this includes random frameskips and takes longer to train.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 714422,
          "key": "3a62f539-aa29-42fc-8a81-b3c5bc94654e",
          "title": "pong with REINFORCE (workspace)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "3a62f539-aa29-42fc-8a81-b3c5bc94654e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 714691,
              "key": "d5761f38-e6b3-41d9-832e-b1d1a3adbb30",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewehwvczpnkcj",
              "pool_id": "jupytergpu",
              "view_id": "jupyter-sdq7i8affws",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/pong-REINFORCE.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        },
        {
          "id": 713972,
          "key": "811cf82e-c682-43c5-9133-5b368a3602c5",
          "title": "Importance Sampling",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "811cf82e-c682-43c5-9133-5b368a3602c5",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 715086,
              "key": "1be260e4-b002-44bc-8bb5-44e2720ac0d4",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Importance Sampling",
              "instructor_notes": ""
            },
            {
              "id": 797417,
              "key": "b5f2667e-4191-4e36-b85d-4f322a71e9b9",
              "title": "Importance Sampling",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "cYPvWriOPIk",
                "china_cdn_id": "cYPvWriOPIk.mp4"
              }
            },
            {
              "id": 714385,
              "key": "b4035a20-10a7-4146-9f00-76b318ff90df",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## 1. Policy Update in REINFORCE\n\nLet’s go back to the REINFORCE algorithm. We start with a policy, <span class=\"mathquill\">\\pi_\\theta</span>, then using that policy, we generate a trajectory (or multiple ones to reduce noise) <span class=\"mathquill\">(s_t, a_t, r_t)</span>. Afterward, we compute a policy gradient, <span class=\"mathquill\">g</span>, and update <span class=\"mathquill\">\\theta' \\leftarrow \\theta + \\alpha g</span>.\n\nAt this point, the trajectories we’ve just generated are simply thrown away. If we want to update our policy again, we would need to generate new trajectories once more, using the updated policy.\n\nYou might ask, why is all this necessary? It’s because we need to compute the gradient for the current policy, and to do that the trajectories need to be representative of the current policy.\n\nBut this sounds a little wasteful. What if we could somehow recycle the old trajectories, by modifying them so that they are representative of the new policy? So that instead of just throwing them away, we recycle them!\n\nThen we could just reuse the recycled trajectories to compute gradients, and to update our policy, again, and again. This would make updating the policy a lot more efficient. \nSo, how exactly would that work?\n",
              "instructor_notes": ""
            },
            {
              "id": 714386,
              "key": "c7138fbe-ab58-4659-a3c9-b13dda9af38c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## 2. Importance Sampling\n\nThis is where importance sampling comes in. Let’s look at the trajectories we generated using the policy <span class=\"mathquill\"> \\pi_\\theta</span>. It had a probability <span class=\"mathquill\"> P(\\tau;\\theta)</span>, to be sampled.\n\nNow Just by chance, the same trajectory can be sampled under the new policy, with a different probability <span class=\"mathquill\"> P(\\tau;\\theta')</span>\n\nImagine we want to compute the average of some quantity, say <span class=\"mathquill\"> f(\\tau)</span>. We could simply generate trajectories from the new policy, compute  <span class=\"mathquill\"> f(\\tau)</span> and average them. \n\nMathematically, this is equivalent to adding up all the <span class=\"mathquill\"> f(\\tau)</span>, weighted by a probability of sampling each trajectory under the new policy. \n\n<div class=\"mathquill\">\n\\sum_\\tau P(\\tau;\\theta') f(\\tau)\n</div>\n\nNow we could modify this equation, by multiplying and dividing by the same number, <span class=\"mathquill\"> P(\\tau;\\theta)</span> and rearrange the terms.\n\n<div class=\"mathquill\">\n\\sum_\\tau \\overbrace{P(\\tau;\\theta)}^{\n\\begin{matrix}\n\\scriptsize\n\\textrm{sampling under}\\\\\n\\scriptsize\n\\textrm{old policy }\n\\pi_\\theta\n\\end{matrix}\n} \n\\overbrace{\\frac{P(\\tau;\\theta')}{P(\\tau;\\theta)}}^{\n\\begin{matrix}\n\\scriptsize \n\\textrm{re-weighting}\\\\\n\\scriptsize\n\\textrm{factor}\n\\end{matrix}\n}\n f(\\tau)\n</div>\n\nIt doesn’t look we’ve done much. But written in this way, we can reinterpret the first part as the coefficient for sampling under the old policy, with an extra re-weighting factor, in addition to just averaging.\n\nIntuitively, this tells us we can use old trajectories for computing averages for new policy, as long as we add this extra re-weighting factor, that takes into account how under or over–represented each trajectory is under the new policy compared to the old one.\n\nThe same tricks are used frequently across statistics, where the re-weighting factor is included to un-bias surveys and voting predictions. ",
              "instructor_notes": ""
            },
            {
              "id": 714387,
              "key": "0d29e232-a146-4cb7-942e-f305785032ec",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## 3. The re-weighting factor\n\nNow Let’s a closer look at the re-weighting factor. \n\n<div class=\"mathquill\">\n\\frac{P(\\tau;\\theta')}{P(\\tau;\\theta)}\n=\\frac\n{\\pi_{\\theta'}(a_1|s_1)\\, \\pi_{\\theta'}(a_2|s_2)\\, \\pi_{\\theta'}(a_3|s_3)\\,...}\n{\\pi_\\theta(a_1|s_1) \\, \\pi_\\theta(a_2|s_2)\\, \\pi_\\theta(a_2|s_2)\\, ...}\n</div>\n\nBecause each trajectory contains many steps, the probability contains a chain of products of each policy at different time-step.\n\nThis formula is a bit complicated. But there is a bigger problem. When some of policy gets close to zero, the re-weighting factor can become close to zero, or worse, close to 1 over 0 which diverges to infinity.\n \nWhen this happens, the re-weighting trick becomes unreliable. So, In practice, we want to make sure the re-weighting factor is not too far from 1 when we utilize importance sampling\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 714388,
          "key": "9457a4c7-953e-43f2-9be7-c85277768def",
          "title": "PPO part 1- The Surrogate Function",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "9457a4c7-953e-43f2-9be7-c85277768def",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 715087,
              "key": "d0e608f8-1905-4df8-a951-4ab408e4df1d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# PPO Part 1: The Surrogate Function",
              "instructor_notes": ""
            },
            {
              "id": 797418,
              "key": "0864d675-e12d-40e5-b55f-b81a65c2af8f",
              "title": "PPO Part 1: The Surrogate Function",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "Y-boYZlpO7g",
                "china_cdn_id": "Y-boYZlpO7g.mp4"
              }
            },
            {
              "id": 714390,
              "key": "8f506dbc-70ca-4056-a63c-7183880a06c0",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Re-weighting the Policy Gradient\n\nSuppose we are trying to update our current policy, <span class=\"mathquill\">\\pi_{\\theta'}</span>. To do that, we need to estimate a gradient, <span class=\"mathquill\">g</span>. But we only have trajectories generated by an older policy <span class=\"mathquill\">\\pi_{\\theta}</span>. How do we compute the gradient then?\n\nMathematically, we could utilize importance sampling. The answer just what a normal policy gradient would be, times a re-weighting factor <span class=\"mathquill\">P(\\tau;\\theta')/P(\\tau;\\theta)</span>:\n\n<div class=\"mathquill\">\ng=\\frac{P(\\tau; \\theta')}{P(\\tau; \\theta)}\\sum_t \n\\frac{\\nabla_{\\theta'} \\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)}R_t^{\\rm future}\n</div>\n\nWe can rearrange these equations, and the re-weighting factor is just the product of all the policy across each step -- I’ve picked out the terms at time-step <span class=\"mathquill\">t</span> here. We can cancel some terms, but we're still left with a product of the policies at different times, denoted by \"<span class=\"mathquill\">...</span>\".\n\n<div class=\"mathquill\">\ng=\\sum_t\n\\frac{...\\, \\cancel{\\pi_{\\theta'}(a_t|s_t)} \\,...}\n{...\\,\\pi_{\\theta}(a_t|s_t)\\,...}\n\\,\n\\frac{\\nabla_{\\theta'} \\pi_{\\theta'}(a_t|s_t)}{\\cancel{\\pi_{\\theta'}(a_t|s_t)}}R_t^{\\rm future}\n</div>\n\nCan we simplify this expression further? This is where proximal policy comes in. If the old and current policy is close enough to each other, all the factors inside the \"<span class=\"mathquill\">...</span>\" would be pretty close to 1, and then we can ignore them.\n\nThen the equation simplifies\n\n<div class=\"mathquill\">\ng=\\sum_t\n\\frac{\\nabla_{\\theta'} \\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}R_t^{\\rm future}\n</div>\n\nIt looks very similar to the old policy gradient. In fact, if the current policy and the old policy is the same, we would have exactly the vanilla policy gradient. But remember, this expression is different because we are comparing two *different* policies\n",
              "instructor_notes": ""
            },
            {
              "id": 714392,
              "key": "25f1dfe6-5a03-41a3-8cae-743ff98f8af5",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## The Surrogate Function\n\nNow that we have the approximate form of the gradient, we can think of it as the gradient of a new object, called the surrogate function\n\n<div class=\"mathquill\">\ng=\\nabla_{\\theta'} L_{\\rm sur}(\\theta', \\theta)\n</div>\n\n\n<div class=\"mathquill\">\nL_{\\rm sur}(\\theta', \\theta)=\n\\sum_t\n\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}R_t^{\\rm future}\n</div>\n\nSo using this new gradient, we can perform gradient ascent to update our policy -- which can be thought as directly maximize the surrogate function. \n\nBut there is still one important issue we haven’t addressed yet. If we keep reusing old trajectories and updating our policy, at some point the new policy might become different enough from the old one, so that all the approximations we made could become invalid. \n\nWe need to find a way make sure this doesn’t happen. Let’s see how in part 2.\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 714389,
          "key": "4a6fa90a-eeb1-4412-ac1b-35d6ca4f3809",
          "title": "PPO part 2- Clipping Policy Updates",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "4a6fa90a-eeb1-4412-ac1b-35d6ca4f3809",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 714391,
              "key": "e1abef08-5962-49a8-ada1-9d9e6bcb942d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# PPO Part 2: Clipping Policy Updates",
              "instructor_notes": ""
            },
            {
              "id": 797419,
              "key": "3a0c04ee-c45f-442d-9643-9759474f940a",
              "title": "PPO Part 2: Clipping Policy Updates",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "NRzjGGX6c34",
                "china_cdn_id": "NRzjGGX6c34.mp4"
              }
            },
            {
              "id": 715089,
              "key": "3ab114ea-a6c2-40c1-a575-41d5f76c10bb",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## The Policy/Reward Cliff\n\nWhat is the problem with updating our policy and ignoring the fact that the approximations are not valid anymore? One problem is it could lead to a really bad policy that is very hard to recover from. Let's see how:\n\n",
              "instructor_notes": ""
            },
            {
              "id": 714394,
              "key": "701e9642-4db9-41b9-8295-af085a7298cc",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5b9a9625_policy-reward-cliff/policy-reward-cliff.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/701e9642-4db9-41b9-8295-af085a7298cc",
              "caption": "",
              "alt": "",
              "width": 1003,
              "height": 412,
              "instructor_notes": null
            },
            {
              "id": 714395,
              "key": "db40b765-98d7-4d7a-845f-34fe8f6bde98",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Say we have some policy parameterized by <span class=\"mathquill\">\\pi_{\\theta'}</span> (shown on the left plot in black), and with an average reward function (shown on the right plot in black). \n\nThe current policy is labelled by the red text, and the goal is to update the current policy to the optimal one (green star). To update the policy we can compute a surrogate function <span class=\"mathquill\">L_{\\rm sur}</span>  (dotted-red curve on right plot). So <span class=\"mathquill\">L_{\\rm sur}</span> approximates the reward pretty well around the current policy. But far away from the current policy, it diverges from the actual reward.\n\nIf we continually update the policy by performing gradient ascent, we might get something like the red-dots. The big problem is that at some point we hit a cliff, where the policy changes by a large amount. From the perspective of the surrogate function, the average reward is really great. But the actually average reward is really bad! \n\nWhat’s worse, the policy is now stuck in a deep and flat bottom, so that future updates won’t be able to bring the policy back up! we are now stuck with a really bad policy.\n\nHow do we fix this? Wouldn’t it be great if we can somehow stop the gradient ascent so that our policy doesn’t fall off the cliff?",
              "instructor_notes": ""
            },
            {
              "id": 714401,
              "key": "c10d37f5-bee8-4b9e-832c-a55bd2f1247d",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Clipped Surrogate Function",
              "instructor_notes": ""
            },
            {
              "id": 714402,
              "key": "a2fa078c-8cdd-40cd-a826-ef132d29d84d",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5b9a99cd_clipped-surrogate/clipped-surrogate.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/a2fa078c-8cdd-40cd-a826-ef132d29d84d",
              "caption": "",
              "alt": "",
              "width": 994,
              "height": 419,
              "instructor_notes": null
            },
            {
              "id": 714400,
              "key": "b7065fec-84cb-4401-b98f-1ad31585d22f",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "Here’s an idea: what if we just flatten the surrogate function (blue curve)? What would policy update look like then?\n\nSo starting with the current policy (blue dot), we apply gradient ascent. The updates remain the same, until we hit the flat plateau. Now because the reward function is flat, the gradient is zero, and the policy update will stop!\n\nNow, keep in mind that we are only showing a 2D figure with one <span class=\"mathquill\">\\theta'</span> direction. In most cases, there are thousands of parameters in a policy, and there may be hundreds/thousands of high-dimensional cliffs in many different directions. We need to apply this clipping mathematically so that it will automatically take care of all the cliffs. \n",
              "instructor_notes": ""
            },
            {
              "id": 714408,
              "key": "ff8a672d-502b-4ae0-94dc-5de858dee93c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Clipped Surrogate Function\n\nHere's the formula that will automatically flatten our surrogate function to avoid all the cliffs:\n\n<div class=\"mathquill\">\nL^{\\rm clip}_{\\rm sur}\n(\\theta',\\theta)=\n\\sum_t \\min\\left\\{\n\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}R_t^{\\rm future}\n, \n{\\rm clip}_\\epsilon\\!\\!\n\\left(\n\\frac{\\pi_{\\theta'}(a_t|s_t)}\n{\\pi_{\\theta}(a_t|s_t)}\n\\right)\nR_t^{\\rm future}\n\\right\\}\n</div>\n\nNow let’s dissect the formula by looking at one specific term in the sum, and set the future reward to 1 to make things easier.\n",
              "instructor_notes": ""
            },
            {
              "id": 714417,
              "key": "f77fe041-565d-4082-8b26-e1e9c400ce8b",
              "title": null,
              "semantic_type": "ImageAtom",
              "is_public": true,
              "url": "https://video.udacity-data.com/topher/2018/September/5b9a9d58_clipped-surrogate-explained/clipped-surrogate-explained.png",
              "non_google_url": "https://s3.cn-north-1.amazonaws.com.cn/u-img/f77fe041-565d-4082-8b26-e1e9c400ce8b",
              "caption": "",
              "alt": "",
              "width": 743,
              "height": 375,
              "instructor_notes": null
            },
            {
              "id": 714414,
              "key": "fd80cd2c-8096-4234-b7c1-638d5afe462c",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "We start with the original surrogate function (red), which involves the ratio <span class=\"mathquill\">\\pi_{\\theta'}(a_t|s_t)/\\pi_\\theta(a_t|s_t)</span>. The black dot shows the location where the current policy is the same as the old policy (<span class=\"mathquill\">\\theta'=\\theta</span>)\n\nWe want to make sure the two policy is similar, or that the ratio is close to 1. So we choose a small <span class=\"mathquill\">\\epsilon</span> (typically 0.1 or 0.2), and apply the <span class=\"mathquill\">{\\rm clip}</span> function to force the ratio to be within the interval <span class=\"mathquill\">[1-\\epsilon,1+\\epsilon]</span> (shown in purple).\n\nNow the ratio is clipped in two places. But we only want to clip the top part and not the bottom part. To do that, we compare this clipped ratio to the original one and take the minimum (show in blue). This then ensures the clipped surrogate function is always less than the original surrogate function <span class=\"mathquill\">L_{\\rm sur}^{\\rm clip}\\le L_{\\rm sur}</span>, so the clipped surrogate function gives a more conservative \"reward\".\n\n(*the blue and purple lines are shifted slightly for easier viewing*)",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 714688,
          "key": "b56d3e41-1804-4575-9e46-d0a90da98b4b",
          "title": "PPO summary",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b56d3e41-1804-4575-9e46-d0a90da98b4b",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 714748,
              "key": "e5cc4b6c-227c-44d9-92e2-2798208dc6ea",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# PPO Summary",
              "instructor_notes": ""
            },
            {
              "id": 797420,
              "key": "b3ab82be-7e84-4af3-987f-e40338b33505",
              "title": "TLPPO Summary V1",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "qRAUAAWA_kc",
                "china_cdn_id": "qRAUAAWA_kc.mp4"
              }
            },
            {
              "id": 714689,
              "key": "0c1eb762-1dab-44fe-9f52-87085e557ac2",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "So that’s it! We can finally summarize the PPO algorithm\n\n1. First, collect some trajectories based on some policy <span class=\"mathquill\">\\pi_\\theta</span>, and initialize theta prime <span class=\"mathquill\">\\theta'=\\theta</span>\n2. Next, compute the gradient of the clipped surrogate function using the trajectories\n3. Update <span class=\"mathquill\">\\theta'</span> using gradient ascent <span class=\"mathquill\">\\theta'\\leftarrow\\theta' +\\alpha \\nabla_{\\theta'}L_{\\rm sur}^{\\rm clip}(\\theta', \\theta)</span>\n4. Then we repeat step 2-3 without generating new trajectories. Typically, step 2-3 are only repeated a few times\n5. Set <span class=\"mathquill\">\\theta=\\theta'</span>, go back to step 1, repeat.\n\n\n*The details of PPO was originally published by the team at OpenAI, and you can read their paper through this [link](https://arxiv.org/abs/1707.06347)*\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 714888,
          "key": "5a759a51-d665-4959-a492-5e60c8886b28",
          "title": "pong with PPO (code walkthrough)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "5a759a51-d665-4959-a492-5e60c8886b28",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 715093,
              "key": "8b8f6c0d-69df-473d-9579-99c9c6967663",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "# Pong with PPO (code walkthrough)\n",
              "instructor_notes": ""
            },
            {
              "id": 797421,
              "key": "534b412d-23ae-44af-8ec7-370f8f6505d2",
              "title": "Pong with PPO walkthrough",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "XhfhR7Z01S0",
                "china_cdn_id": "XhfhR7Z01S0.mp4"
              }
            },
            {
              "id": 723071,
              "key": "642c4fa8-cb5d-4259-84d7-d4d910138a17",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Additional Notes\n\n* Try normalizing your future rewards over all the parallel agents, it can speed up training\n* Simpler networks might perform better than more complicated ones! The original input contains 80x80x2=12800 numbers, you might want to ensure that this number steadily decreases at each layer of the neural net.\n* Training performance may be significantly *worse* on local machines. I had worse performance training on my own windows desktop with a 4-core CPU and a GPU. This may be due to the slightly different ways the emulator is rendered. So please run the code on the workspace first before moving locally\n* It may be beneficial to train multiple epochs, say first using a small t<sub>max</sub>=200 with 500 episodes, and then train again with t<sub>max</sub> = 400 with 500 episodes, and then finally with a even larger t<sub>max</sub>.\n* Remember to save your policy after training! \n* for a challenge, try the 'Pong-v4' environment, this includes random frameskips and takes longer to train.",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 714423,
          "key": "6889aabc-de5a-4118-89e6-966891b70c27",
          "title": "pong with PPO (workspace)",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "6889aabc-de5a-4118-89e6-966891b70c27",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 714743,
              "key": "b9e10731-7265-4d02-bf68-2d533ed17e5d",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewehwvczpnkcj",
              "pool_id": "jupytergpu",
              "view_id": "jupyter-3f4civ7myon",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/pong-PPO.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        }
      ]
    }
  },
  "_deprecated": [
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    },
    {
      "name": "non_google_url",
      "reason": "(2016/8/18) Not sure, ask i18n team for reason"
    }
  ]
}