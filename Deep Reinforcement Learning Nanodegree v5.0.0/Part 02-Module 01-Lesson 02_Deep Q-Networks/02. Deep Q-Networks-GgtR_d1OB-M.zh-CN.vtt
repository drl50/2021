WEBVTT
Kind: captions
Language: zh-CN

00:00:03.229 --> 00:00:07.500
在 2015 年 DeepMind 取得了突破性的成果

00:00:07.500 --> 00:00:11.684
他们设计了一个可以学会玩视频游戏的智能体 并且比人类玩家的能力更强

00:00:11.685 --> 00:00:15.179
如果你知道电子游戏 “Pong” 的底层游戏状态

00:00:15.179 --> 00:00:18.660
或许可以轻松地编写一个玩这个游戏的程序

00:00:18.660 --> 00:00:21.135
例如球的位置 挡板等

00:00:21.135 --> 00:00:24.554
但是这个智能体只能获得原始的像素数据

00:00:24.554 --> 00:00:26.864
即人类玩家在屏幕上看到的结果

00:00:26.864 --> 00:00:31.859
它学会了从头开始玩各种不同的 Atari 游戏

00:00:31.859 --> 00:00:35.060
他们将这个智能体称之为深度 Q 网络

00:00:35.060 --> 00:00:38.125
我们深入了解下它的工作原理

00:00:38.125 --> 00:00:41.500
正如它的名字所表示的含义 该智能体的核心是

00:00:41.500 --> 00:00:45.609
一个充当函数逼近器的深度神经网络

00:00:45.609 --> 00:00:50.229
你一次传入一个你喜欢的视频游戏屏幕的图片

00:00:50.229 --> 00:00:52.914
它会生成一个动作值向量

00:00:52.914 --> 00:00:56.335
最大值表示采取的动作

00:00:56.335 --> 00:00:58.314
根据强化信号

00:00:58.314 --> 00:01:02.199
它会在每个时间步将游戏得分的变化往回馈送

00:01:02.200 --> 00:01:06.685
一开始 当神经网络初始化为随机的值时

00:01:06.685 --> 00:01:09.340
采取的动作很混乱

00:01:09.340 --> 00:01:11.665
正如你所预期的 效果很差

00:01:11.665 --> 00:01:15.820
但是随着时间的推移 它开始将游戏中的情景和顺序

00:01:15.819 --> 00:01:20.709
与相应的动作关联起来 并且学会很好地玩游戏

00:01:20.709 --> 00:01:23.619
输入空间非常复杂

00:01:23.620 --> 00:01:29.260
Atari 游戏的分辨率为 210 x 160 像素

00:01:29.260 --> 00:01:33.685
每个像素有 128 种可能的颜色

00:01:33.685 --> 00:01:40.105
这依然是一个离散状态空间 但是处理起来非常庞大

00:01:40.105 --> 00:01:42.145
为了降低复杂性

00:01:42.144 --> 00:01:45.774
DeepMind 团队决定稍加处理

00:01:45.775 --> 00:01:47.780
将帧转换为灰阶

00:01:47.780 --> 00:01:52.215
并缩小为 84 x 84 像素的正方形

00:01:52.215 --> 00:01:57.960
正方形图片使他们能够在 GPU 上使用更加优化的神经网络运算

00:01:57.959 --> 00:02:01.634
为了使智能体能够访问一系列的帧

00:02:01.635 --> 00:02:04.800
他们将四个此类帧堆叠起来

00:02:04.799 --> 00:02:09.155
形成 84 x 84 x 4 的最终状态空间

00:02:09.155 --> 00:02:11.560
也许还有其他处理序列数据的方法

00:02:11.560 --> 00:02:16.180
但是这个似乎是一个效果很好的简单方法

00:02:16.180 --> 00:02:17.635
在输出端

00:02:17.634 --> 00:02:20.454
与传统强化学习设置

00:02:20.455 --> 00:02:23.410
（一次仅生成一个 Q 值）不同

00:02:23.409 --> 00:02:25.930
深度 Q 网络会为一个前向传递中的

00:02:25.930 --> 00:02:30.444
所有可能的动作生成一个 Q 值

00:02:30.444 --> 00:02:34.974
不这么处理的话 你需要单独为每个动作运行该网络

00:02:34.974 --> 00:02:38.784
现在 你可以直接根据该向量采取动作

00:02:38.784 --> 00:02:45.034
要么是随机形式 要么选择值最大的动作 很简单 对吧

00:02:45.034 --> 00:02:49.620
这些创新型输入和输出转换

00:02:49.620 --> 00:02:53.534
支持在后台使用强大却简单的神经网络结构

00:02:53.534 --> 00:02:57.329
屏幕图片首先用卷积层处理

00:02:57.330 --> 00:03:01.094
使系统能够发现空间关系

00:03:01.094 --> 00:03:03.930
并探索空间规则空间

00:03:03.930 --> 00:03:08.175
此外 因为将四个帧堆叠起来作为输入

00:03:08.175 --> 00:03:13.800
这些卷积层还将从这些帧中提取时间属性

00:03:13.800 --> 00:03:16.485
原始 DQN 智能体

00:03:16.485 --> 00:03:21.700
使用了三个此类卷积层 并采用 ReLU 激活函数 即正则化线性单元

00:03:21.699 --> 00:03:26.224
然后是一层采用 ReLU 激活函数的完全连接隐藏层

00:03:26.224 --> 00:03:31.835
以及一层完全连接的线性输出层 用于生成动作值向量

00:03:31.835 --> 00:03:36.320
他们在所有接受测试的 Atari 游戏上应用了同一架构

00:03:36.319 --> 00:03:41.609
但是每个游戏都从头开始学习并使用重新初始化的网络

00:03:41.610 --> 00:03:45.115
训练此类网络需要大量数据

00:03:45.115 --> 00:03:49.435
但即使这样 也不能保证会收敛于最优值函数

00:03:49.435 --> 00:03:53.949
实际上 在某些情况下 网络权重会因为动作和状态

00:03:53.949 --> 00:03:57.369
之间的关系非常紧密而振荡或发散

00:03:57.370 --> 00:04:01.405
这样会导致非常不稳定并且效率很低的策略

00:04:01.405 --> 00:04:03.520
为了克服这些挑战

00:04:03.520 --> 00:04:06.040
研究人员想出了多个技巧

00:04:06.039 --> 00:04:08.949
稍微修改了基础 Q 学习算法

00:04:08.949 --> 00:04:11.919
我们将了解其中的两个技巧

00:04:11.919 --> 00:04:14.799
我认为这两个技巧是他们的成果中最重要的贡献

00:04:14.800 --> 00:04:18.329
即经验回放和固定 Q 目标

