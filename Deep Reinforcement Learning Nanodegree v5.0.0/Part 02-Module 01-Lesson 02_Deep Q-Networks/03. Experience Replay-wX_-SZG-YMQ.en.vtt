WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.230
The idea of experience replay and its application to training

00:00:04.230 --> 00:00:08.914
neural networks for reinforcement learning isn't new.

00:00:08.914 --> 00:00:14.414
It was originally proposed to make more efficient use of observed experiences.

00:00:14.414 --> 00:00:17.429
Consider the basic online Q-learning algorithm where we

00:00:17.429 --> 00:00:20.609
interact with the environment and at each time step,

00:00:20.609 --> 00:00:24.600
we obtain a state action reward next state tuple.

00:00:24.600 --> 00:00:27.435
Learn from it and then discarded.

00:00:27.434 --> 00:00:31.189
Moving on to the next tuple in the following timestep.

00:00:31.190 --> 00:00:32.925
This seems a little wasteful.

00:00:32.924 --> 00:00:38.399
We could possibly learn more from these experienced tuples if we stored them somewhere.

00:00:38.399 --> 00:00:44.164
Moreover, some states are pretty rare to come by and some actions can be pretty costly,

00:00:44.164 --> 00:00:47.875
so it would be nice to recall such experiences.

00:00:47.875 --> 00:00:51.450
That is exactly what a replay buffer allows us to do.

00:00:51.450 --> 00:00:55.920
We store each experienced tuple in this buffer as we are interacting with

00:00:55.920 --> 00:01:01.300
the environment and then sample a small batch of tuples from it in order to learn.

00:01:01.299 --> 00:01:06.185
As a result, we are able to learn from individual tuples multiple times,

00:01:06.185 --> 00:01:11.469
recall rare occurrences, and in general make better use of fire experience.

00:01:11.469 --> 00:01:14.909
But there is another critical problem that experience replay can

00:01:14.909 --> 00:01:18.944
help with and this is what DQN takes advantage of.

00:01:18.944 --> 00:01:21.494
If you think about the experiences being obtained,

00:01:21.495 --> 00:01:26.939
you realize that every action AT affects the next state ST in some way,

00:01:26.939 --> 00:01:32.099
which means that a sequence of experienced tuples can be highly correlated.

00:01:32.099 --> 00:01:36.089
A naive Q-learning approach that learns from each of these experiences in

00:01:36.090 --> 00:01:41.585
sequential order runs the risk of getting swayed by the effects of this correlation.

00:01:41.585 --> 00:01:45.674
With experience replay, can sample from this buffer at random.

00:01:45.674 --> 00:01:50.519
It doesn't have to be in the same sequence as we stored the tuples.

00:01:50.519 --> 00:01:53.969
This helps break the correlation and ultimately prevents

00:01:53.969 --> 00:01:58.525
action values from oscillating or diverging catastrophically.

00:01:58.525 --> 00:02:01.743
Now, this might be a little hard to understand,

00:02:01.742 --> 00:02:04.114
so let's look at an example.

00:02:04.114 --> 00:02:09.710
I'm learning to play tennis and I like to practice rallying against the wall.

00:02:09.710 --> 00:02:13.295
I'm more confident with my forehand shot than my backhand,

00:02:13.294 --> 00:02:15.619
and I can hit the ball fairly straight.

00:02:15.620 --> 00:02:18.515
So the ball keeps coming back around the same area

00:02:18.514 --> 00:02:22.189
to my right and I keep hitting forehand shots.

00:02:22.189 --> 00:02:26.525
Now, if I were an online Q-learning agent learning as I play,

00:02:26.525 --> 00:02:28.534
this is what I might pick up.

00:02:28.534 --> 00:02:30.724
When the ball comes to my right,

00:02:30.724 --> 00:02:32.780
I should hit with my forehand,

00:02:32.780 --> 00:02:38.194
less certainly at first but with increasing confidence as I repeatedly hit the ball.

00:02:38.194 --> 00:02:41.639
Great! I'm learning to play forehand pretty well,

00:02:41.639 --> 00:02:45.199
but I'm not exploring the rest of the state space.

00:02:45.199 --> 00:02:47.294
This could be addressed by using

00:02:47.294 --> 00:02:51.559
an Epsilon-Greedy policy acting randomly with a small chance.

00:02:51.560 --> 00:02:57.020
So I try different combinations of states and actions and sometimes I make mistakes,

00:02:57.020 --> 00:03:00.980
but I eventually figure out the best overall policy.

00:03:00.979 --> 00:03:03.229
Use a forehand shot when the ball comes to

00:03:03.229 --> 00:03:06.724
my right and a backhand when it comes to my left.

00:03:06.724 --> 00:03:11.120
Perfect, this learning strategy seems to work well with a Q-table,

00:03:11.120 --> 00:03:17.034
where we have assumed this highly simplified state space with just two discrete states.

00:03:17.034 --> 00:03:23.344
But when we consider a continuous state space things can fall apart. Let's see how.

00:03:23.344 --> 00:03:29.090
First the ball can actually come anywhere between the extreme left and extreme right.

00:03:29.090 --> 00:03:31.340
If I tried to discretize this range into

00:03:31.340 --> 00:03:34.784
small buckets I wouldn't have too many possibilities.

00:03:34.784 --> 00:03:37.699
What if I end up learning a policy with holes in it,

00:03:37.699 --> 00:03:41.984
states or situations that I may not have visited during practice.

00:03:41.985 --> 00:03:47.260
Instead it makes more sense to use a function approximator like a linear combination of

00:03:47.259 --> 00:03:53.379
RBF kernels or a Q-network that can generalize my learning across the space.

00:03:53.379 --> 00:03:58.354
Now, every time the ball comes to my right and I successfully hit a forehand shot,

00:03:58.354 --> 00:04:00.814
my value function changes slightly.

00:04:00.814 --> 00:04:05.000
It becomes more positive around the exact region where the ball came,

00:04:05.000 --> 00:04:10.710
but also raises the value for the forehand shot in general across the state space.

00:04:10.710 --> 00:04:13.925
The effect is less pronounced away from the exact spot,

00:04:13.925 --> 00:04:16.185
but over time it can add up.

00:04:16.185 --> 00:04:20.120
And that's exactly what happens when I try to learn while playing,

00:04:20.120 --> 00:04:23.235
processing each experience tuple in order.

00:04:23.235 --> 00:04:26.375
For instance, if my forehand shot is fairly straight,

00:04:26.375 --> 00:04:29.819
I likely get back the ball around the same spot.

00:04:29.819 --> 00:04:32.954
This produces a state very similar to the previous one,

00:04:32.954 --> 00:04:36.019
so I use my forehand again and if it is successful

00:04:36.019 --> 00:04:39.529
it reinforces my belief that forehand is a good choice.

00:04:39.529 --> 00:04:42.559
I can easily get trapped in this cycle.

00:04:42.560 --> 00:04:47.540
Ultimately, if I don't see too many examples of the ball coming to my left for a while,

00:04:47.540 --> 00:04:50.420
then the value of the forehand shot can become greater

00:04:50.420 --> 00:04:54.074
than backhand across the entire state space.

00:04:54.074 --> 00:04:58.865
My policy would then be to choose forehand regardless of where I see the ball coming.

00:04:58.865 --> 00:05:02.925
Disaster. Okay, how can we fix this?

00:05:02.925 --> 00:05:07.100
The first thing I should do is stop learning while practicing.

00:05:07.100 --> 00:05:10.400
This time is best spent in trying out different shots

00:05:10.399 --> 00:05:14.489
playing a little randomly and thus exploring the state space.

00:05:14.490 --> 00:05:17.569
It then becomes important to remember my interactions,

00:05:17.569 --> 00:05:21.034
what shots worked well in what situations, et cetera.

00:05:21.035 --> 00:05:24.463
When I take a break or when I am back home and resting,

00:05:24.463 --> 00:05:29.180
that's a good time to recall these experiences and learn from them.

00:05:29.180 --> 00:05:33.995
The main advantage is that now I have a more comprehensive set of examples.

00:05:33.995 --> 00:05:35.954
Somewhere the ball came to my right,

00:05:35.954 --> 00:05:37.654
somewhere it came to my left,

00:05:37.654 --> 00:05:40.434
some forehand shots, some backhand.

00:05:40.435 --> 00:05:43.939
I can generalize patterns from across these examples,

00:05:43.939 --> 00:05:47.089
recalling them in whatever order I please.

00:05:47.089 --> 00:05:50.239
This helps me avoid being fixated on one region of

00:05:50.240 --> 00:05:54.819
the state space or reinforcing the same action over and over.

00:05:54.819 --> 00:05:56.314
After a round of learning,

00:05:56.314 --> 00:05:59.464
I can go back to playing with my updated value function.

00:05:59.464 --> 00:06:04.389
Again, collect a bunch of experiences and then learn from them in a batch.

00:06:04.389 --> 00:06:09.125
In this way, experience replay can help us learn a more robust policy,

00:06:09.125 --> 00:06:12.439
one that is not affected by the inherent correlation

00:06:12.439 --> 00:06:17.295
present in the sequence of observed experience tuples.

00:06:17.295 --> 00:06:18.425
If you think about it,

00:06:18.425 --> 00:06:20.920
this approach is basically building a database of

00:06:20.920 --> 00:06:24.210
samples and then learning a mapping from them.

00:06:24.209 --> 00:06:27.219
In that sense experience replay helps us reduce

00:06:27.220 --> 00:06:30.370
the reinforcement learning problem or at least value

00:06:30.370 --> 00:06:34.090
learning portion of it to a supervised learning scenario.

00:06:34.089 --> 00:06:35.814
That's clever.

00:06:35.814 --> 00:06:38.769
We can then apply other models learning techniques and

00:06:38.769 --> 00:06:40.379
best practices developed in

00:06:40.379 --> 00:06:44.589
the supervised learning literature through reinforcement learning.

00:06:44.589 --> 00:06:48.009
We can even improve upon this idea, for example,

00:06:48.009 --> 00:06:53.000
by prioritizing experience tuples that are rare or more important.

