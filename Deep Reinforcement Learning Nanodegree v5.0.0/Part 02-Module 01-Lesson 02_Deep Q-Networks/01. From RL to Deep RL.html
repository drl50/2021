<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   From RL to Deep RL
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Deep Q-Networks
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. From RL to Deep RL.html">
       01. From RL to Deep RL
      </a>
     </li>
     <li class="">
      <a href="02. Deep Q-Networks.html">
       02. Deep Q-Networks
      </a>
     </li>
     <li class="">
      <a href="03. Experience Replay.html">
       03. Experience Replay
      </a>
     </li>
     <li class="">
      <a href="04. Fixed Q-Targets.html">
       04. Fixed Q-Targets
      </a>
     </li>
     <li class="">
      <a href="05. Deep Q-Learning Algorithm.html">
       05. Deep Q-Learning Algorithm
      </a>
     </li>
     <li class="">
      <a href="06. Coding Exercise.html">
       06. Coding Exercise
      </a>
     </li>
     <li class="">
      <a href="07. Workspace.html">
       07. Workspace
      </a>
     </li>
     <li class="">
      <a href="08. Deep Q-Learning Improvements.html">
       08. Deep Q-Learning Improvements
      </a>
     </li>
     <li class="">
      <a href="09. Double DQN.html">
       09. Double DQN
      </a>
     </li>
     <li class="">
      <a href="10. Prioritized Experience Replay.html">
       10. Prioritized Experience Replay
      </a>
     </li>
     <li class="">
      <a href="11. Dueling DQN.html">
       11. Dueling DQN
      </a>
     </li>
     <li class="">
      <a href="12. Rainbow.html">
       12. Rainbow
      </a>
     </li>
     <li class="">
      <a href="13. Summary.html">
       13. Summary
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          01. From RL to Deep RL
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="from-rl-to-deep-rl">
          From RL to Deep RL
         </h1>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          So far, you've solved many of your own reinforcement learning problems, using solution methods that represent the action values in a small table.  Earlier in the nanodegree, we referred to this table as a
          <strong>
           Q-table
          </strong>
          .
         </p>
         <p>
          In the video below,
          <strong>
           Kelvin Lwin
          </strong>
          will introduce you to the idea of using neural networks to expand the size of the problems that we can solve with reinforcement learning.  This context is useful preparation for exploring the details behind the Deep Q-Learning algorithm later in this lesson!
         </p>
         <p>
          <em>
           Kelvin is a Senior Deep Learning Instructor at the
           <a href="https://www.nvidia.com/en-us/deep-learning-ai/education" rel="noopener noreferrer" target="_blank">
            NVIDIA Deep Learning Institute
           </a>
           .
          </em>
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
         <p>
          From RL to Deep RL
         </p>
        </h3>
        <video controls="">
         <source src="01. From RL to Deep RL-7HLJ0uaR1F0.mp4" type="video/mp4"/>
         <track default="true" kind="subtitles" label="en" src="01. From RL to Deep RL-7HLJ0uaR1F0.en.vtt" srclang="en"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-stabilizing-deep-reinforcement-learning">
          ## Stabilizing Deep Reinforcement Learning
         </h2>
         <p>
          As you'll learn in this lesson, the Deep Q-Learning algorithm represents the optimal action-value function
          <span class="mathquill ud-math">
           q_*
          </span>
          as a neural network (instead of a table).
         </p>
         <p>
          Unfortunately, reinforcement learning is
          <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.3097&amp;rep=rep1&amp;type=pdf" rel="noopener noreferrer" target="_blank">
           notoriously unstable
          </a>
          when neural networks are used to represent the action values.  In this lesson, you'll learn all about the Deep Q-Learning algorithm, which addressed these instabilities by using
          <strong>
           two key features
          </strong>
          :
         </p>
         <ul>
          <li>
           Experience Replay
          </li>
          <li>
           Fixed Q-Targets
          </li>
         </ul>
         <p>
          Watch the video below to learn more!
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
         <p>
          DQN Overview
         </p>
        </h3>
        <video controls="">
         <source src="01. DQN Overview-WgiAvr7COR0.mp4" type="video/mp4"/>
         <track default="true" kind="subtitles" label="en" src="01. DQN Overview-WgiAvr7COR0.en.vtt" srclang="en"/>
        </video>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="-additional-references">
          ## Additional References
         </h2>
         <ul>
          <li>
           <p>
            Riedmiller, Martin. "Neural fitted Q iterationâ€“first experiences with a data efficient neural reinforcement learning method." European Conference on Machine Learning. Springer, Berlin, Heidelberg, 2005.
            <br/>
            <a href="http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf" rel="noopener noreferrer" target="_blank">
             http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf
            </a>
           </p>
          </li>
          <li>
           <p>
            Mnih, Volodymyr, et al. "Human-level control through deep reinforcement learning." Nature518.7540 (2015): 529.
            <br/>
            <a href="http://www.davidqiu.com:8888/research/nature14236.pdf" rel="noopener noreferrer" target="_blank">
             http://www.davidqiu.com:8888/research/nature14236.pdf
            </a>
           </p>
          </li>
         </ul>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="02. Deep Q-Networks.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('01. From RL to Deep RL')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
