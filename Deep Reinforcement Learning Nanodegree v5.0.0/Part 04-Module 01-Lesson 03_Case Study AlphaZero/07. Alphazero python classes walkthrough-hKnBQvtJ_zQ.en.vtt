WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.404
Hello, welcome. In this screencast,

00:00:02.404 --> 00:00:05.994
I want to walk you through how I implement some of

00:00:05.995 --> 00:00:07.860
the gaming environment and

00:00:07.860 --> 00:00:11.634
the tree search environment so that in case you want to edit the files,

00:00:11.634 --> 00:00:14.679
you can get an understanding of how they are implemented.

00:00:14.679 --> 00:00:21.750
So let's go to the ConnectN.py to look at how the game class is implemented.

00:00:21.750 --> 00:00:28.079
So, the important pieces this class, the ConnectN class.

00:00:28.079 --> 00:00:34.710
When it initialize, it keeps track of the size of the game board, the winning conditions,

00:00:34.710 --> 00:00:37.700
the state is a bunch of stuff with a bunch of zeros,

00:00:37.700 --> 00:00:41.429
it also saves the last move whether there's a pie_rule,

00:00:41.429 --> 00:00:46.200
whether the pie_rule switch side was applied.

00:00:46.200 --> 00:00:53.180
So, I also supply a copy function that allows the class to be copied more quickly.

00:00:53.179 --> 00:00:59.435
This is very useful when using the Monte Carlo tree search to explore.

00:00:59.435 --> 00:01:02.594
Now, whenever a move is placed,

00:01:02.594 --> 00:01:05.319
the code will try to compute a score.

00:01:05.319 --> 00:01:06.819
To speed things up,

00:01:06.819 --> 00:01:12.064
it only computes scores from the last move.

00:01:12.064 --> 00:01:16.215
Given that if you play the game sequentially,

00:01:16.215 --> 00:01:20.365
you can't win a game without involving the last move.

00:01:20.364 --> 00:01:25.419
So, what it does is it takes the last move and compute the horizontal, vertical,

00:01:25.420 --> 00:01:30.260
diagonal lines around that last move to

00:01:30.260 --> 00:01:35.625
see how many of the same pieces are in a row and compute a score here.

00:01:35.625 --> 00:01:39.790
So, this is very fast, all this function.

00:01:41.090 --> 00:01:44.715
Now, there's also a function that's useful.

00:01:44.715 --> 00:01:50.270
It's mostly for rendering that when the game has been finished,

00:01:50.269 --> 00:01:57.444
a player has won, it will return precisely the location of the pieces that won the game.

00:01:57.444 --> 00:02:00.719
So this is useful for displaying the graphics.

00:02:00.719 --> 00:02:08.724
Now, you can see the moves here is the function that advanced the game by one step.

00:02:08.724 --> 00:02:13.144
Now, it's fairly straightforward it checks whether

00:02:13.145 --> 00:02:18.420
the locations are in range and whether the current location is zero.

00:02:18.419 --> 00:02:22.984
Although there is one subtlety here is the pie_rule.

00:02:22.985 --> 00:02:25.800
So, when the first player makes a move,

00:02:25.800 --> 00:02:29.100
the second player can overtake that piece.

00:02:29.099 --> 00:02:31.049
The way I've implemented is that,

00:02:31.050 --> 00:02:32.630
in the first move,

00:02:32.629 --> 00:02:35.055
if pie_rule is enabled,

00:02:35.055 --> 00:02:39.200
the first player will place instead of one,

00:02:39.199 --> 00:02:41.549
it will place 0.5, half,

00:02:41.550 --> 00:02:45.320
on the location and this code checks for that.

00:02:45.319 --> 00:02:48.729
If the second player decide to overtake that place,

00:02:48.729 --> 00:02:51.424
the 0.5 will become a negative one.

00:02:51.425 --> 00:02:54.380
Otherwise, the original move, 0.5,

00:02:54.379 --> 00:02:58.750
will turn into a legitimate piece plus one.

00:02:59.719 --> 00:03:02.509
There are also some useful function,

00:03:02.509 --> 00:03:08.159
available_moves just tells you which pieces are available.

00:03:08.159 --> 00:03:14.405
So that's it for the ConnectN file. Let's go back.

00:03:14.405 --> 00:03:22.055
Now, let me go to the Play.py file that does the rendering.

00:03:22.055 --> 00:03:23.840
There's quite a bit of bookkeeping,

00:03:23.840 --> 00:03:28.854
and here there are a lot of functions that utilize matplotlib plotting functions.

00:03:28.854 --> 00:03:33.019
Now, the interesting part is when you initialize a class,

00:03:33.020 --> 00:03:39.670
it automatically calls play which tackles the rendering.

00:03:39.669 --> 00:03:49.804
Now, if one of the players none or one of them is human,

00:03:49.805 --> 00:03:53.060
I have to add in

00:03:53.060 --> 00:03:59.485
an interactive element when a button is clicked such that a move is registered.

00:03:59.485 --> 00:04:06.230
So, I have to do this when there's a human player in in the play class,

00:04:06.229 --> 00:04:14.495
and you can check out the click function that will display a circle in a move.

00:04:14.495 --> 00:04:16.660
There's also a draw_winner.

00:04:16.660 --> 00:04:18.210
So, every time you make a move,

00:04:18.209 --> 00:04:21.414
the game checks whether someone won,

00:04:21.415 --> 00:04:26.330
and then the draw winner will put an extra marker on

00:04:26.329 --> 00:04:32.479
the winning pieces to indicate that the game ends in a win and loss.

00:04:33.110 --> 00:04:38.290
Now, when both players are AIs,

00:04:38.290 --> 00:04:39.910
the function is slightly different.

00:04:39.910 --> 00:04:43.400
Instead of having an interactive element,

00:04:43.399 --> 00:04:52.349
I use a function animation that basically draws each move as each move is played.

00:04:52.350 --> 00:04:54.215
So you can check that out.

00:04:54.214 --> 00:04:57.319
You can also modify the coloring scheme,

00:04:57.319 --> 00:05:01.719
the sizes of things if you would like when you edit this function.

00:05:01.720 --> 00:05:06.950
Now, let's go back and walk through the most important part of the code,

00:05:06.949 --> 00:05:10.545
the Monte Carlo tree search implementation.

00:05:10.545 --> 00:05:19.775
Now, the first subtlety is that I've included a bunch of transformation T0 to T7.

00:05:19.774 --> 00:05:23.414
Basically, when you have a square board

00:05:23.415 --> 00:05:28.800
and the board is symmetric under rotations and flips,

00:05:28.800 --> 00:05:35.785
the board shouldn't change when I do different combinations of rotations and flip,

00:05:35.785 --> 00:05:40.525
and also the inversion operation.

00:05:40.524 --> 00:05:48.844
So, every time I put the game state to the neural network,

00:05:48.845 --> 00:05:52.505
I randomly choose one transformation

00:05:52.504 --> 00:05:56.449
together with one inverse transformation to apply to this state.

00:05:56.449 --> 00:06:00.649
This speed sub training by a little bit and it's helpful.

00:06:00.649 --> 00:06:03.435
So, and this policy,

00:06:03.435 --> 00:06:09.360
this process_policy is a bridge between turning the policy in

00:06:09.360 --> 00:06:17.180
a game and compute the output that's properly fed into the Monte Carlo tree search class.

00:06:17.180 --> 00:06:21.475
Now, let's look at the implementation of the Monte Carlo tree.

00:06:21.475 --> 00:06:24.370
I've defined a node class.

00:06:24.370 --> 00:06:27.055
The node class contains children nodes,

00:06:27.055 --> 00:06:32.009
and U value, and the V value, and the N value.

00:06:32.009 --> 00:06:38.810
It also contains the V value evaluated from the neural network for bookkeeping.

00:06:38.810 --> 00:06:45.410
Also, probability evaluated from the neural network,

00:06:45.410 --> 00:06:49.070
and this is torch so such that later on when

00:06:49.069 --> 00:06:52.709
we compute the loss function for backpropagation.

00:06:52.709 --> 00:06:55.349
You can retrieve it from here.

00:06:57.279 --> 00:07:03.609
Now, a lot of this is bookkeeping wanted to know is that there's

00:07:03.610 --> 00:07:07.629
this create_child function that will expand

00:07:07.629 --> 00:07:14.704
the current node into children and store it in the self.child member,

00:07:14.704 --> 00:07:20.930
and sometimes a children might be a winning move.

00:07:20.930 --> 00:07:23.420
So, in the create_child,

00:07:23.420 --> 00:07:31.160
it acknowledges that by telling the tree to always pick the winning move.

00:07:31.160 --> 00:07:38.305
So, all winning moves have U values equal to plus infinity,

00:07:38.305 --> 00:07:40.790
and that speed things up.

00:07:40.790 --> 00:07:42.405
Such that if there's a winning move,

00:07:42.404 --> 00:07:46.584
I don't have to explore all the other non-winning moves.

00:07:46.584 --> 00:07:50.169
It's not necessary but it's useful for training,

00:07:50.170 --> 00:07:52.670
especially given that training takes quite a while.

00:07:52.670 --> 00:07:55.960
Now, let's look through the explore function.

00:07:55.959 --> 00:07:58.669
So, each time you explore,

00:07:58.670 --> 00:08:03.310
you have to play a new game simulated by the critic.

00:08:03.310 --> 00:08:06.685
So, you start from the top of the node.

00:08:06.685 --> 00:08:10.564
So, we start from the current node, it's the self,

00:08:10.564 --> 00:08:14.279
and you then you keep going through the children,

00:08:14.279 --> 00:08:18.694
and then you pick the ones with maximum U.

00:08:18.694 --> 00:08:24.714
Sometimes there are multiple nodes with the same maximum U.

00:08:24.714 --> 00:08:30.239
In that case, I randomly choose an action, so random.choice.

00:08:30.240 --> 00:08:32.299
Now, as I mentioned before,

00:08:32.299 --> 00:08:36.084
there are some nice speedups we can obtain.

00:08:36.085 --> 00:08:41.960
For example, if all the moves are losing,

00:08:41.960 --> 00:08:47.235
then I've bookept it to be negative infinity so it knows that,

00:08:47.235 --> 00:08:52.830
and then it will convert the current U value to be infinite.

00:08:52.830 --> 00:08:56.590
You might wonder why if the maximum U is negative infinity,

00:08:56.590 --> 00:08:59.545
why that U is positive infinity?

00:08:59.544 --> 00:09:04.485
Well, because every time we go to the next level,

00:09:04.485 --> 00:09:06.470
the player gets changed.

00:09:06.470 --> 00:09:08.615
So, if right now,

00:09:08.615 --> 00:09:12.049
the maximum U for all the children are negative infinity,

00:09:12.049 --> 00:09:13.733
meaning they're all losing,

00:09:13.734 --> 00:09:17.870
then it means previously the previous player made a really good move,

00:09:17.870 --> 00:09:20.294
and that's a winning move for the previous player.

00:09:20.294 --> 00:09:27.929
So, I have this condition here and also update the V to be 1.0.

00:09:27.929 --> 00:09:34.069
Now, the same thing is true for when the maximum U is in infinite.

00:09:34.070 --> 00:09:37.520
It's a winning move for the current player,

00:09:37.519 --> 00:09:39.934
but from the perspective of the previous player,

00:09:39.934 --> 00:09:41.789
that's a losing game,

00:09:41.789 --> 00:09:45.659
given that the current player can win.

00:09:45.659 --> 00:09:51.865
So then, the current.U has to be negative infinity and also current.V is negative one.

00:09:51.865 --> 00:09:54.335
So this is not strictly necessary,

00:09:54.335 --> 00:09:56.634
but it saves time.

00:09:56.634 --> 00:09:59.179
Whenever we encounter winning move,

00:09:59.179 --> 00:10:02.009
we don't have to explore the other possibilities.

00:10:02.009 --> 00:10:05.529
That's useful. Now, as you go through this code,

00:10:05.529 --> 00:10:08.629
it's essentially whenever I also keep

00:10:08.629 --> 00:10:12.450
these conditions when checking for winning conditions,

00:10:12.450 --> 00:10:15.259
and then you increment the V's count,

00:10:15.259 --> 00:10:17.524
and you go backpropagation,

00:10:17.524 --> 00:10:20.579
you go back to the mother, increment the V's count for the mother,

00:10:20.580 --> 00:10:24.250
and you also update the average V for the mother.

00:10:24.250 --> 00:10:27.034
Notice that there's an extra negative sign here.

00:10:27.034 --> 00:10:32.209
This is because every time when I go to the next or previous layer,

00:10:32.210 --> 00:10:39.210
the player is switched and current.V is from the perspective of the current player.

00:10:39.210 --> 00:10:43.710
So, that's why when I update the average,

00:10:43.710 --> 00:10:46.820
there is a negative current.V.

00:10:46.820 --> 00:10:56.140
Then after that, I can update all the U values in all the sibling branches.

00:10:56.139 --> 00:10:59.225
Now, onward to the next step,

00:10:59.225 --> 00:11:04.710
this next function basically choose the next action for the tree.

00:11:04.710 --> 00:11:08.580
You see that I find the maximum U.

00:11:08.580 --> 00:11:10.545
If the maximum U is infinite,

00:11:10.544 --> 00:11:12.179
that means there's a winning move.

00:11:12.179 --> 00:11:17.174
I speed things up by telling the agent to always choose one the winning move,

00:11:17.174 --> 00:11:19.509
there's no reason to not to.

00:11:19.509 --> 00:11:22.434
Now, when that doesn't happen,

00:11:22.434 --> 00:11:26.514
then I have to look at the V's count like this,

00:11:26.514 --> 00:11:29.144
and there's a temperature parameter,

00:11:29.144 --> 00:11:33.169
it's basically fixed to one by default so that the probabilities is

00:11:33.169 --> 00:11:37.865
simply proportional to the V's counts.

00:11:37.865 --> 00:11:43.284
After that, it increments to the next step.

00:11:43.284 --> 00:11:46.495
You can see here the next day,

00:11:46.495 --> 00:11:52.845
I went on the choose the children based on the probability,

00:11:52.845 --> 00:11:56.870
the random.choices give you a list that has one element,

00:11:56.870 --> 00:11:59.450
so I just choose the zero element,

00:11:59.450 --> 00:12:05.930
and it outputs other useful values for the updating the policy.

00:12:05.929 --> 00:12:13.879
So, that's it. That's all the classes that are implemented for the AlphaZero agent.

